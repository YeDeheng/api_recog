rolling median in python
i have some stock data based on daily close values . i need to be able to insert these values into a python list and get a median for the last 30 closes . is there a python library that does this ?
thank you~
yueer
#url
it would be more efficient to do the median of the last 29 values . if 30 was an arbitrary choice , you can avoid having to calculate the mean of the middle two if you choose an odd-numbered window size .
that's why it's nice to have a specialized library so i don't have to manipulate my preferences over the fear that there is a bug in my algorithm . it looks like pandas handles arbitrary intervals and timestamps the data . nice !
agreed , but my point was really : understanding the algorithms you're using can often lead to improved performance . worth bearing in mind if / when you nede to optomise further down the road ...
yes there is :
#url
in pure python , having your data in a python list ` a ` , you could do #code
( this assumes ` a ` has at least 30 items . )
using the numpy package , you could use #code
isn't the median just the middle value in a sorted range ?
so , assuming your list is ` stock_data ` : #code
now you just need to get the off-by-one errors found and fixed and also handle the case of ` stock_data ` being less than 30 elements ...
let us try that here a bit : #code
for an list with an even number of values the median is the mean of the two middle values .
have you considered pandas ? it is based on ` numpy ` and can automatically associate timestamps with your data , and discards any unknown dates as long as you fill it with ` numpy.nan ` . it also offers some rather powerful graphing via matplotlib .
basically it was designed for financial analysis in python .
nice : #url

compute a compounded return series in python
greetings all , i have two series of data : daily raw stock price returns ( positive or negative floats ) and trade signals ( buy=1 , sell=-1 , no trade=0 ) .
the raw price returns are simply the log of today's price divided by yesterday's price : #code
an example : #code
the trade signal series looks like this : #code
to get the daily returns based on the trade signals : #code
these daily returns might look like this : #code
i need to use the daily_returns series to compute a compounded returns series . however , given that there are 0 values in the daily_returns series , i need to carry over the last non-zero compound return " through time " to the next non-zero compound return .
for example , i compute the compound returns like this ( notice i am going " backwards " through time ): #code
and the resulting list : #code
my goal is to carry over the last non-zero return to the accumulate these compound returns . that is , since the return at index i is dependent on the return at index i+1 , the return at index i+1 should be non-zero . every time the list comprehension encounters a zero in the daily_return series , it essentially restarts .
@user : this kind of comprehensions starts easily to look quite a mess . may i suggest more suitable tools , like #url thanks
@user the method i need to use is (( 1+daily [ i ]) * ( 1+compounded [ i-1 ])) -1 . the end goal is to create a plot of the cumulative compounded returns . the method you mention is just that : cumulative simple returns-it does not include previous returns ( i.e. returns are not re-invested )
@user i'm not sure what you mean ...
@user : just that there exists packages in ` python ` like ` scipy\numpy ` to handle this kind of computations very straightforward manner . if you are working serious manner with this kind of series , then i'll just recommend you to get to know ' more advanced ' ways to handle them than ' raw ' ` python ` . thanks
@user oh i see . i have been looking through the scipy and numpy packages and source for a method for this calculation . numpy has the finance package but does not include a method for calculating a compound return series ( at least what i could find ) .
@user : it's nice to hear that you are aware of ` scipy\numpy ` . if something you need doesn't exists there , you'll still be free to implement ( missing pieces ) according to your own requirements , but using more reasonable data types than what ' raw ' ` python ` can provide . you may re tag and edit your question to indicate that ` scipy\numpy ` solutions are eligible as well . thanks
there is a fantastic module called pandas that was written by a guy at aqr ( a hedge fund ) that excels at calculations like this ... what you need is a way to handle " missing data " ... as someone mentioned above , the basics are using the nan ( not a number ) capabilities of scipy or numpy ; however , even those libraries don't make financial calculations that much easier ... if you use pandas , you can mark the data you don't want to consider as ` nan ` , and then any future calculations will reject it , while performing normal operations on other data .
i have been using pandas on my trading platform for about 8 months ... i wish i had started using it sooner .
wes ( the author ) gave a talk at pycon 2010 about the capabilities of the module ... see the slides and video on the pycon 2010 webpage . in that video , he demonstrates how to get daily returns , run 1000s of linear regressions on a matrix of returns ( in a fraction of a second ) , timestamp / graph data ... all done with this module . combined with psyco , this is a beast of a financial analysis tool .
the other great thing it handles is cross-sectional data ... so you could grab daily close prices , their rolling means , etc ... then timestamp every calculation , and get all this stored in something similar to a python dictionary ( see the ` pandas.dataframe ` class ) ... then you access slices of the data as simply as : #code
see the pandas rolling moments doc for more information on to calculate the rolling stdev ( it's a one-liner ) .
wes has gone out of his way to speed the module up with cython , although i'll concede that i'm considering upgrading my server ( an older xeon ) , due to my analysis requirements .
edit for strimp's question :
after you converted your code to use pandas data structures , it's still unclear to me how you're indexing your data in a pandas dataframe and the compounding function's requirements for handling missing data ( or for that matter the conditions for a 0.0 return ... or if you are using ` nan ` in pandas .. ) . i will demonstrate using my data indexing ... a day was picked at random ... ` df ` is a dataframe with es futures quotes in it ... indexed per second ... missing quotes are filled in with ` numpy.nan ` . dataframe indexes are ` datetime ` objects , offset by the ` pytz ` module's timezone objects . #code
to give a simple example of how to calculate a column of continuous returns ( in a ` pandas.timeseries `) , which reference the quote 10 minutes ago ( and filling in for missing ticks ) , i would do this : #code
no lambda is required in that case , just dividing the column of values by itself 600 seconds ago . that ` .shift ( 600 )` part is because my data is indexed per-second .
hth ,
\mike
( +1 ) for pointing out ` pandas ` . what kind of data volumes is it designed to handle ? could it realistically be useful on one-minute bars for 1,000 instruments over a year ( roughly 100k observations per instrument ) , or is it designed for lower-frequency stuff ?
i'm munching through tick data on index futures ( es , nq , ym ) ... i summarize the data at low-second intervals and process during rth ... so far , my largest tick files are esm10 ( s&p 500 futures ) quotes from may 6th ( flash crash ) , may 7th , and may 20th of last year ... my simulations ( including all calculations ) for those run about 5-10 minutes for a single instrument . my may 6th file has about 1.2million individual tick entries , and that processing time includes summarizing at intervals and over 15 different analysis per-trading-sec . my server is a 2ghz xeon quad-core ( 512kb cache ) with 4gb dram
sounds promising , thanks again .
never thought of using pandas even though i'm familiar with it and it's included in my python distro ( enthought ) . i'll give it a shot and report back , thanks .
@user pennington : for sure ` pandas ` is way more specific to op's realm . however , imho if there still exists some missing functionality needed , then to create such functionality ( by yourself ) , one would posses much higher chances to implement that a proficient way if only one is also ' familiar enough ' with ` scipy\numpy ` . thanks
indeed , you need to be familiar with numpy to get the most of pandas , but i have not found much that requires an outright extension ... for the most part , i'm combining the outputs of various pandas methods to get what i need , if it's not already built-in . most of my issues revolve around optimizing for speed ... that that tends to involve more work with cython than pandas or numpy .
@user pennington : so i was able convert my ~ 1,000 line program easily to using pandas . great call ... any idea how i might get my compound return series created ? i was was thinking of using a lambda function in the apply method of datamatrix but i'm having some challenges ... thanks
@user ... see the edit to my answer above ... without knowing more details , the best i can do is give an example with my data .
@user pennington : thanks , see my comment .
imagine i have a datamatrix with closing prices , some indicator value , and a trade signal like this : #code
i use the signal to create a datamatrix of returns based on the trade signal : #code
what i ended up doing is this : #code
for some reason i really struggled with this one ...
i'm in the process of converting all my price series to pytables . looks promising so far .
@user ... please test this out instead of the ` for ` loop and tell me if it works ... ` compounded = ( 1 + compounded.shift ( 1 )) * ( 1 + indicator_returns ) - 1 ` . you might need to reassign ` compounded [ 1 ]` after it finishes , but this should be faster than iterating over the matrix line-by-line ...
@user pennington : couponded is an array in this case and has no shift method ...
ok , is there anything else i can assist with ?
no , thanks mike .
@user pennington : done , sorry , new to stack :)
no problem at all ... i'm new too ... thanks for following up
the cumulative return part of this question is dealt with in wes mckinney's excellent ' python for data analysis ' book on page 339 , and uses cumprod() from pandas to create a rebased / indexed cumulative return from calculated price changes .
example from book : #code

sort a pandas datamatrix in ascending order
the pandas dataframe object has a sort method but pandas datamatrix object does not .
what is the best way to sort this datamatrix object by index ( the date column ) in ascending order ? #code
the result should be the datamatrix with 2 / 8/ 2011 as the first entry and 2 / 16 / 2011 as the last entry . the entries in the compound_ret column should follow their date in the sort . so the result should look something like this : #code
did you try it ? at least in the version of pandas i tried , ` datamatrix ` inherits from ` dataframe ` . #code
thanks for the reply , see above .
never , fails . just when i post , i figure it out . whatever version i'm on has sortup() and sortdown() methods for the datamatrix . i found them using dir ( datamatrix ) .
weird . guess it's an older version ; i just cloned the current one from github .
i have version 0.2 , the new version is 0.3 .
indeed between 0.2 and 0.3 i renamed ` sortup ` / ` sortdown ` to the single ` sort ` methods . sorry about that .
i definitely recommend keeping up on the bleeding edge of pandas if you can ( #url ) ! also , consider using ipython for all your interactive work ( #url ) -- i find that having tab completion and easy introspection of objects helps a great deal for finding methods and exploring docstrings .

how to get the correlation between two timeseries using pandas
i have two sets of temperature date , which have readings at regular ( but different ) time intervals . i'm trying to get the correlation between these two sets of data .
i've been playing with pandas to try to do this . i've created two timeseries , and am using ` timeseriesa.corr ( timeseriesb )` . however , if the times in the 2 timeseries do not match up exactly ( they're generally off by seconds ) , i get null as an answer . i could get a decent answer if i could :
a ) interpolate / fill missing times in each timeseries ( i know this is possible in pandas , i just don't know how to do it )
b ) strip the seconds out of python datetime objects ( set seconds to 00 , without changing minutes ) . i'd lose a degree of accuracy , but not a huge amount
c ) use something else in pandas to get the correlation between two timeseries
d ) use something in python to get the correlation between two lists of floats , each float having a corresponding datetime object , taking into account the time .
anyone have any suggestions ?
you have a number of options using pandas , but you have to make a decision about how it makes sense to align the data given that they don't occur at the same instants .
use the values " as of " the times in one of the time series , here's an example : #code
you can see these are off by 30 seconds . the ` reindex ` function enables you to align data while filling forward values ( getting the " as of " value ): #code
note that ' pad ' is also aliased by ' ffill ' ( but only in the very latest version of pandas on github as of this time ! ) .
strip seconds out of all your datetimes . the best way to do this is to use ` rename ` #code
note that if rename causes there to be duplicate dates an ` exception ` will be thrown .
for something a little more advanced , suppose you wanted to correlate the mean value for each minute ( where you have multiple observations per second ): #code
these last code snippets may not work if you don't have the latest code from #url . if ` .mean() ` doesn't work on a ` groupby ` object per above try ` .agg ( np.mean )`
hope this helps !
ha , you got it before i could :-) ...
if i'm reading the last part right , the last part computes the mean for values between 00 and 60 seconds ( the mean for xx : xx : 30 , not xx : xx : 00 ) , and assigns the result to xx : xx : 00 . the simple way around this would be ` date.replace ( second=30 )` , an overly complex way to get the averages per minute would be : ` ts_mean = seriest.groupby ( lambda date : date.replace ( second=0 ) if date.second <3 0 else date.replace ( second=0 ) +timedelta ( minutes=1 )) .mean() `

how to groupby a columns and sum values of other columns in pandas
i have a very large file ( 5gb ) , and i need to count the number of occurence using two columns #code
so obviously i have to find #code
how can i do that in a very fast way .
i used : #code
but it's not giving the right results ? any thoughts
iiuc you need ` count ` : #code
and if you need new column , use ` reset_index ` with parameter ` name ` : #code
or you can use ` size ` ( column ` c ` is omit ): #code
how it works ? if my answer was helpful , you can upvote and accept . thanks .

how to get a value from a cell of a data frame not as a series ?
i'm trying to extract data from a cell and get a ` series ` though i didn't define the data as a ` series ` .
why it's happening ?
and what a way for solving it ?
i created a ` dataframe ` from a ` .csv ` file : #code
with the columns _ ` id ` and ` sub_category `
when i'm to extracr a cell : #code
i'm getting a ` series ` ....
could you provide [ minimal , complete , and verifiable example ] ( #url ) ?
are you after `apps.iloc['sub_category'][apps[apps['_id '] == app_id ] .index .tolist() ] .values [ 0 ]` ?
from the docs :
getting values from an object with multi-axes selection uses the
following notation ( using ` .loc ` as an example , but applies to ` .iloc ` and
` .ix ` as well ) . any of the axes accessors may be the null slice : . axes
left out of the specification are assumed to be : . ( e.g. `p.loc['a ']` is
equiv to `p.loc['a ' , : , :] `)
` series ` : ` s.loc [ indexer ]`
` dataframe ` : ` df.loc [ row_indexer , column_indexer ]`
using ` .iloc [ ]` as you seem to intend , this would look as follows to extract a single cell value : #code
the cells of the data frmae is type series
could you please show your data ? if the cells contain ` series ` , than you should be expecting to get ` series ` back when selecting a single cell . can't really help very effectively without seeing input and output of your calcs .
unnamed : 0 _id name category \
33955 33955 com.imdb.mobile imdb movies & tv entertainment
description sub_category
33955 search the worlds largest collection : ... entertainment.0
that's my result :
could you pls add to your question where you can format the output ?

what is the best way to search list of words in the pandas dataframe which is having 1 million records ?
i want to search set of keywords ( e.g. word = [ ' mobile','bags ' , etc ]) in the pandas dataframe having approx . 1 million records .
if any of the word matches then i want to add extra column ( new_col ) having the value 1 .
text new_col
we are the provide of jute bags 1
this will automatically have o ( n*m ) performance , which for any sizable dataframe length ( n ) and word list ( m ) will get daunting quickly . vectorized is the way to go though , like as follows :
`df[df['text '] .str .contains ( " mobile|bags ")]`
i tried the same , but it didn't respond for last 5-7 minutes so i killed it . i tried this on 4gb ram machine .
welcome to the problems of computation . you can try a single query on a dataframe with 10 , 100 , 1000 , and you can view how the ** time complexity ** differs with dataframe size . the issue is your query is ** necessarily ** long ( it needs to query every element in the dataframe for every possible word , or o ( n*m ) .
yeah .. thanks alex :)
@user this is not duplicate , recheck the question statement , here problem is with the size of data not with the how to check .
@user that's the fastest way . if you're running out of memory : chunk it up .
@user out of memory is not the problem in this case , it is taking time of around 10 minutes for 1 million data . i want the more optimized way to search only such records which is having any of the set of keywords .
@user out of memory => swap => slow . like i say , chunks for pandas or a different more specific data structure ( not a dataframe ) / database .

merging geodataframes in geopandas ( crs do not match )
i am trying to merge two geodataframes ( want to see which polygon each point is in ) .
the following code gets me a warning first ( "` crs does not match ! `")
and then an error ( "` rtreeerror : coordinates must not have minimums more than maximums `") .
what exactly is wrong in there ? are crs coordinates systems ? if so , why are they not loaded the same way ? #code
link to data for reproduction :
shapefile ,
gps points
can you provide a reproducible example ? ( some code to make the two dataframes that reproduces the issue )
crs is indeed a coordinate reference system . you can check it with the ` .crs ` attribute of the geodataframe . ` polygonsgeodataframe ` will have the crs that is specified in the shapefile , while the ` pointsgeodataframe ` will have no crs . if both have the same crs , you can do ` pointsgeodataframe.crs = polygonsgeodataframe.crs `
@user the code is a bit tricky since i don't know how to reproduce the " geometry " column that geopandas made out of the shapefile , but i've edited the question to provide a link to the shapefile , and a link to the simple csv i am using .
@user using pointsgeodataframe.crs = polygonsgeodataframe.crs indeed makes the warning disappear . however the error about minimums more than maximums is still there .

pandas - remove duplicate rows except the one with highest value from another column
i have a large data frame ( more than 100 columns , and several 100 thousand rows ) with a number of rows that contain duplicate data . i am trying to remove the duplicate rows , keeping the one with the largest value in a different column .
essentially , i am sorting the data into individual bins based on time period , so across periods , one would expect to find a lot of duplication , as most entities exist across all time periods . what can't be allowed , however , is for the same entity to appear more than once in a given time period .
i tried the approach in python pandas : remove duplicates by columns a , keeping the row with the highest value in column b , on a subset of the data , with the plan to recombine with the original dataframe , df .
example data subset : #code
in the example above , i would like to keep the first instance ( as liq is higher with 1.72 ) and discard the second instance ( liq is lower , with 0.09 ) . note that there can be more than two duplicates in a given period_id .
i tried this was but it was very slow for me ( i stopped it after more than 5 minutes ): #code
i ultimately did the below , which is more verbose and ugly , and simply throws out all but one duplicate , but this is also very slow ! given the speed of other operations of similar complexity , i thought i would ask here for a better solution .
so my request is really to fix the above code so that it is fast , the below is given as guidance , and if in the vein of the below , perhaps i could also have discarded the duplicates based on index , rather than the reset_index / set_index approach that i have employed : #code
hows about this :
update all your columns with the max data .
pick a row ( say the first ) .
this should be much faster as it's vectorized . #code
note : i'd like to use groupby first or last here but i think there's a bug where they throw away your old index , i don't think they should ... nth is the works however .
an alternative is to first slice out the ones which don't equal liq max : #code
hi andy , interesting . are you saying transform the subset of the data ( as per code snippet two ) , and then update df with this ? or transform the entire dataset ? more than two mins in its still executing g.transform ( " max ") . more than 750k rows and more than 100 columns ...
@user the transform is the expensive part . but looking at the title again that may not be completely necessary ... do you just want the row with the highest liq field in each ( unique_id , period_id ) group ?
that sounds right :
( i ) in case where duplicate fields : keep max ( liq ) field , discard the others .
( ii ) in case where no duplicate fields : keeping max should just keep the single row that is already there .
@user added an alternative as the last line . will be a lot faster if that's all you need .
this still seems very slow : g = df.groupby ([ " unique_id " , " period_id "] , as_index=false ) . g [ " liq "] .transform ( " max ")] .groupby ([ " unique_id " , " period_id "]) .nth ( 0 ) . indeed , it failed due to memory error !
it occurred to me that it might be much faster to identify those groups that have duplicates using count . then i can apply the max transformation to that grouping , and then recombine the two into one .
indeed , i can use max , or simply sort by value ( descending ) on these and take the first element using g.nth ( 0 ) ?
i think the problem is that g [ " liq "] .transform ( " max ") resets the index , losing the original index in the process ? certainly df [ df [ " liq "] == g [ " liq "] .transform ( " max ") results in a memory error ... i'm still struggling with this . doing g.size() produces an effective count of the number of duplicates , and is very , very , fast , so i am trying to use this to get the unique_id and period_id pairs where size > 2 , then apply the max idea above to those which should be much faster , and then i'll need to recombine with the original data frame .
how's about you reset the index , then return the index of the maximum row for each group ( for the groups with > 1 ) , and then iloc based on that .
thanks andy , still grapping with pandas . any chance you could give me code ?
does this do it faster ? if so will append to answer . #url
thanks andy , this is what i was trying to do so please add it ( apologies for lateness , tied up with other debugging ! ) . it is faster , but still slow : is there a vectorized solution ? it is precisely max_mores = g2.agg ( lambda x : x.loc [ x.idxmax() ]) that is slow . i'm not familiar with transform , but i used .size() ? is there a drawback with that .

pandas dataframe apply specific function to each column
i have the following dataframe : #code
and i have the following series that contains multiplication factors that each column will be multiplied by : #code
what is the best way to return a dataframe that is the same shape as the original dataframe and contains each value multiplied by the appropriate multiplication factor . i can do this with a for loop but am wondering what the more efficient way is . #code
#url
won't this get the same thing ? #code
wow . yes - a lot easier than i had thought .
as long as the variable names match it will multiply matching variables .

pandas dataframe : area plot stacked='false ' not working
i have an area plot that remains stacked even if i explicitly deactivate the stacking by setting the argument stacked='false ' .
here's some example code : #code
the area plot looks like this on my machine :
i have two questions :
1 . ) how can i fix the area plot so that it's not stacked ?
2 . ) is there any way to get rid of the repeated " [ ax.legend ( loc=1 ) for ax in plt.gcf() .axes ]" after each plot ?
thanks in advance !
try changing `'false '` to ` false ` i'm not sure why it doesn't fail with `'false '` ...
i don't know my way around the pandas plotting code to check , but i bet there is a ` if stacked : ` conditional someplace in the code and `bool('false ') is true `
for 1 . ) changing ' false ' to false helped . thanks !!!
@user please answer your own question .
setting stacked=false instead of ' false ' solved the first problem for me ! #code
if anyone knows how to fix the legend position ( see original question ) , let me know !

dataframe with column names derived from column values and cell values by condition
i have to create a result pandas dataframe from a source pandas dataframe having two columns . the result dataframe should have headers of two types , one type should be from the source dataframe derived from one of the column values appending the column header with the values . the other header is taken as it is from the source dataframe with unique values if there are duplicates . the result dataframe cell values should be 1 depending on whether there was a corresponding derived header from the column values or 0 if none .
the dataframes are as below #code
can you show your efforts
as i am unable to proceed much , i created the header x first , and then filling the values with unique values . then saving it with filling all values with 0 first . then i am trying to next column header v and trying to append with underscore with unique values of v . here i am stuck as how to proceed . can you please help .
could you use something like this . i'm not sure its the best solution so i'm interested to see what others post ... #code
this outputs the following : #code
this seems to be working but header x is missing
edited the answer . should be exact now
jezrael's answer is much better ...
jimbo your solution also works perfectly
you can use function ` crosstab ` , then find values higher as ` 1 ` and convert it to ` 1 ` and ` 0 ` by ` astype ` : #code
this is shorter and concise . got to learn crosstab .

extracting a column from every frame in a panel
i have a panel data containing some data frames . all of them have a column named ' n0 ' . i'd like to an array containing the means of n0 for every panel . i managed with this : #code
but it seems too cumbersome . isn't there any cleaner way to extract the n0 columnes , like data['n0 '] ?
please show structure of panel data . is it a list of data frames ? a stacked , large data fame with panel indicators ? i see a good candidate for a ` groupby() ` .
you could use ` pd.panel.apply ` ( see docs ) as illustrated with random sample data : #code
which , as ` dataframe ` , looks as follows : #code
using ` .apply() ` as below gives the ` mean ` for each ` column ` by ` dataframe ` , the sample illustrates how to select only ` b ` . #code
using a ` multiindex ` ` dataframe ` instead might be simpler because better documented as it seems to be the more common use case .

pandas retrieving value from an excel table row
i am trying to fetch multiple values from a certain row , and i have achieved the job by using the following code : #code
and this prints out the values fine yet , it looks like this : #code
how would i exclude the brackets and the unicode thing from the returned value ?
i am returning this to a jinja2 html template , if there is an html trick that can remove those , the html solution will work as well .
thank you in advance .
try accessing the first element in the ` list ` ( or more accurately , ` numpy.ndarray ` , see docs ) that ` .values ` returns . #code

how to select rows from a data frame using a condition from many columns in pandas ?
i have data like this . #code
i'd like to select all the rows that either ` from_time ` is nan or ` to_time ` is nan and ` signdesc1 ` contains either ` am|pm `
this is what i come up with #code
but doing this wouldn't work #code
`data[(data['from_time '] .isnull() | data['to_time '] .isnull() ) | ( data['signdesc1 '] .str .contains ( ' ( am|pm )'))]`
should work .

how to write a pandas dataframe to django model
i have been using pandas in python and i usually write a dataframe to my db table as below . i am now now migrating to django , how can i write the same dataframe to a table through a model called mymodel ? assistance really appreciated . #code
use your own pandas code along side a django model that is mapped to the same sql table
i am not aware of any explicit support to write a pandas dataframe to a django model . however , in a django app , you can still use your own code to read or write to the database , in addition to using the orm ( e.g. through your django model )
and given that you most likely have data in the database previously written by pandas ' ` to_sql ` , you can keep using the same database and the same pandas code and simply create a django model that can access that table
e.g. if your pandas code was writing to sql table ` mytable ` , simply create a model like this : #code
now you can use this model from django simultaneously with your existing pandas code ( possibly in a single django app )
django database settings
to get the same db credentials into the pandas sql functions simply read the fields from django settings , e.g. : #code
the alternative is not recommended as it's inefficient
i don't really see a way beside reading the dataframe row by row and then creating a model instance , and saving it , which is really slow . you might get away with some batch insert operation , but why bother since pandas ' ` to_sql ` already does that for us . and reading django querysets into a pandas dataframe is just inefficient when pandas can do that faster for us too . #code
thank you bakkal , yes i have been using the pandas in python directly and through qt . i was getting stuck at the setting up of the create_engine() values . i will try to set the values as picked from settings file .
awesome ! exactly what i needed ! thank you so much !

return to flat series from a pandas crosstab
i've have a time series in a pandas dataframe ( a list of ip connections with two ip addresses ): #code
flows is #code
but i'd like to get #code
thanks for your help . the goal is to feed that in a plotting library
something like this ? i use ` counter ` on the zipped values of the two columns in the original dataframe . #code
thanks for this answer , it works also , but i do prefer the one from mike with pure pandas groupby . don't you think ? ( i am not sure which one is performing better )
yes , i agree ` groupby ` is probably better . this is an alternative solution .
this works : #code
thanks you ! that perfect mike
great that it helped . btw , you have the privilege to [ upvote ] ( #url ) . ;)
oh yes . i am not so familiar with stack exchange ' etiquette ' . done

downsample pandas dataframe to smaller size
i have an arbitrary dataframe of size 2000 x 2000 . is it possible to compress this into a smaller dataframe where each element represents the mean of a small block of the original dataframe without using loops ? the block size can be anything but for the sake of this question assume it is 40 rows and 50 columns so that the resulting dataframe has 50 rows and 40 columns .
i've tried extending the answer here but something is not clicking for me . thanks !
in the answer you linked to , change the first ` n ` to ` 2000 / 40 ` and the second ` n ` to ` 2000 / 50 ` .

vectorized update to pandas dataframe ?
i have a dataframe for which i'd like to update a column with some values from an array . the array is of a different lengths to the dataframe however , but i have the indices for the rows of the dataframe that i'd like to update .
i can do this with a loop through the rows ( below ) but i expect there is a much more efficient way to do this via a vectorized approach , but i can't seem to get the syntax correct .
in the example below i just fill the column with ` nan ` and then use the indices directly through a loop . #code
is this an array or a series / df ? you could just assign the series directly : `df['newcol '] = new_values ` or construct a series : `df['newcol '] = pd.series ( new_values )` the extra rows in ` new_values ` will be ignored
the values to update are currently in an array but could be transformed if the solution requires it . maybe i'm wrong but wouldn't your solution ignore the fact i only want to update certain indices ? for example , i may want to update the 2nd , 8th , 20th .. index ( in the example these are in update_idx ) but wouldn't your approach just update the first n rows of the dataframe ( where n is the length of new_values ) ?
then i think ` df.loc [ update_idx , ' new_col '] = new_values ` should work
perfect - thanks very much . if you care to submit that as an answer i can accept it !
if you have a list of indices already then you can use ` loc ` to perform label ( row ) selection , you can pass the new column name , where your existing rows are not selected these will have ` nan ` assigned : #code
example : #code

pandas / python : replace multiple values in multiple columns
all , i have an analytical csv file with 190 columns and 902 rows . i need to recode values in several columns ( 18 to be exact ) from it's current 1-5 likert scaling to 0-4 likert scaling .
i've tried using replace : #code
but that throws a value error : " replacement not allowed with overlapping keys and values "
i can use map : #code
but , i know there has to be a more efficient way to accomplish this since this use case is standard in statistical analysis and statistical software e.g. spss
i've reviewed multiple questions on stackoverflow but none of them quite fit my use case .
e.g. pandas - replacing column values , pandas replace multiple values one column , python pandas : replace values multiple columns matching multiple columns from another dataframe
suggestions ?
what's wrong with just subtracting ` 1 ` from the column ?
no need for a mapping . this can be done as a vector addition , since effectively , what you're doing , is subtracting ` 1 ` from each value . this works elegantly : #code
or , without ` numpy ` : #code
you don't need to do this just do `df['job_perf1 '] = df['job_perf1 '] -1 `
@user aaaah , that makes sense , thanks for pointing that out . :)
now i just need to figure out how to loop through my 18 columns .
thanks !
is your entire df 18 cols ? if so then ` df = df -1 ` will just do it all otherwise , compose a list of the cols of interest and then do ` for col in col_list : df [ col ] = df [ col ] - 1 `
@user i really need to look through the docs more often . anyways , for what it's worth , pandas converts a scalar to an appropriately sized array under the hood , cf . [ pandas source ] ( #url ) .
@user entire dataframe is 190+ columns so can't do the operation on the entire data frame .
also , that code is much less complex than i imagined , is that because we are doing a column operation instead of a row or cell based operation ? ( obvi still pretty new to python / pandas )
that's correct we're performing column / series ops here , actually i think this would just work : ` df [ cols_list ] = df [ cols_list ] - 1 `
@user awesome , thanks for the insight and thanks for the help !!
you can simply subtract a scalar value from your column which is in effect what you're doing here : #code
also as you need to do this on 18 cols , then i'd construct a list of the 18 column names and just subtract ` 1 ` from all of them at once : #code

why does dask.dataframe compute() result gives indexerror in specific cases ? how to find reason of async error ?
when using current version of dask ( ' 0.7.5 ' , github : [ a1 ]) due to large size of data , i was able to perform partitioned calculations by means of dask.dataframe api . but for a large dataframe that was stored as record in bcolz ( ' 0.12.1 ' , github : [ a2 ]) i got an indexerror when doing this : #code
error was ( abbreviated traceback output ): #code
actually the error was only there when doing the dd.concat action . something like #code
was working .
but also when parts of data were read in memory this error was in some cases there , at least for partition lengths ( npartition ) > 1 and specific data sizes . #code
see full testing code _test_dask_error.py , and full output with tracebacks _test_out.txt .
actually at that step i stopped my investigation , because i have no clue how to debug this error in async.py to the root cause . sure i will report this as bug ( if there is no hint to user / usage error ) . but : how to do the debugging to find the root cause ?
_ [ a1 ]: _ #url
_ [ a2 ]: _ #url
taken from the faq of the dask documentation
q : how do i debug my program when using dask ?
if you want to dive down with a python debugger a common cause of
frustration is the asynchronous schedulers which , because they run your
code on different workers , are unable to provide access to the python
debugger . fortunately you can change to a synchronous scheduler like
` dask.get ` or ` dask.async.get_sync ` by providing a ` get= ` keyword
to the ` compute ` method :: #code
both ` dask.async.get_sync ` and ` dask.get ` will provide traceback
traversals . ` dask.async.get_sync ` uses the same machinery of the async
schedulers but with only one worker . ` dask.get ` is dead-simple but does
not cache data and so can be slow for some workloads .
comment
i'm curious to see what the issue is . if the cause is not immediately obvious after using the method above then i recommend raising an issue on the dask issue tracker .
thanks , this works . so for second question rtfm applies partly .
it's nice to have questions like this one on stackoverflow regardless , if only to point people to the right places within the documentation .
after using #code
it became clear , that the error #code
happened , because in ` _loc ` not exact boundaries are given , but ` stop ` is out-of-bound #code
out :
( 0 ,
1000000 ,
2000000 ,
3000000 ,
4000000 ,
5000000 ,
6000000 ,
7000000 ,
8000000 ,
9000000 ,
9999999 )
and in interrupted task was ( e.g . ) called #code
although last index label is 1999999 .
problem is that for ` pandas.dataframe.loc ` it is given : " allowed inputs are : [ ... ] a slice object with labels ' a':'f ' , ( note that contrary to usual python slices , both the start and the stop are included ! )" ( taken from documentation stable version , 0.17.1 ) .
apparently for small numbers no out-of-bounds error is raised , but for large numbers ( i > ~1e6 ) i got indexerror with this test : #code
with pd.dataframe.iloc this uncertain behaviour seems indeed not an issue according to documentation : " .iloc will raise indexerror if a requested indexer is out-of-bounds , except slice indexers which allow out-of-bounds indexing . " , and indeed short tests showed no irregular out-of-bounds error here : #code
it is rather sure not a proper fix for given dask problem , because ` _loc ` is written more generic , but eventually only for specific calls which are essentially #code

pandas python sorting according to a pattern
i have a pandas data frame that consists of 5 columns . the second column has the numbers 1 to 500 repeated 5 times . as a shorter example the second column is something like this ` ( 1 , 4 , 2 , 4 , 3 , 1 , 1 , 2 , 4 , 3 , 2 , 1 , 4 , 3 , 2 , 3 )` and i want to sort it to look like this ` ( 1 , 2 , 3 , 4 , 1 , 2 , 3 , 4 , 1 , 2 , 3 , 4 , 1 , 2 , 3 , 4 )` . the code i am using to sort is ` df= res.sort ([ 2 ] , ascending=true )` but this code sorts it ` ( 1 , 1 , 1 , 1 , 2 , 2 , 2 , 2 , 3 , 3 , 3 , 3 , 4 , 4 , 4 , 4 )` .
any help will be much appreciated . thanks
do you have another column with those five different kind of entries since those double ? so like a --> 1 , a --> 2 , a --> 3 , a --> 4 , ... a --> 500 , b --> 1 , b --> 2 , ... b --> 500 , ... , e --> 1 , e --> 2 , e --> 3 , ... , e --> 5 . if so , you could use this as a combined index in order to sort the data properly . please paste a small snippet of your complete dataframe or at least a significant part of it .
so i have 10 --> 1 , 10 --> 1 , 10 --> 1 , 10 --> 1 , 10 --> 2 ,..., 10 --> 2 , 10 --> 3 ,.., 10 --> 3 , 10 --> 4 ,.., 10 --> 4 , then 20 --> 1 ,......., 20 --> 4 , then 30 --> 1 ,... 30 --> 4 and so on till 100
how's about this : sort by the cumcount and then the value itself : #code
very cool answer !
awesome answer .

filtering records in pandas python - syntax error
i have a pandas data frame that looks like this : #code
if i use the following i export a new csv that pulls only those records with a distance less than 5000 . #code
now , i wish to export a csv with values from 5001 to 10000 . i can't seem to get the syntax right ... #code
you have the conditions reversed ? ( in the opp order :) )
#code
distance_2 = all_results [ 5001 < all_results.distance < 10000 ]
distance_2.to_csv ( ' . / distance_2.csv ' , " , ")
results in : valueerror : the truth value of a series is ambiguous . use a.empty , a.bool() , a.item() , a.any() or a.all() .
that is the famous error ..., and @user just provided the answer .
thanks ;) makes sense !
unfortunately because of how python chained comparisons work , we can't use the 50 x 100 syntax when ` x ` is some vectorlike quantity . you have several options .
you could create two boolean series and use ` ` to combine them : #code
use ` between ` to create a boolean series and then use that to index ( note that it's inclusive by default , though ): #code
or finally you could use ` .query ` : #code

sci kit learn regression random forest regressor implementation
i'm having a problem where my scikit learn regression forest trains almost instantly ( in a few seconds ; i know from experience with this machine it should take about a half hour or more on a dataset of the size i am working on ) and then predicts the exact same output for every row of input data .
my current theory is that it has something to with the order of magnitude of the target variables - around 10^-11 . i tried multiplying them by 100,000 to see what happened , and it starting running forever and not doing anything until i killed the script .
the code is below : #code
you're probably wondering why i ran it back on the training data - i was just trying to test if it was actually doing anything , which it isn't .
thank you for your help !
edit :
this is the describe() output for the target data . the training data is mostly similar in magnitude : #code
i tried standardizing the data and running the forest ; it printed no output but the memory usage keeps creeping up so it must be doing something . do regression forests take high computer power ? i'm using a laptop with an i7 processor and an ok graphics card ; i can classify fine .
what does your training target look like ? try running .describe() on the pandas series .
multiple things can go wrong : 1 ) make sure your x_train has the correct shape . from the documentation : ` array x of size [ n_samples , n_features ]` .
2 ) try to scale and center your data using a [ ` robustscaler `] ( #url ) or [ ` standardscaler `] ( #url ) . 3 ) some old version of joblib has known performance problems . what happens when you try with ` n_jobs=1 ` ?
i tried the n_jobs fix , and made sure x_train had the right shape . i updated the question with responses to the other suggestions .
tell us about x . how wide is x_train ? you say that you can classify fine , and one difference between regression and classification rf's in scikit-learn is the default number of features to use . in classifiers , it is sqrt ( n_features ) , but in regressors , it is n_features . so if the width of x_train is , say , 10000 , then the regressor would consider 100 times as many features to split on .
also , you are troubleshooting at this point . for now , you should use n_estimators= < 2 to 5 trees > and maybe limit max_depth , too , until you get it working , then go back to 100's of trees to get a better result .

convert list to datetime in pandas
i have the foll . list in pandas : #code
is there a way to convert it into datetime ?
i tried :
` pd.to_datetime ( pd.series ( str ))`
you have to specify the format argument while calling pd.to_datetime . try #code
this gives #code
for setting the current year , a hack may be required , like #code
to get #code

in python convert day of year to month and fortnight
i want to convert day ( say 88 ) and year ( say 2004 ) into its constituent month and nearest number out of 2 possibilities :
if the day of month ranges from 1 to 14 , then return 1 , else return 15
i am doing this : #code
however , this is only part of the way to the soln . the end result for day 88 and year 2004 should be :
march 15 ( month is march and closest day is 15th march ) .
-- edit : changed question to reflect that i mean day of year and not julian day
hang on , arent julian days absolute from sometime way back ? what does julian day 88 of year 2004 mean ? 2004 julian year ?
good catch @user , changed question to reflect that i mean day of year and not julian day
how about something like .. #code
you can use the replace method : #code
the reason this returns a new datetime rather than updating is because datetime objects are immutable ( they are can't be updated ) .
note : there's a format for that day of the month : #code
it is important to note ( i see that it confuses people new to python ) that ` datetime ` is immutable object and therefore ` .replace() ` can't modify it inplace -- it returns a new datetime object instead .
@user .f .sebastian added comment to be more specific about that . imo immutability is a little surprising ( most python objects are mutable ) ... i guess datetimes are complicated enough without every method potentially mutating them !
it is not python-specific : having a mutable date / time type is a bad design . it is not a coincidence that basic types such as ` str ` , ` int ` , ` float ` are immutable in python and many other languages ( notable exception is c / c++ where strings are mutable by default ) .
something like this ? #code
it would be easy to convert the tuple pair into something useful depending on your requirements .
for example #code
however , you have not specified the type of data you are expecting ( datetime.date , pd.timestamp , string , etc . )

chosing a different value for nan entries from appending dataframes with different columns
i am concatenating multiple months of `csv's ` where newer , more recent versions have additional columns . as a result , putting them all together fills certain rows of certain columns with ` nan ` .
the issue with this behavior is that it mixes these ` nan ` s with true null entries from the data set which need to be easily distinguishable .
my only solution as of now is to replace the original nans with a unique string , concatenate the csv's , replace the new nans with a second unique string , replace the first unique string with nan .
given the amount of data i am processing , this is a very inefficient solution . i thought there was some way to determine how panda's ` dataframe ` fill these entries but couldn't find anything on it .
updated example : #code
and append #code
gives #code
but i want #code
you need to be a bit more specific , please add some small examples of your data and examples on what you want to achieve .
updated an example of what is happening and what i would like to have happen
please add your edit to the response and accept the answer !
in case you have a core set of columns , as here represented by ` df1 ` , you could apply ` .fillna() ` to the ` .difference() ` between the core set and any new columns in more recent ` dataframes ` . #code
the issue with this is that it will replace all nan values in that column . if the 3 is nan then it would become " predated " and would not be distinguishable .
however i got the solution to work by adding column c to df1 with nan values , using fillna , and then appending .

how to replace infinity in pyspark dataframe
it seems like there is no support for replacing infinity values . i tried the code below and it doesn't work . or am i missing out something ? #code
or do i have to take the painful route : convert pyspark dataframe into pandas dataframe , replace infinity values , and convert it back to pyspark dataframe
it seems like there is no support for replacing infinity values .
actually it looks like a py4j bug not an issue with ` replace ` itself . see support nan / inf between python and java .
as a workaround you can try either udf ( slow option ): #code
or expression like this : #code
why are ` udf ` s slower than expressions ?
@user because ` dataframe ` is not a python object it requires a full round trip .
@user another aspect , which is not pyspark specific , is that udf is just a black box for optimizer . it shouldn't matter here but in general it means that you cannot reason about an operation which requires udf . finally , as far as i know , internal representation doesn't use standard scala types . so even in scala or java you may prefer using expressions directly without udfs .
thanks , the problem is that sometimes i have problems figuring out how to achieve things like that using expressions instead of ` udf ` , i asked because i had a code that converted an array of letters to a ` sparsevector ` using ` udfs ` and the code never finished
well , it is not always possible but if there is a choice then expression >> jvm-udf >> python-udf .
i will , thanks for your guidance

how to append on a dataframe with timezone aware timestamp column ?
i have a data frame with a timestamp column and a numeric column . i am able to append a new row to it if the timestamp column is timezone naive . #code
but if i set timezone for the timestamp column , and then try to append new rows , i get error . #code
any help on how can i append new rows to a dataframe having timezone aware timespamp column will be greatly appreciated .
what is your pandas version . i can run this example fine in 0.16.1 . as an aside , rather than doing apply ( pd.to_datetime ) , just do pd.to_datetime ( df ) . this line : df [ 0 ]= df [ 0 ] .apply ( pd.to_datetime ) also seems to be wrong it seems you want df['timestamp '] = df['timestamp '] . .
@user this . this is possibly my biggest gripe of pandas-wild code . i've seen things like : ` df.apply ( lambda x : x.sum() )` and worse . :/
@user , thanks for pointing out the mistake in the question . i am using pandas version 0.17.1 .

apply functon with a condition on the first row
i would like to convert data .
for instance , i would like to apply ( lambda x : x+ 273.15 ) on each columns which contain c data .
a set of data :
before #code
after #code
what would be the best solution ?
i only managed to split the dataframe into dataframes with only one unit , work on each dataframes and concatenate them all .
i guess there is a better way to do this :o
i also tried this : #code
edit : using eumiro solution #code
edit : other solution if directly read from csv file #code
why not just use ` df.temp = df.temp + 273.15 ` ?
@ anton : it's just an example here . the real data could be in several unit ( fahrenheit , kelvin etc ... ) . so i need to check the unit first to know what function use .
could you provide example with several units in ` temp ` column ?
databis=pd.dataframe({'time':['s ' , 0 ., 1 . ] ,
' temp ' :[ ' c ' , 20 ., 30 . ] ,
' pressure':['pa ' , 10^5 , 10^5 ] ,
' tempbis':['k ' , 300 ., 500 . ] } ) . it's not several units in the same column but several column in differents units .
how are you getting that dataframe ? if you use ` read_csv ` you could pass for ` header ` two rows , then your units will be in 2nd level of header .
yes thanks ! i don't think to use that > . <
you don't need to use drop in case you using ` read_csv ` with ` header =[ 0 , 1 ]` because first row already read as header . just delete that line
consider to accept @user or mine solution if any of them helped you
increasing a celsius column by 273.15 ( ` x = x + 273.15 `) makes it a kelvin column without updating the description , so at some point your data is inconsistent .
the best solution is not to put units into the first row of real data at all . can't you name your columns ` temp [ c ]` or ` temp c ` and test the name of the column ? #code
` df ` is now : #code
yes i already think about this solution but i need something more powerfull .
@user : mixing metadata and data is not more powerful . it is more wrong .
i can't choose the input format . but maybe i could manage to modify the dataframe to fit your example ?
@user - dataframes are great for homogenous data , such as columns full of float values . putting a string into the first row breaks the whole advantage of dataframes . you even cannot add a number to the whole column .
i understand your point of view but it's not entirely true . i can add a number to the whole column using a function with exeption . i just need to know how to filter the column with a condition on the first row . however , i am trying your solution . i still have to fit the right format .
it's not a good idea to store units in first row . you could do 2 level header for that and then operate with them as with usual numeric columns . first you need to do 2 level header , then drop 1st row and cast ` reset_index ` to make it from 0 ( or you could omit that if you are fine with index starting from 1 ): #code
note : if you are reading dataframe with ` read_csv ` function you could pass ` header =[ 0 , 1 ]` . then you'll have 2 level header
thanks for your help !

iterating over date range between two pandas dataframes for category count
i have two pandas dataframe ( df1 and df2 ):
df1 has 12 columns , where a1 , a2 , ..., a9 are empty columns . below is a sample for df1 : #code
df2 has 4 columns . below is a sample : #code
now , i would want the output to be like this , df1 : #code
i.e for each stock , start_date end_date combination of df1 , the result should have count of each category in that date-time range from df2 .
here in final output , a1 = count [ opening ( 0-100 ) closing ( 0-10 )] , a2 = count [ opening ( 101-200 ) closing ( 11-20 )] , a3 = count [ opening ( 201-400 ) closing ( 21-50 )] , a4 = count [ opening ( 0-100 ) closing ( 11-20 )] and so on , all 9 combinations .
i've the r code for this but works poorly for a bigger data set.can anybody help me with how to do this in python / pandas . any help is appreciated !!
you can try this solution , where i remove empty columns of ` df1 ` , but it works with them too : #code
how does it work ?
thanks , works perfectly fine . just one more thing , what if i've another column ( double or float ) in df1 . is it possible to get that in the final output by changing the " how " in merge ?
i think ` on ` in function ` merge ` is for matching - better example with pictures is [ here ] ( #url ) . ` df = df1.merge(df2,on='stock ' , how='left ')` is same as ` df = pd.merge ( df1 , df2,on='stock ' , how='left ')` .
thanks , help appreciated . works similar to merge function in r . is there a way to add another column ( with values ) present in df1 in the final output() ?
hmmm , i think if you merge ` df1 ` and ` df2 ` by ` stock ` , you get all other columns from df1 , so another columns too . try it .
oh yes , it worked thankfully . cheers

replace numeric constant in pandas panel
i have a panel of data that contains a rolling daily correlation matrix of two identical dataframes with multiple columns . i'd like to calculate some daily descriptive statistics but want to ignore correlation coefficients of 1.0 ( since i'm correlating the dataframe with itself ) and can't figure out the best way to do so . the replace method doesn't seem to be working for me . how best to go about this ? #code
for what it's worth , i gave up and ended up looping through each value and replacing self correlation values with nan . #code

accelerate the speed of loop over pandas groupby
i have a big data frame in which contains many subsets . for instance , #code
i want to accelerate the loop so i tend to use multi_threading . but i wonder if there are another approach ? because if i use threading like code below , i worries many threads read ` gr ` sametime #code
could you please give me any advices ? thanks in advance
i don't think this is going to work , multi-threading isn't a magic bullet ... have you tried [ dask ] ( #url ) .
thanks , i didn't know it . i am investigating it .

pandas dataframe.to_latex() doesn't recognize column_format
i am creating latex tables from pandas dataframes using the .to_latex() command . according to its documentation it has a column_format keyword argument , but when i try to set it to ' cccc ' i get an error : #code
why does this happen ?
edit : the error message i get is actually #code
could you show your dataframe which you are trying to save ?
show your code , i don't see ` cols_format ` as an argument .
the keyword argument is ` column_format ` not ` cols_format ` .
yes , i'm using column_format , i don't know why i wrote cols_format in my question .

how to create a dataframe out of rows while retaining existing schema ?
if i call map or ` mappartition ` and my function receives rows from pyspark what is the natural way to create either a local pyspark or pandas dataframe ? something that combines the rows and retains the schema ?
currently i do something like : #code
in order to create a spark sql dataframe you need a hive context : #code
with the hivecontext you can create a sql dataframe via the inferschema function : #code
does this work on slave nodes ?
good point . this only works on a rdd . so you would invoke it for your variable ' combine ' before you call the mappartition .
alternatively , it would be even better if you would read in your data as dataframe right away . you can do that for several input sources among them are json , hive tables and many more
i have a dataframe but when i call mappartition each slave node sees a row iterator , for convenience i'd like to combine these rows
ah , now i understand . unfortunately , i can't think of a good solution . which dataframe operation do you want to apply if you had the rows combined into a dataframe ?
ultimately topandas and then a pandas function . the approach sketched above works but it feels hacky
you could use ` topandas() ` , #code
that doesn't answer my question i need it to operate inside a map call on a partition . if there's a map that passes dataframe that would be good too .
sorry , i couldn't understand ` map that passes dataframe ` . what is expected as output of spark dataframe ? you want to create dataframe for each partition ?
mappartition passes a row iterator for each partition , so i can't use dataframe functions
what is the natural way to create either a local pyspark
there is no such thing . spark distributed data structures cannot be nested or you prefer another perspective you cannot nest actions or transformations .
or pandas dataframe
it is relatively easy but you have to remember at least few things :
pandas and spark dataframes are not even remotely equivalent . these are different structures , with different properties and in general you cannot replace one with another .
partitions can be empty .
it looks like you're passing dictionaries . remember that base python dictionary is unordered ( unlike ` collections.ordereddict ` for example ) . so passing columns may not work as expected .
#code
thank you the explanation helps . the approach is similar to what i'm using now , but is there a natural way to pass the row schema other than just column names ?
i am not sure if i understand the question . pandas dataframes use columns and dtype parameters which can be passed in a closure but it is not something that will be recognized by spark . if you want a spark dataframe you should pass this ` createdataframe ` and pass schema ( which is different than pandas dtypes ) there .
i want to maintain the schema , in the same way i'd imagine the normal topandas does . calling createdataframe and then topandas would be fine if i knew how to call createdataframe and maintain the row schema . although i'm guessing there might be a loss of efficiency ?
` topandas ` simply collects and creates local data structure with the same ` columns ` names as spark data frame . nothing more , nothing less . ` row ` ( as ` pyspark.sql.row `) has no schema - it is just a ` tuple ` with a few added methods and ` __fields__ ` attribute which stores names .
interesting , topandas doesn't also enforce column types ? and types aren't built into the rows ?
no , it doesn't .

display progress of lambda function
i'm calling a lambda function getidfromurl that takes around 1 hour to complete , as the dataframe contains almost one million lines . how can i monitor the progress of this lambda function ? #code
you can't , really . if you made it a non- ` lambda ` function you could do some logging in it .
why can't you display progress from within getidfromurl() ?
@user well , technically , in python3 , you could : ` lambda x : x if not print('progress ') else none ` . which should never be done in real code , ever , of course .
@user please mail your pythoning license back to gvr ;o )
@user happy christmas to you , too !

how to reindex a a pandas dataframe by date range without deleting values
background :
i have the following data frame downloaded with pyodbc with dates between 1999 and 2015 : #code
i then filtered the data for all dates greater than 2010-01-01 and sorted by ascending dates : #code
i then created a date index with values between 2010-01-01 and today with pandas's date_range function : #code
and re-indexed the data frame #code
my problem is that when i re-indexed the data frame , all of the data was deleted : #code
from the original filtered data frame you can see that there were transactions on the 2010-04-01 #code
question
how do i re-index this data frame with this date range and keep all of the original values ? i am trying to create a common index on several different data frames from difference databases to add together into an aggregated data frame . your help would be greatly appreciated . thanks !
you're re-indexing by a datetimeindex when the index is not a datetimeindex : #code
hence the nans and nats .
perhaps you want to make ` order_datec ` the index : #code
then to resample .
if you reindex , you're going to lose the rows with duplicate dates .
would i resample the date_index or ceitest first ? could you give me an example of how to resample these data frames ? thanks for your help andy !
@user this is * instead * of ` date_index ` . once you have a datetimeindex you can do ` df.resample ( " d " , how= " sum ")` or similar . look up how to resample separately . it's similar to a groupby , you can also do ` df.groupby ([ pd.timegrouper ( " d ") , " regionc "]) .sum() ` etc etc .
thank you andy !
i think you need set index from column ` order_datec ` before reindex : #code
and final you can check ` notnull ` values by ` isnull ` with ` any ` : #code
all together : #code
#code
there can be many ` nat ` and ` nan ` , check data : #code
in the end you can set index name and ` reset_index ` index - column name is index name : #code
thank you for the response jezrael . i tried your code and print ceifinal.head() returned an empty data frame .
hmmm , maybe you can check both indexes : ` print ceitest.index ` and ` print ceifinal.index ` ( before reset index )
ir returns with this sample : `datetimeindex(['2011 - 10-11 ' , ' 2014-01-30 '] , dtype='datetime64 [ ns ]' , name=u'order_datec ' , freq=none )
datetimeindex(['2010 - 01-01 ' , ' 2010-01-02 ' , ' 2010-01-03 ' , ' 2010-01-04 ' ,
' 2010-01-05 ' , ' 2010-01-06 ' , ' 2010-01-07 ' , ' 2010-01-08 ' ,
' 2010-01-09 ' , ' 2010-01-10 ' ,
...
' 2015-12-14 ' , ' 2015-12-15 ' , ' 2015-12-16 ' , ' 2015-12-17 ' ,
' 2015-12-18 ' , ' 2015-12-19 ' , ' 2015-12-20 ' , ' 2015-12-21 ' ,
' 2015-12-22 ' , ' 2015-12-23 '] ,
dtype='datetime64 [ ns ]' , length=2183 , freq='d ')`
thank you jezrael !
you can upvote my answer . thanks .

how to slice pandas data frame by column header value when the column header is a date-time value ?
i have an excel file where the column name consists of date-time value .
as you can see the header value is in date-time format . i have loaded this to pandas dataframe and the header values are indeed saved as date-time value .
now if i need to query from pandas such as , " pick all columns which are greater than may-15 " how can i do that ?
i am aware that by querying ` df [ df.columns [ 3 :]] ` i can achieve this . but i really want to slice based on the value of column header and not based on the position of the column .
please help .
i don't know panda , but you can try doing a filter of the headers that are date-time and if those are comparables , compared it by the value you desire ...
@user any updates ?
@user just updated my answer remembering that you can convert columns into datetime objects and use conditionals as you would normally !
querying works best to filter observations ( rows ) , based on one or more variables ( columns ) . the way your data is organized doesn't allow for a natural query ( you're trying to filter columns as opposed to using them as criteria in the filter ) . you can read more about tidying dataframes here
of course you can come up with a contorted way to do what you want , but i'd strongly suggest you tidy your data like this #code
then you can easily query based on the ` date ` , and make sure that column is of a date type so you can use e.g. #code
then when you're done and if you have to , you can always bring back the dataframe to its original format if required ( if you can , better to avoid it and focus on organizing your data , it pays in the long run )
thanks for the answer . this is what i am trying to achieve . i want to transform the data with year and month as separate columns . the data format i provided could run for upto 5 years ; so 5 * 12 columns . i want to transform this into 13 columns ( 1 column for year and 12 for months ) . so i am trying to find ways programmatically .
one easy-fix method would be to replace the month string with its equivalent number . #code
this should make your dates in the form 15.04 , 15.05 .. 16.11 etc .
alternatively : you can also convert your headers into date-times and query them in that way : #code
thanks . your tip lead me to a solution . df.loc [: , df.columns <= datetime ( 2015 , 5 , 1 )] did not work because of the non-datetime columns . if you filter it out it works . df.loc [: , [ col for col in df.columns if col not in ( " name " , " location ") and col > = datetime ( 2015 , 4 , 1 ) and col <= datetime ( 2016 , 3 , 1 )]]

pivoting a pandas dataframe containing strings - ' no numeric types to aggregate ' error
there is a good number of questions about this error , but after looking around i'm still not able to find / wrap my mind around a solution yet .
i'm trying to pivot a data frame with strings , to get some row data to become columns , but not working out so far .
shape of my df #code
sample format #code
what i would like to pivot to #code
where the question values become the columns , with the response_answer being in it's corresponding column , and retaining the ids
what i have tried #code
dataerror : no numeric types to aggregate
what is the way to pivot a data frame with string values ?
what do you want to achieve by pivoting string values ?
there's something off about this data . why is the same contact_id answering the same question multiple times . are you grouping by contact_id or something else ... what ??
the goal is to ultimately create a reporting table for use in tableau ; put the data in a form where more questions can be answered .
as far as the data , there are missing details that could be added in ; the multiple contact_ids are from multiple surveys - those survey ids are not listed
the default ` aggfunc ` in ` pivot_table ` is ` np.sum ` and it doesn't know what to do with strings and you haven't indicated what the index should be properly . trying something like : #code
this explicitly sets one row per ` id , contact_id ` pair and pivots the set of ` response_answer ` values on ` question ` . the ` aggfunc ` just assures that if you have multiple answers to the same question in the raw data that we just concatenate them together with spaces . the syntax of ` pivot_table ` might vary depending on your pandas version .
here's a quick example : #code
the index should be perhaps id , because it is the unique identifier created by select row_number()
over() as id
to give each row an unique int .
if i added the survey id in addition to the contact_id then those would be unique , and i could dispense with the generated id
you can index the pivot in whatever manner you think matches the level of aggregation you desire . just set the ` index ` arg to the specification you want .
the indexes for the purposes of this question is id and contact_id
then i want the response_answer strings to be listed in the data frame values under their pivoted question column headers to create a sql table from
a higher level question could be , is pandas the best python tool to pivot tables with string data ; it seems it might be ...
that's the exact thing the first snippet in the answer does ... give it a try and see if it satisfies your desire .
final solution form that works well ( i added the screening_id to basically form a unique composite index of the contact_id and screening_id ): pivot_table = unified_df.pivot_table(index=['id','contact_id','screening_id '] , columns='question ' , values='response_answer ' , aggfunc=lambda x : ' ' .join ( x ))

parse lists in pandas columns
i am trying to figure out how to parse pandas columns containing lists : my problem is that these are recognised as strings , whereas i would like them to be treated as lists , to iterate through them .
this is an example of my cells : `[('p105 ' , 1 ) , ( ' p31 ' , 1 ) , ( ' p225 ' , 1 ) , ( ' p70 ' , 1 )]`
when i try to iterate through it , i only get the characters contained in the string one by one ( i.e. [ , ( , ' , p etc . ) .
how do i make pandas ' understand ' that these are lists ?
edit : i have found a way to do that : i apply ` ast.literal_eval ` to each line .
example : #code
if i use ` ast.literal_eval ` , instead : #code
my doubt now is how efficient this approach will be to process millions of lines .
can you clarify what exactly you are doing . if you save the above as a list , then make it into a dataframe , you get two columns . you can then happily iterate between each one as you please . ie pd.dataframe ( [( ' p105 ' , 1 ) , ( ' p31 ' , 1 ) , ( ' p225 ' , 1 ) , ( ' p70 ' , 1 )])
i have a data frame of which two columns were saved as python lists . to run my analysis , i have to select the tuples in each row's lists in which an element is present , e.g. : finding p105 in [( ' p105 ' , 1 ) , ( ' p31 ' , 1 ) , ( ' p225 ' , 1 ) , ( ' p70 ' , 1 )] .
now the problem is that i cannot iterate through the list , but only through its characters .
i would personally split that into further columns and iterate over them : #code
or #code
thanks . however , this splits also the tuples within the lists . i will try to change it , in order to avoid that .
@user try : df['col '] .apply ( lambda x : pd.series ( x.replace ( ') , ' , ') && ' ) .split ( ' && ')))

pass plot to function matplotlib python
i was hoping to create a function that will set the x axis limit for a plot . here is what is working currently : #code
here is what i was hoping for : #code
however , there is an issue in ` set_limits ` with ` plot.xlim ( min , max )` , #code
how can set_limits be modified in order to fix this ?
how did you import ` matplotlib ` ? e.g. where does ` plt ` come from ?
@user sorry about that , question is now fixed to reflect that
the problem is that ` pyplot ` is a stateful module which is meant to be convenient for people who are used to matlab . in particular , the statefulness is why ` plt.set_xlim ` works -- ` pyplot ` has a reference to the figure ( and axes ) that you're currently working with . if you want to work with the plots themselves , this doesn't work ( well ) and you'll need to use the object oriented interface provided by ` matplotlib.figure ` . [ reference ] ( #url ) . i suppose you _could_ use ` pyplot.gcf() ` to get the current figure ...
the problem is that the object passed into the ` set_limits ` function as written as used is the ` collection ` artist returned by scatter which does not have any notion of limits .
you probably want to be doing something like this : #code
if you are not varying the size or color of the markers you are better off using ` ax.plot ( ..., linestile='none ' , marker='o ')` which will render faster . in this case something like ( if you have 1.5.0 + ) #code
and it should ' do the right thing ' .

concatenating / merging data frames and editing column names - python / pandas
i have built a dataframe out of a python dictionary , with the following command : #code
that gave me the following frame ` population ` : #code
out of that dataframe i created another one with its moving average , using the following command : #code
that gave me the following frame ` population_movav ` : #code
i want to combine them so they get like this : #code
synthesizing , i need to concatenate them and change the column name of the variable ` population_movav ` . tried the ` pd.concat ` but for some reason it is not working out right .
can someone shine some light on it ?
you need to use ` pd.concat ` with ` axis=1 ` and then rename your last column to ' population_movav ' : #code
edit
if you need to change only the last column you could do following : #code
would you mind adding the renaming command for the last column ?
you can use ` join ` with ` rsuffix ` : #code
you can add a new column simply by referencing it by name in an assignment : #code
gives you #code
if it work , it is the best answer . mayby you can change column name to ` population_movav `
i edited the label to match the example

pandas - read_csv with missing values in headline
i have this kind of csv file : #code
so the first row does not explicitly specify all columns , and kind of assumes that you understand that the date is the first 3 columns .
how do i tell read_csv to consider the first three columns as one date column ( while keeping the other labels ) ?
maybe this other thread will help : #url
you can parse your columns directly as a date , if you use the ` parse_dates ` argument .
from the docs :
parse_dates : boolean , list of ints or names , list of lists , or dict , default false
if true -> try parsing the index . if [ 1 , 2 , 3 ] -> try parsing columns 1 , 2 , 3 each as a separate date column . if [[ 1 , 3 ]] -> combine
columns 1 and 3 and parse as a single date column . { foo : [ 1 , 3 ] } ->
parse columns 1 , 3 as date and call result foo a fast-path exists
for iso8601-formatted dates .
for your file , you can do something like this : #code
the thing with the missing values in headline is solved by passing the ` names ` argument and ` header=0 ` ( to overwrite the existing header ) . then it is possible to specify which columns should be parsed as a date .
see another example here .
that's where ` parse_dates ` comes in : #code

python pandas least extreme of three row dataframes
i have 3 rows of ` dataframe ` each stored in separate variables . how can i create a new ` pandas ` ` dataframe ` so that it contains the least extreme element of each ` columns ` ?
so if i had : #code
i want back something : #code
i can do it with loops but was hoping for a more elegant / efficient solution because this can get big in terms of number of elements in each row .
thanks
there's a singe row in each dataframe ?
yea only 1 row in each dataframe
is the column with dates significant ? in other words : will you have to also find the minimum date ? the solutions posted won't do that if you need it .
creating sample data - three single-row ` dataframes ` with positive and negative values , and use ` pd.concat() ` to combine for calculation : #code
apply function to each column that returns the value associated with the ` index ` of the ` min ` ` abs ` value like so : #code
will this will work if the date column is a datetime dtype ?
` date ` is the ` index ` in the example , and the op is looking for values closest to zero in the ` columns ` . it's also not quite clear how to determine the ' least extreme ' date , doesn't necessarily have to be the minimum date .
yes , thanks , it will ignore the index . should have confirmed it myself before asking the question , but the fingers type faster than the brain thinks sometimes ... ;-)
you can temporarily concatenate them , then take the index of minimum of the absolute values : #code
will this will work if the date column is a datetime dtype ?
it looks like the dates are index values , so the abs and min functions aren't applied to the index .
i really like your answer , but when i reconstructed the dataframe and used your answer like this : df.ix [ df.abs() .idxmin() ] it didn't work . first , when the index was a datetime it threw an exception , and then when i dropped the date column altogether it didn't handle the nan properly .
would you post the code you used to reconstruct the dataframe ?
sure . see my post . thanks !

dates to durations in pandas
i feel like this should be done very easily , yet i can't figure out how . i have a ` pandas ` ` dataframe ` with column date : #code
i want to have a columns of durations , something like : #code
my attempt yields bunch of 0 days and ` nat ` instead : #code
any ideas ?
` timedeltas ` are useful here : ( see docs )
starting in v0.15.0 , we introduce a new scalar type timedelta , which is a subclass of datetime.timedelta , and behaves in a similar manner , but allows compatibility with np.timedelta64 types as well as a host of custom representation , parsing , and attributes .
timedeltas are differences in times , expressed in difference units , e.g. days , hours , minutes , seconds . they can be both positive and negative .
#code
you could : #code
alternatively , you can calculate the difference between points in time using ` .shift() ` ( or ` .diff() ` as illustrated by @user hayden ): #code
to get : #code
you can convert these from ` timedelta64 ` ` dtype ` to ` integer ` using : #code
you can use diff : #code
` df.date [ 1 :] - df.date [: -1 ]` doesn't do what you think it does . each element is subtracted by series / dataframe index mapping , not by location in the series .
calculating ` df.date [ 1 :] - df.date [: -1 ]` does : #code

stripping all trailing empty spaces in a column of a pandas dataframe
i have a pandas df that has many string elements that contains words like this : #code
which has many leading white spaces in front of it . when i compare this string to : #code
i realized that the comparison was false due to the leading spaces .
although i can solve this by iterating over every element of the pandas df , the process is slow due to the large number of records i have .
this other approach should work , but it is not working : #code
so when i inspect an element : #code
it returns : #code
what's going on here ?
replace your function with this : #code
you almost had it right , you needed to get rid off the '' inside strip()
alternatively you could use ` str.strip ` method : #code

why doesn't blas work in anaconda3-2.4.1-windows-x86_64 ?
#code
when i install anaconda3-2.4.1-windows-x86_64 the result is ` true ` and i find that numpy runs so slow .
when i install anaconda3-2.3.0-windows-x86_64 the result is ` false ` and numpy works well .
in anaconda3-2.4.1-windows-x86_64 , ` numpy.__config__.show() ` outputs : #code
in anaconda3-2.3.0-windows-x86_64 , ` numpy.__config__.show() ` outputs : #code
how to fix it ?
which version of numpy ? did you have mkl installed ?
the version of numpy in anaconda3-2.4.1-windows-x86_64 is 1.10.1 , and 1.9.2 in anaconda3-2.3.0-windows-x86_64 .
how to tell whether mkl is installed ? @user h
looks like it's not . this might be just a regression in numpy . create two different environments which each version of numpy and report back if you see a similar discrepancy .
you can install the mkl optimizations using ` conda install mkl `

stringio and pandas read_csv
i'm trying to mix stringio and bytesio with pandas and struggling with some basic stuff . for example , i can't get " output " below to work , whereas " output2 " below does work . but " output " is closer to the real world example i'm trying to do . the way in " output2 " is from an old pandas example but not really a useful way for me to do it . #code
they seem to be the same in terms of type and contents : #code
but no , not the same : #code
more to the point of the problem i'm trying to solve : #code
` io.stringio ` here is behaving just like a file -- you wrote to it , and now the file pointer is pointing at the end . when you try to read from it after that , there's nothing after the point you wrote , so : no columns to parse .
instead , just like you would with an ordinary file , ` seek ` to the start , and then read : #code
uh , actually with an ordinary file i don't seek ( explicitly , at least ) , i just type " read_csv ( file )" . but thanks , that works !
@user : i mean " like you would with an ordinary file that you're trying to read from after you've written to " .
ok , thanks . i did try closing it but that didn't help .

pd.apply ( pd.series.interpolate ) with more arguments
how do i rewrite #code
so it can achieve the same functionality as #code
thanks in advance .
you need to supply a function , and pick the axis ( 0 for columns ): #code
you could pass an argument to ` apply ` : #code
or #code
anton , why there is a " , " after ' nearest ' in args()
@user to make it tuple , if you'll pass `('nearest ')` it will interpret as 7 arguments because it'll try to split that string . actually you could try both ways and you'll see . `type(('a '))` and `type(('a ' , ))` will give ` str ` and ` tuple ` accordingly .
thank you so much

loop inside a list containing filtering condition and filter the dataframe in python pandas
i have a data-frame and i want to filter it by some conditions .
conditions are like #code
now i am doing something like from these conditions identifying main condition and its sub conditions . #code
now i want to filter out the data-frame by a main list and then by its sub list by looping over the list . ( bcoz i am not clear about list compression technique ) . #code
now i can read the main filtering condition from the list by #code
and i am able to get its sublists by #code
but i am not able to get a way to filter out the data-frame in-between .
but i can a think of a way like defining a function out side which will get the df and the condition then filter it and return ( but not successful ) #code
but i think there is definitely a better way to achieve that ( may be a yield or a multiprocessing returning dfs at last and merging . ) .or filter out the data-frame inside the loop itself.for each sub condition they should get their main condition filtered data-frame , which is obvious .
kindly help me with the code .

writing to excel with pandas
i have 3 lists of 192 values . i've just learnt about pandas and extracting to excel but still got problems working with it .
so basically what i'm doing now is making a numpy array with those 3 lists in it . after that i want to make 3 columns in excel , every column one list . could somebody help me adjust my code ?
error i'm getting : #code
example of what those 3 lists look like : #code
` all-results ` has only one row , so 1 is out of bounds
you need to change how you are creating ` allresults ` . instead of list of list you just need one list . delete extra brackets ` [ ` and `]` : #code
looks like you've made a list of list , which you didn't want . so change #code
to #code

how to add row to pandas dataframe with missing value efficiently ?
suppose i have a many rows with different column names , how to add them to a pandas ' dataframe more efficiently . #code
i want the result like this : #code
there are two ways :
1 . #code
get all unique column names first , then
#code
as the first way need ` join ` , so should way 2 be faster than way 1 ?
or there are any better solution ?
you will get ` nan ` rather than a ` 0 `
that's not a big deal , just ` d.fillna ( 0 )` .
you also could try to use ` pd.concat ` and ` combine_first ` . your 2nd method isn't working properly ( or may be i missed something ) . results : #code
benchmarking : #code
from these method ` pd.concat ` is faster
thanks , ` pd.concat ` is really faster .

pandas : read head and middle of a file
i just started with pandas in python and so far very good .
i have a big cvs file and i want to read only a portion of it . by the read_csv documentation ( link ) there's the option ` skiprows ` , which says :
skiprows : list-like or integer , default none
line numbers to skip ( 0-indexed ) or number of lines to skip ( int ) at the start of the file
at first i thought that i could use this to read a 1st portion of my cvs file , process it , then read the 2nd portion and so on . but , when i read the 2nd portion , the headers are not there ( because the header line was skipped ) .
i tried ` header=0 ` but as the documentation states :
header=0 denotes the first line of data rather than the first line of the file .
then i saw that it's possible to read chunks of file . sounds great , however the documentation is not that clear to me , so here are my questions :
for each chunk , does the line index continue from value of the previous chunk plus 1 , or does it restart to zero ?
is the header set for each chunk ?
is it possible to use the ` read_csv ` command with the ` skiprows ` option , and still read the head in the first line of the file ? ( i could still open the file , read the first line and use it as header in the ` names ` option , but i don't really like this ) .
check this [ examples about chunking ] ( #url ) in the docs .
answers :
it's restart to zero with each chunk
yes
no
for question 3 you could use following , to keep the first row for header : #code
as @user pointed out in the comment take a look to the example with chunks . example : #code
thanks . i saw the example , but the explanation does make it explicit that the index restarts at 0 for each chunk . i would prefer to rely in a proper documentation rather than a example . the same goes for the header : the example it's not clear in saying that the headers ( text columns ) are the same for each chunk . in the example , it only shows numbered columns , which is not my case .
sorry , i just saw that you provided ` columns ` in the ` dataframe ` . but still , the header is not the one in the file .
as for your last edit , i'll try it . :)
what do you mean that header is not the one in the file ? do you have multiple headers ?
sorry , the ' not ' word should not be there .
what i wanted to point out is that you are providing the header as a parameter by using `columns=['a','b','c','d','e ']` . but the header is always the first line of my file .
but in ` data ` it's first row . you could verify that from the above example with ` pd.read_csv ( stringio ( data ) , sep= ' \s+ ' , header=none )`
now i got it !! sorry for the confusion . i'll try in my solution and if works i'll accept your answer . thanks ! :)

how does the plotting function of pandas and matplotlib sync ?
i'm new . look at the snippet of the code that results in a graph . #code
my question is - how plt.show() ( matplotlib.pyplot ) is getting the values of x and y when plt is not being called with any parameters ? the plot function is of the dataframe object . does it store the value somewhere default from which plt can get the values ?
` plt.show() ` does not ' plot ' the values , it will show the current already creaetd figure ( s ) . and it is `df['high '] .plot() ` which creates this figure under the hood .
could you post it as answer so that i could accept ?
all of the pandas plotting functions are implemented by calling out to mpl .
` plt.show() ` does not ' plot ' the values , it will show the current already created figure ( s ) .
it is `df['high '] .plot() ` which creates this figure under the hood . the pandas plotting functions are implemented by calling out to matplotlib . by default , it will create a new figure , unless the specify with the ` ax ` keyword argument a subplot on which to add the plot .

adding sheets to existing excelfile with pandas
i have two excel files . a participant completes the experiment and the results should be given in into the two excel files on another sheet . this should happen for every new participant . the sheetname is 001 , 002 , ..., dependent on what the participantnumber is . but the excelfiles just keep getting overwritten .
the code i'm using : #code
so basicly it always creates those 2 excelfiles with the correct sheet name but except of adding a new sheet , the existing sheet just gets overwritten by the new sheet . how do i solve that ?
edit : i found out that using a workbook of openpyxl , i can use create_sheet after loading in a workbook to get a new sheet in it but them i'm stuck on how i can edit that exact sheet with the ` pandas.dataframe ` i created .
you should create an ` excelwriter ` object and use it to save the dataframes . you will need to call its ` save() ` method for the excel file to be actually saved . #code
i tested it , but it still doesn't create a new sheet in de excelfile , just overwrites the existing sheet ; probably because everytime the experiment runs , the whole code runs and ew creates a new excelfile that has the same name of an already existing excelfile . that gets overwritten with a new excel file . what you gave me would help if i could create an ew , run experiment x times and add x dataframes to ew , then save ew . but the experiment runs x times with everytime making an ew and an ew2 ( because of 2 excelfiles i need ) . i should open ( or create ) an excelfile with name ' x ' and add a sheet .

find how many weeks passed in python pandas
i have a pandas dataframe with a datetime column . my problem is the following :
i have a starting date of 04 / 0 8/ 2014 . since then , i count weeks in chunks of 16 weeks . so , from 04 / 0 8/ 2014 until 11 / 0 8/ 2014 , it will be week 1 . after 16 weeks , it will start again from week 1 . i want to create a new column where it will find the week of the current chunk based on the datetime column .
this is what i have done , but it seems that it doesn't work as it should . #code
i calculated the number of days between the two days , then divided by 7 days to find the number of weeks and then divided by 16 to find the week of chunk .
if i use a date of 23 / 12 / 2015 , it should be week 9 . but , the above code seems wrong .
your ` df ` isn't defined . you should post an [ mvce ] ( #url )
if you need the week in a period of 16 , you need the modulo , not devision . so change " / " to " % " . and get int() before that . #code
p.s. but the first week would be 0 , not 1 .
here is a way to do it using built-in numpy / pandas time series functionality without using modulo operator : #code

error using simple pandas script
i'm trying to run a simple script to sort a dataframe by a column but am getting thrown off by this error .
the following code i'm trying to run : #code
this is the read out #code
it's not letting me post as it's saying i have mostly code , even though it's mostly error log , i'm going to babble down here until i have enough text
tbh , i'm no [ panda ] expert but if ` ascending =[ false ]` then why is ` inplace=true ` and not ` inplace =[ true ]` ?
@user , inplace parameters expects only a bool while ascending and by are allowing a list . there would be an error if both list were not having the same sizes .
do you have a space after the prediction word in your excel ? that would explain the keyerror . and according to the way the print is done it looks like it ( last char of the column name should be aligned with the last char of the values )
thanks man , this was the problem
sort_values is depreciated ! use df.sort instead . #code
also the inplace argument needs a boolian false not a list [ false ]

get hour from column pandas
i have a hard time with pandas timestamp column .
i have a dataframe with the following format ( #url ) . how can i get the hours from this column ?
thanks !
starting with time in ` string ` format , convert to ` timestamp ` and extract hour using ` .dt .hour ` : #code
depending on your data - which you should show to facilitate a more targeted response - you may only need the second part .

filter / update python dataframe by data type
i'm in the midst of trying to parse csv files for input to a sql database and am having just a bit of trouble trying to manipulate a data frame to account for various data types .
i have dataframe that has columns like so : #code
the column for " value " contains both numeric data and text data . i basically want to create 2 new columns in the data frame called ` value_num ` and ` value_text ` . for those values in the value column that are numeric , i'd like to copy them into the new ` value_num ` column leaving ` value_text ` null , for those that are text i'd like to copy them into the ` value_text ` column leaving ` value_num ` null .
i would then like to delete the old value column .
can you give some sample code ?
if i understand your question this should work : #code
using apply to go through row by row and test whether the value is numeric of string is the quickest way separate them . #code
you can use the vectorized ` str ` function and check if the string is numeric . if it is a number , it will return nan because it is not a string , so that needs to be filled with true . a string number ( e.g. ' 3 ') will also be selected , so the entire column needs to be converted to floats using ` .astype ( float )` .
you then use the tilda negation operator to check for the opposite condition , i.e. not numeric . #code

seasonal_decompose on pandas dataframe with time series - python
i have a pandas dataframe with a few time series , as you can see below : #code
i want to decompose the first time series " divida " in a way that i can separate its trend from its seasonal and residual components .
i found an answer here :
time series decomposition function in python
i'm trying to use the following code : #code
however it keeps getting me this error : #code
can someone shine some light on it ?
what's your ` divida.index.dtype ` ? it should be a datetimeindex
works fine when you convert your ` index ` to ` datetimeindex ` : #code
access the components via : #code
quick question : how do i access that result ? i'm only getting the
never mind , found out : s.resid , s.seasonal , s.trend

pandas add unique count column
given the following data frame : #code
i would like a column ( ' c ') of the count of unique values for ' b ' per group of ' a ' like this via a lambda x function : #code
thanks in advance !
how do you get 1 unique value for b for the " bar " group ?
@user , i thought the same , i think it is the count of each element per group , two 2's and two 4's in the first and one 5 one 4 etc .. in the next . like ` df.groupby ( " a ") [ " b "] .value_counts() `
if pc is right about your goal , maybe #code
would give you what you want ? we're grouping on ( a , b ) pairs .
cute bit of history : this is originally what i'd done , but then i tried it again and found i didn't need the ` [ " a "]` . but the reason it worked the second time without it is that i had the ` c ` column then , so there was something for the code to act on .. ( sigh )
i think you nailed it .
i had to ` df [ " c "] = df.groupby ([ " a " , " b "]) [ " a "] .transform ( " count ")` to avoid a ` valueerror : wrong number of items passed 0 , placement implies 1 `
@user : aargh , i know why . i'll put it back .

skip rows with bad dates while using pd.read_csv
i'm reading in csv files from an external data source using ` pd.read_csv ` , as in the code below : #code
however , somewhere in the csv that's being sent , there is a misformatted date , resulting in the following error : #code
this causes the entire application to crash . of course , i can handle this case with a try / except , but then i will lose all the other data in that particular csv . i need pandas to keep and parse that other data .
i have no way of predicting when / where this data ( which changes daily ) will have badly formatted dates . is there some way to get ` pd.read_csv ` to skip only the rows with bad dates but to still parse all the other rows in the csv ?
check ` skiprows ` parameter of [ ` read_cvs `] ( #url ) . you could pass a list of row numbers with bad date , but you need to know row numbers .
what are the potential formats ?
somewhere in the csv that's being sent , there is a misformatted date
` np.datetime64 ` needs iso8601 formatted strings to work properly . the good news is that you can wrap ` np.datetime64 ` in your own function and use this as the ` date_parser ` : #code
i need pandas to keep and parse that other data .
i often find that a more flexible date parser like ` dateutil ` works better than ` np.datetime64 ` and may even work without the extra function : #code
downvote all you like , please add a comment so i can improve the answer . thank you .
i don't want to change the core functionality of the code . i need to use ` np.datetime64 ` for project-specific reasons , so that needs to stay .
sorry , didn't mean to downvote . other than the suggestion to move away from ` np.datetime64 ` , this is a good solution . thanks !
@user i see , updated to reflect your requirement better
here's another way to do this using pd.convert_objects() method : #code

pandas time stamp correction
i have 2 time-series files i wanted to merge both . i can do the merging but the real issue is the format of timestamp in both files .
first file : `" 2014-12-14 00:10 : 00 "`
second file : `" 3 / 30 / 2015 8:30 "`
i'd prefer to go with second file time stamp format .
when i tried to merge i'm getting below error ( of course it's expected one because of the timestamp issue ): #code
convert each ( from strings ) to pandas timestamp / datetime64 . #code
note : ` to_datetime ` also accepts a format option to be more specific .
this works on series / columns too : #code

change pandas string column with commas into float
i ran the code : #code
and received the error : #code
df [ " votes "] is a string of numbers with commas as thousand separators . what is the best way to convert it into a float so i can perform the operation ?
how did you load this data ? if you used ` read_csv ` then you can tell pandas to treat commas as thousands separators : ` read_csv ( thousands= ' , ')`
if each element in df['votes '] is of the form u'x , xxx , xxx.xx ' for example ,
we can do : #code
you could use ` str.replace ` to change ` , ` to nothing and then convert column to float with ` astype ` method : #code

convert numpy arrays to pandas dataframe with columns
i want to normalize my both categorical and numeric values . #code
after that i need to combine 2 ` numpy ` arrays back to ` pandas ` dataframe , but below approach doesn't work for me . #code
error message : ` valueerror : shape of passed values is ( 475 , 243 ) , indices imply ( 83 , 243 )`
one more approach :
#code
error message :
` column : date and msg=singleton array array ( 1444424400.0 ) cannot be considered a valid collection .
column : qtr_hr_start and msg=singleton array array ( 21600000l , dtype=int64 ) cannot be considered a valid collection .
... `
ps . is there any way to avoid numpy et all ? as example , i want to leverage on ` pandas_ml ` library
doesn't work does not explain why it failed . why doesn't it work ? it gives an error or it doesn't give the expected output ?
i added an example of how to do this pure pandas . although , if your goal is machine learning , it might be better to go the pure numpy route and not convert back to pandas .
agree , but i am investigating very convenient library ` pandas_ml ` , and here all calculations based on pandas
what you are looking for is ` pandas.get_dummies() ` . it will perform one hot encoding on categorical columns , and produce a dataframe as the result . from there you can use ` pandas.concat ([ existing_df , new_df ] , axis=0 )` to add the new columns to your existing dataframe . this will avoid the use of a numpy array .
an example of how it could be used : #code
any advice about :
1 . how correctly replace categorical columns ? 2 how normalize numeric columns in this case properly ?
what about pretty simple following approach ? #code

pandas-style transform of grouped data on pyspark dataframe
if we have a pandas data frame consisting of a column of categories and a column of values , we can remove the mean in each category by doing the following : #code
as far as i understand , spark dataframes do not directly offer this group-by / transform operation ( i am using pyspark on spark 1.5.0 ) . so , what is the best way to implement this computation ?
i have tried using a group-by / join as follows : #code
but it is very slow since , as i understand , each category requires a full scan of the dataframe .
i think ( but have not verified ) that i can speed this up a great deal if i collect the result of the group-by / mean into a dictionary , and then use that dictionary in a udf as follows : #code
is there an idiomatic way to express this type of operation without sacrificing performance ?
i understand , each category requires a full scan of the dataframe .
no it doesn't . dataframe aggregations are performed using a logic similar to ` aggregatebykey ` . see dataframe groupby behaviour / optimization a slower part is ` join ` which requires sorting / shuffling . but it still doesn't require scan per group .
if this is an exact code you use it is slow because you don't provide a join expression . because of that it simply performs a cartesian product . so it is not only inefficient but also incorrect . you want something like this : #code
i think ( but have not verified ) that i can speed this up a great deal if i collect the result of the group-by / mean into a dictionary , and then use that dictionary in a udf
it is possible although performance will vary on case by case basis . a problem with using python udfs is that it has to move data to and from python . still , it is definitely worth trying . you should consider using a broadcast variable for ` nametomean ` though .
is there an idiomatic way to express this type of operation without sacrificing performance ?
in pyspark 1.6 you can use ` broadcast ` function : #code
but it is not available in = 1.5 .
thanks for the reply . i wasn't aware of the cartesian product behavior in df.join() ; i had assumed incorrectly that the default behavior was to join on any columns that shares the same name . adding an explicit equality test with an alias for the category column column from the table of means sped things up massively .
you're welcome . it is always useful to check execution extended execution plan ( ` df.explain ( extended=true )`) . the most common issues ignoring configuration are related to cartesian products and even if you provide a join expression it may not be optimized .

unexpected result using .ix indexing with list vs range
can someone explain this behavior to me ? #code
i was expecting both indexing operations to return the same ( first ) result .
then i sort of got it : #code
now , i don't know the internals of pandas and why it implicitly converts strings to dates when given a range but not when given a list , but my guess is that a range makes it clear that we mean an object with ordinal nature so pandas perhaps checks the index , sees that it is a datetime and so parses the strings as dates .
but then the question becomes , why does it do the right thing when we supply a single string ? #code
is it a performance issue of not trying to convert multiple values when given a list ? some other design decision ?
it's even simpler to demostrate the unexpected behavior : `df.ix[['2000 - 01-01 ' , ' 2000-01-02 ']]` -- gives nans
@user exactly . and as you pointed out , it does the right thing with a single date string .
i'm pretty sure this is a known bug ... but no-one wants to implement it ( or at least no-one has ) . edit : i can't find it on github ... but i think it's there somewhere .
accessing datetimeindex with strings is kind-of hacked in ( because r does this it's in there , but easy to find some edge cases like this ) . that is to say :
it works for slices .
it works for single access .
it may work for some other cases , but i wouldn't count on it .
it's much better to use timestamps rather than strings : #code
as mentioned in your answer , this is a little cleaner ( though there is no ambiguity in this case ) as ` .loc ` rather than ` .ix ` .

pandas dataframe - how to combine multiple rows to one
i have a dataset in the form : #code
i want to combine the rows where the value of a matches and produce something like that #code
i have tried expressing it in terms of join or merge but have failed so far . is there any simple way to express that or will i have to use apply and create a new dataframe ?
first , create a ` groupby ` object based on column ` a ` . then create a new dataframe ` df2 ` which uses ` ix ` to index column ` b ` of each group based on the value ` n ` from column ` a ` . set the index of this dataframe equal to the key values from the ` groupby ` ( i.e. the unique values from column ` a `) .
finally , use a list comprehension to set the new column values equal to ` c_1 ` , ` c_2 ` , ..., etc . #code
i'd also recommend using ` groupby ` but i think we can use ` pivot ` to simplify things . first , we create a new c column with the column labels we want to use , and then we call ` pivot ` : #code

extracting data from a text field with pandas
i am trying to create a pandas dataframe by extracting information from notes . i would like to get a few columns #code
notes : #code
i would make a second dataframe and i would try to pull the single word in the single quotes for the procedure event . #code
the focus of this question is a bit off . the challenging part here is extracting the data fields from text following some non-typical pattern . pandas is of little help for this purpose . storing the results in a dataframe or even just a list of dicts afterwards is very straightforward compared to that .
i suggest you read #url
yep , valgur is right . that should be possible using regular expressions
to give you an idea , here might be something to start with ( however , be aware that it is my first time using regular expressions ): #code
`['call call to ( home ) ( 999 ) 555-9898 ended . partial generic vm -- - voice mail left ' ,
' call call to ( work ) ( 999 ) 555-9898 ended . partial - voice mail , no message left -- ' ,
' call call to ( work ) ( 999 ) 555-9898 ended . positive spoke to receptionist -- ' ,
' call call to ( mobile ) ( 999 ) 555-9898 ended . partial generic vm -- - unable to reach customer , voice message left and text sent ' ,
" procedure procedure ' verify ' is checked " ,
" procedure procedure ' duplicate check ' is checked " ,
" procedure procedure ' check something ' is checked " ,
" procedure procedure ' scenario ' is checked " ,
" procedure procedure ' attempt ' is checked "]` #code
` [[ ' ( 999 ) 555-9898 '] , [ ' ( 999 ) 555-9898 '] , [ ' ( 999 ) 555-9898 '] , [ ' ( 999 ) 555-9898 ']]
[[ ' partial '] , [ ' partial '] , [ ' positive '] , [ ' partial ']]
`

insert select * from on duplicate results in error 1136
i am attempting to insert / update a df into a mysql table . i'm receiving this error
" _mysql_exceptions.operationalerror : ( 1136 , " column count doesn't match value count at row 1 ")"
on the execute line in the code below . any suggestions on a workaround or a better way ? #code
this is the schema of the tables i am attempting to insert / update data . #code
what is the schema for table ` db.t ` ? is seams that the two table schemas do not match .
the schemas of the tables ` db.films ` and ` db.t ` do not match . so you need to define wich columns of table ` db.films ` will be set to wich value from table `` .
try this query : #code
i'm sorry paul , but that was a typo on my behalf which i updated . the fields do match .
you have updated the question while i was writing the answer . i now updated the answer too .
thanks , fyi : i just needed to specify the insert into fields as it still works with the asterisk in the select part of the statement .
i know , it works now . but think about , what will happen , if you or someone else decide to reorder the attributes in the object for table t .

pandas calculate logical or based on several columns
this is data frame i am working on : #code
i am trying to create recipe column that will include not nan recipe based on row values .
for instance 1st row value will be aaa , 2nd - ' bbb ' , etc
will appreciate help .
just to clarify , there are additional columns in df , but recipe column should take into account only 3 mentioned .
thank you
so there's always just one non-nan value in each row , and you want that value in the new column ?
that is correct . thank you
you could just do ` df.max() ` for this example , but you may be looking for a more general solution .
can you please write a code ?
that's the code . type ` df.max() ` and you'll get a series with the result you're looking for .
assuming we have additional columns in df and value calculated only based on these 3 ?
a simple solution would be to : #code
just iterate through the ` rows ` and use ` .dropna ` to get rid of the missing values , which you can write to a new ` dataframe ` column like so : #code
thank you , stefan , but this seems not working . i would like to get recipe column as df column
what's the error you're getting ? works for me with ` pandas 0.17.1 ` .
df doesn't have recipe column . data frame still need contain ' recipe @user ' , ' recipe @user ' and ' recipe @user '
updated accordingly .
thank you , stefan . this is working . in case df has additional columns , how code will looks if i would like to refer to only 3 specific columns i mentioned ? thank you
you're welcome - see updated answer .
let us [ continue this discussion in chat ] ( #url ) .
you could use ` apply ` with ` axis=1 ` to apply for rows with ` any ` method , if you have only one valid value and all other are ` nan ` ( using @user example ): #code
edit
it's a seems a bit like a hack but i think that should work ( calling ` min ` if ` dtype ` is numeric alternatively ` any `) : #code
note : ` dtype.kind `
thank you , anton . works perfect .
anton , this solution works fine with strings . with numerical values it isn't robust . what need to be changed in your solution that will be possible to extract only numerical value from row values ? thank you
could you show appropriate example ? you could convert everything to string with ` astype ( str )` but it's not a good way ...
data set updated . operation is an integer
what output should be for that example ? i've got `'aaa ' , ' bbb ' , ' ccc '` . is it wrong ?
should be ' 1 ' , ' 2','3 ' for each row respectively
try edited version . hope it'll work
spasibo , anton . works perfect . appreciate your help .
let us [ continue this discussion in chat ] ( #url ) .

linear regression and gradient descent in scikit learn / pandas ?
in coursera course for machine learning #url , it says gradient descent should converge .
i m using linear regression from scikit learn . it doesn't provide gradient descent info . i have seen many questions on stackoverflow to implement linear regression with gradient descent .
how do we use linear regression from scikit-learn or pandas in real world ?
or
why does scikit-learn or pandas doesn't provide gradient descent info in linear regression output ?
one note is for logisticregression , it does provide an argument called ` solver ` where you can select which optimizer it will use . it will show debug information for the optimizer if you set ` verbose=1 ` . #url
scikit learn provides you two approaches to linear regression :
1 ) ` linearregression ` object uses ordinary least squares solver from scipy , as lr is one of two classifiers which have closed form solution . despite the ml course - you can actually learn this model by just inverting and multiplicating some matrices .
2 ) ` sgdclassifier ` which is an implementation of stochastic gradient descent , very generic one where you can choose your penalty terms . to obtain linear regression you choose loss to be ` l2 ` and penalty also to ` none ` ( linear regression ) or ` l2 ` ( ridge regression )
there is no " typical gradient descent " because it is rarely used in practise . if you can decompose your loss function into additive terms , then stochastic approach is known to behave better ( thus sgd ) and if you can spare enough memory - ols method is faster and easier ( thus first solution ) .
thanks this helps .

pandas and seaborn - heatmap with no colors
i've been working with ` seaborn ` and its ` heatmap ` function . i would like to build a matrix with annotated values from pandas dataframe ` df ` : #code
so far it worked fine with : #code
which returns :
at the end i'm interested to keep only the values and save the structure as a simple table , discarding the colors . i tried to set ` cmap=none ` but it doesn't work , and without the ` cmap ` , seaborn assigns a default ` cmap ` to the heatmap .
uhm .. i don't know if i did understand you properly ... is what you want just a table ?
if i didn't understand you wrong , and all you want is to ignore the colormap , you can create your custom colormap with the background color of your choice using matplotlib's listedcolormap : #code
which will yield :
replace the string ` white ` with a str , hex or rgb color to set up the background color of your choice .
however , i believe ` pandas ` offers better export options . find bellow a screen of all the possible export options :
depending where you want to insert the table , maybe ` to_html ` , ` to_json ` or ` to_latex ` are better options than plotting with seaborn .

pandas dataframe relative indexing
i would like an easy way to access indices relative to a given index in a pandas ` dataframe ` . please see the code below where a draw an analogy to a ` numpy ` array : #code
here is the output : #code
this should be possible for general relative indexing ( not just +1 in one direction ) .
one idea behind pandas is to be able to easily perform bulk operations to data and avoid the need to iterate . can you elaborate on what you are trying to accomplish at a higher level ?
why don't you just use integer based indexing , i.e. ` iloc [ 2 , 2 ]` ?
at the highest level this is a noughts and crosses game ( 5 in a row ) . i've decided to hold the board in a dataframe ( mostly because i'm new to python ) . my ai opponent can now find potential threats by looking for chains of consecutive x's . say a chain of three x's is at f5 , f6 , f7 . my convention gives the address of the chain as f7 and it's length as 3 . if the ai wants to put a o down to block the chain it must go to the address f7 and iterate the numeric index by 1 to get to f8 .
happy to hear it if i'm doing this a silly way .
if i understand correctly , then you want to identify a new ` index ` label relative to another ` index ` label to select a new ` row ` .
` pandas ` provides the ` pd.index.get_loc() ` method to retrieve the zero-based ` integer ` ` index ` position of a label . once you have the ` integer ` position , you can get the label for any offset and select a new row accordingly :
starting at ` index ` ` label ` `'2 '` for value ` 7 ` in ` column c ` : #code
the integer-based ` index ` corresponding to the ` row ` label `'2 '` is ` 1 ` : #code
adding ` 2 ` to get to integer-based ` index ` position ` 3 ` yields ` label ` ` 4 ` : #code
with corresponding ` row ` value ` 15 ` : #code
hope this helps .
what if my label's are not integers ? maybe i'm missing the point of what a dataframe is supposed to do for me . what if i have a dataframe where the indexing is in calendar dates and i want to access everything between july 23 1991 and one hundred days after that ( without having to know what date is 100 days later ) ?
i've used ` string ` labels to illustrate how you can go back and forth between ` label ` and ` index ` based selection ; you can use ` datetime ` just as well . however , your question was about a specific example . for the game example you then introduced , you may be best off just using ` integer ` -based indexing behind the scenes , translating the field names accordingly to facilitate the arithmetic . also please take a look at the docs : #url

pandas filter counts
i've some data that i group by occurrence in one second intervals . i'm having issues finding the correct way to filter out counts below a certain threshold , e.g. i would not want to show anything below a count of 100 . i've tried various versions of filter / lambda constructs but i was not able to filter before or after the .count() method is called . #code
the output currently resembles this : #code
i would like it to look like this : #code
you can try rename column ` date ` to ` count ` and then subset of rows , where column ` count ` is ` 3 ` ( you can change it to value ` 100 `) : #code
next approach is set column name : #code

using pandas and numpy to parametrize stack overflow's number of users and reputation
i noticed that stack overflow's number of users and their reputation follows an interesting distribution . i created a pandas df to see if i could create a parametric fit : #code
which returns this : #code
if i graph this , i obtain the following chart :
the distribution seems to follow a power law . so to better visualize it , i added the following : #code
which produced the following :
is there an easy way to use pandas to find the best fit to this data ? although the fit looks linear , perhaps a polynomial fit is better since now i am dealing in logarithmic scales .
numpy has a lot of functions to do fitting . for polynomial fits , we use numpy.polyfit ( documentation ) .
initalize your dataset : #code
fit a 2nd-degree polynomial #code
next , let's plot the original + fit : #code
excellent answer and illustration of steps ; i added the image inline . thank you very much jos .
` python ` , ` pandas ` , and ` scipy ` , oh my !
the scientific python ecosystem has several complimentary libraries . no one library does everything , by design . ` pandas ` provides tools to manipulate table-like data and timeseries . however , it deliberately doesn't include the type of functionality you're looking for .
for fitting statistical distributions , you'd typically use another package such as ` scipy.stats ` .
however , in this case , we don't have the " raw " data ( i.e. a long sequence of reputation scores ) . instead we have something similar to a histogram . therefore , we'll need to fit things at a bit lower level than ` scipy.stats.powerlaw.fit ` .
stand-alone example
for the moment , let's drop ` pandas ` entirely . there aren't any advantages to using it here , and we'd quickly wind up converting the dataframe to other data structures anyway . ` pandas ` is great , it's just overkill for this situation .
as a quick stand-alone example to reproduce your plot : #code
what does this data represent ?
next , we need to know what we're working with . what we've plotted is similar to a histogram , in that it's raw counts of the number of users at a given reputation level . however , note the little " + " beside each bin the reputation table . this means that , for example , 2082 users have a reputation score of 25000 or greater .
our data is basically an estimate of the complimentary cumulative distribution function ( ccdf ) , in the same sense that a histogram is an estimate of the probability distribution function ( pdf ) . we'll just need to normalize it by the total number of users in our sample to get an estimate of the ccdf . in this case , we can simply divide by the first element of ` num_users ` . reputation can never be less than 1 , so 1 on the x-axis corresponds to a probability of 1 by definition . ( in other cases , we'd need to estimate this number . ) as an example : #code
you might wonder why we're converting things over to a " normalized " version . the simplest answer is that it's more useful . it allows us to say something that isn't directly related to our sample size . tomorrow , the total number of stackoverflow users ( and the numbers at each reputation level ) will be different . however , the total probability that any given user has a particular reputation won't have changed significantly . if we want to predict john skeet's reputation ( highest rep . user ) when the site hits 5 million registered users , it's much easier to use the probabilities instead of raw counts .
naive fit of a power-law distribution
next , let's fit a power-law distribution to the ccdf . again , if we had the " raw " data in the form of a long list of reputation scores , it would be best to use a statistical package to handle this . in particular , ` scipy.stats.powerlaw.fit ` .
however , we don't have the raw data . the ccdf of a power-law distribution takes the form of ` ccdf = x** ( -a + 1 )` . therefore , we'll fit a line in log-space , and we can get the ` a ` parameter of the distribution from ` a = 1 - slope ` .
for the moment , let's use ` np.polyfit ` to fit the line . we'll need to handle the conversion back and forth from log-space by ourselves : #code
there's an immediate problem with this fit . our estimate states that there's a greater than 1 probability that users will have a reputation of 1 . that's not possible .
the problem is that we let ` polyfit ` choose the best-fit y-intercept for our line . if we take a look a ` params ` in our code above , it's the second number : #code
by definition , the y-intercept should be 1 . instead , the best-fit intercept is about ` 1.16 ` . we need to fix that number , and only allow the slope to vary in the linear fit .
fixing the y-intercept in the fit
first off , note that ` log ( 1 ) -- 0 ` . therefore , we actually want to force the y-intercept in log-space to be 0 instead of 1 .
it's easiest to do this using ` np.linalg.lstsq ` to solve for things instead of ` np.polyfit ` . at any rate , you'd do something similar to : #code
hmmm ... now we have a new problem . our new line doesn't fit our data very well . this is a common problem with power-law distributions .
use only the " tails " in the fit
in real-life , observed distributions almost never exactly follow a power-law . however , their " long tails " often do . you can see this quite clearly in this dataset . if we were to exclude the first two data points ( low-reputation / high-probability ) , we'd get a very different line and it would be a much better fit to the remaining data .
the fact that only the tail of the distribution follows a power-law explains why we weren't able to fit our data very well when we fixed the y-intercept .
there are a lot of different modified power-law models for what happens near a probability of 1 , but they all follow a power-law to the right of some cutoff value . based on our observed data , it looks like we could fit two lines : one to the right of a reputation of ~1000 and one to the left .
with that in mind , let's forget about the left-hand side of things and focus on the " long tail " on the right . we'll use ` np.polyfit ` but exclude the left-most three points from the fit . #code
test the different fits
in this case , we have some additional data . let's see how well each different fit predicts the top 5 user's reputation : #code
wow ! they all do a pretty awful job ! first off , this is a good reason to use the full series when fitting a distribution instead of just the binned data . however , the root of the problem is that a power-law distribution isn't a very good fit in this case . at first glance , it looks like an exponential distribution might be a better fit , but let's leave that for later .
as an example of how badly the different power-law fits over-predict the low-probability observations ( i.e. the users with the highest rep ) , let's predict jon skeet's reputation with each model : #code
this yields : #code
very nice . i was going to suggest scipy.stats.gamma . raw data is available at #url it doesn't do a very good job with the raw data either .
great explanation joe ! i'll add the score and ranks of the top 5 users to my data frame and try to see the fit for a 4th degree polynomial , which was a good fit for my original problem ( txs to jos polfliet ) . thank you @user for pointing out to where the raw data is , the log graph looks very interesting .

check for nan values in some particular column in a datframe
suppose i have a dataframe : #code
i want to check for nan in only some particular column's and want the resulting dataframe as : #code
here i want to check for nan in only column ' a ' and column ' c ' .
how this can be done ?
so you want to get rid of rows that don't have any nan values ? the way the question is phrased is not particularly clear .
@user i added some more details to my post . i hope now the question is clear to you .
so you want to find ` nan ` in particular column and drop them ? in your example if you'll subset with `'a '` and `'b '` columns only ` 0 ` row will left ..
no i don't want to drop them . i want the resulting dataframe to contain only those rows where column ' a ' and column ' b ' contain nan .
but your expecting result doesn't contain ` nan ` from `'b '` column ...
sorry it was my mistake . it was column ' a ' and column ' c ' . as in my dataframe row number - 1 doesn't contain nan in column ' a ' and column ' c ' , thats why i have excluded it .
try edited answer
you could do that with ` isnull ` and ` any ` methods : #code
note : if you just want clear rows without any ` nan ` you could use ` dropna ` method
edit
if you want to subset your dataframe you could use mask with your columns and apply it to the whole dataframe : #code
i want to check for null only in some particular column . not all the columns . check my post i added some more details there .
thanks :) . i got my answer .

pandas median over grouped by binned data
i have a dataframe with users , score , times , where each user's different scores and the number of times they received it are listed : #code
and so on .
i'd like to calculate for each user the median of the scores .
for that i guess i should create a row-duplicated df , such as - #code
and then use groupby and apply to calculate the median somehow ?
my questions -
is this the correct approach ? my df is very large so the solution has to be time efficient .
if this is indeed the way to go - can you please advise how ? it keeps failing for me whatever i try to do .
i believe you may be interested in weighted median #url
for your 2nd , row duplicated dataframe you could do `df.groupby('user ') .agg ( np.median )` which will give you ` 1 ` and ` 10 `
i believe you need weighted median . i used function ` weighted_median ` from here , you can also try `wquantile`'s ` weighted.median ` , but it interpolates in a bit different way so you may achieve nonexpected results ): #code
thanks ! seems to be working nice
#code
thanks ! this is what i tried doing as well , but it took too much time with my data and i went with the answer of ilya

aggregating data and getting sum and counts
i have an object in python with a lot of rows :
input : #code
and i need to aggregate this data ( like a pivot table ) .
output i am working on : #code
could someone suggest the appropriate object in python to use such that i could have the following output ? #code
are you using ` pandas ` for that ?
nope , i don't know that
as suggested , look into the pandas package , put the data in a dataframe , it can do pivot tables , or just use the groupby function . #url
are ` team ` , ` player ` , ` trips ` , ` time ` lists or can you put them in lists ?
i can put them in lists
can you install packages , specifically can you do ` pip install pandas ` ?
sure , just done .
use groupby function for pandas dataframes
put your data into a list of lists , each inner list will be a row in the dataframe . #code
call ` groupby() ` , pass the column you wish to use as your grouper ,
and apply a function to the groups .
examples
ex . 1 find the number of trips each team went on . ` team ` is the grouper , and we apply the function ` count() ` on column `['trips ']` . #code
ex . 2 ( multiple columns ) : find the total time each player on a team spent traveling . we use 2 columns `['team ' , ' player ']` as the grouper , and apply the function ` sum() ` on column `['time ']` . #code
ex . 3 ( multiple functions ) : for each player on a team , find the total number of trips and total time spent traveling . #code
hi thanks it works . but i need to have , for instance team2 ; player293 1 17656 ; player333 1 18373 in the same line ( and not 2 lines ) . thanks a lot for your help
@user doe happy to help , and welcome to stackoverflow ! since the question was marked as too broad and my answer was becoming long , i thought it may be better to split the answer into 2 . to see how to print everything in one line , please refer [ here ] ( #url ) . i used this answer to show how to use pandas to get the sum and counts . if you can , try to edit your post accordingly .
if this answer or any other one solved your issue , please mark it as accepted . thank you .

python - how select / drop elements in data frames that have multiple ( 2 ) indices
i have the following python data frame ( test ) with 2 indices ( permkey and ccy ) #code
to select all cash positions , this code does the job : #code
how do i select all non cash related positions ( there can be many ) - i.e. what is the command to drop the cash positions in this df ?
i tried something like this #code
but it results in an empty data frame #code
you can look at the first level of your index and create a boolean array of the non-cash indices : #code
then use the noncash array as a filter for your dataframe : #code
this does not work . test.index.levels [ 0 ] ! = ' cash ' only gives an array of 2 values false , true and then results in error messages when executing test [ noncash ] as it expects three values
seems like it was just use the drop statement differently #code

append data in realtime to an empty pandas dataframe
i'd like to add some data , in realtime , to an empty dataframe : #code
the output is : #code
whereas i wanted : #code
how to append data to a dataframe like this ?
you are indexing into the dataframe into column t with ` df [ t ]` . i think you would like to index into it by row instead .
from the looks of it though , it appears a series may be better suited since you are updating by a time index . #code
in the case that a dataframe is needed , it can be appended using ` df.ix [ row_index ]` : #code
thanks , but i really need a dataframe ( because i have several columns )
added an example for that . to append a single row using an index .
consider using pandas ' append() function to migrate lists of your looped data to dataframe : #code
this is not possible because in reality my ` for ` loop is much longer than ` range ( 5 )` ( its duration is 1 day ) and i need to be able to access to the * updated dataframe * inside the loop . i added some comments in my code in original question .
see update . still same concept but instead of appending to lists in loop and then a bulk append to df out of loop , each iteration appends to df in loop .
did the update work for your needs ? please do accept if my solution helped you .

executing multiple python pandas dataframe methods on one csv
i'm a bit new to programming in general . i've picked up a small project to automate some csv changes via pandas dataframe .
i've been able to figure out a few of the changes i need to make , unfortunately , when i print the current data frame , it only prints out the changes from one of the functions and not the other ( and vice-versa ) .
my code so far : #code
as you can see the two methods that i'm using is " rename " and " insert " . any help would be much appreciated . thanks !
your ` rename ` does not have effect on ` df ` because it returns a new dataframe which is not used . if you want to modify ` df ` , use ` inplace ` : #code
thanks so much happy001 . it worked ! i'll read more about inplace , since i'm anticipating adding more methods to my script .

select specific columns only form a dataframe in python
using python and pandas as pd , i am trying to output a file that has a subset of columns based on specific headers .
here is an example of an input file #code
the structure of gene_input : #code
using a different loop , i generated two dictionaries . the first one has the keys ( sample 1 and sample 7 ) and the second has the keys ( sample 4 and 8) .
i would like to have the following output ( note that i want the samples from each of the dictionaries to be consecutive ; i.e. all dictionary 1 first , then all dictionary 2 ):
the output that i am looking for is : #code
i have tried the following but none worked : #code
in order to extract the first set of columns then somehow combine it , but that failed . it kept giving me attributes error , and i did update pandas . i also tried the following : #code
as well as some other commands / loops / lines . sometimes i only get the first key only to be in the output , other times i get an error ; depending on which method i tried ( i am not listing them all here for sake of convenience ) .
anyway , if anyone has any input on how i can generate the output file of interest , i'd be grateful .
again , here is what i want as a final output : #code
for a specific set of columns in a specific order , use :
` df = gene_input[['sample1 ' , ' sample2 ' , ' sample4 ' , ' sample7 ']]`
if you need to make that list ([ ' sample1 ' ,... ]) automatically , and the names are as given , you should be able to build the two lists , combine them and then sort :
` column_names = sorted ( dictionary1.keys() + dictionary2.keys() )`
the names that you have should sort correctly . for output , you should be able to use :
` df.to_csv ( output file name , sep= ' \t ')`
edit : added part about output
thank you . df.to_csv fully answers my request . one more quick question : if i want to have multiple index columns in gene_input = pd.read_table ( args.gene , sep= " \t " , index_col=0 ) , how can i do that ? i know it must be in index_col but i couldnt choose more than 1 column as an index . that could help me combine other columns too . thanks

pandas.io sql.execute() variable-unicode-query-param with in operator
#code
when i try the test code above , it's works like a charm .
but because of the reason that the query should work dynamically , somehow i did think about the code below ( but not works ) .
i think the problem related with escape character of unicode , but i have no idea how could i solve this problem despite of searching trying many cases in stackoverflow . #code
if possible , please let me know how could it be applicable to read_sql_query , or read_sql related with sqlalchemy.text() .
any advice would be most welcome ! thank you for your help .
cf . #url
#code
someone posted very close to the correct answer but deleted soon . anyway i tweaked that code and both above worked fine .

how to save pandas dataframe generated dynmically as column values as .csv file
i have a pandas dataframe and it has an id column header with values , now i want to generate new column values using the index length concatenating underscore and predefined constant which would increase as the index increases . for example if the there are 3 indexes . the value should be 1_1 , 1_2 and 2_1 and so on . #code
the above code results in saving the result as id header but the values are saved horizontally not vertically as column values . and it is saved as ' 1_1 ' ... ,'2_1 ' so on . i want to save k in the above as result below without the quotations #code
this solved the problem for me : #code

using pandas dataframe to generate trading signals
i have two dataframes with the following layouts :
quotes dataframe #code
signals dataframe #code
the first dataframe is stock prices and moving averages . the second dataframe contains signals for when to purchase a stock ( entry ) and when to sell ( exit ) .
the entry part is already ok . but i'm having problems with the exit part .
it would not make sense to have an exit signals on the ` 2008-06-26 ` because no stock has been purchased yet . and it would not make sense to have an exit signal on both ` 2008-06-30 ` and ` 2008-07-01 ` because we can't sell the same stocks twice .
so i have a way to generate signals in the ` long exit ` column , but i need to filter them by looking backwards from each date to figure out if the is a ` long entry = 1 ` earlier and no ` long exit = 1 ` between the ` long entry = 1 ` and the ` date ` i'm looking at .
the dataframe that i need looks like this , but how can i do that with pandas ?
signals dataframe #code
i looks to me answer to your question goes beyond so . did you have a chance to read articles at quantstart.com , eg #url or this one by the same author #url all that material is relevant in your context ( notice i am not sponsored of whatsoever [ unfortunately ] by quantstart.com )
the question is not about anything quant related . it is only the context . the real problem is how to transform / filter the dataframe
i also recommend posting some example code of how you're generating the ` dataframe ` . the context helps people provide better answers .
you have to track what is currently in your portfolio . maybe you can create a specific dataframe or add a column ' holding ' somehwere and test whether it is > 0 before spurring trading order ( eg see slides 14 and 15 in the second link i provided above ) .
here's a sketch of how you could track the balance of your entry / exit signals so you only signal exit when there's a previous entry not yet canceled by subsequent exit :
starting with : #code
add a ` column ` for your new , filtered signals : #code
iterate through the ` dataframe ` , calculate the prior balance ( this assumes you'd not be signaling entry and exit for the same day ) , and filter accordingly : #code
to get the desired result given the cases you mentioned : #code
hope this helps .

how to draw the dataframe without the breaktime
i have a ` pandas ` ` dataframe ` loaded from a file . when i draw it , it shows all time with a straight line , but i don't need the middle of break time . how to do ? #code
how to remove the line
full ipython notebook runtime
could you please upload a few periods of the data to a small file we can download to test with ?
it helps to convert the ` datetimeindex ` to ` string ` representation as demonstrated below - the series jumps to show the ' missing ' values for the trading break : #code
that is not what i want.i hope they link tegother .
so how about ` .dropna() ` ( assuming missing values cause the straight line ) ? it would help if you could show your data to get closer towards #url
i have tried .dropna() , but there is a line ...... by the way , how to upload my raw data file ?
you could do ` df.loc [ time_before : time_after ]` using timestamps before and after your line starts so one can see what type of data you have at these points in time . one would assume these are ` np.nan ` which may cause ` matplolib ` to interpolate hence the ` .dropna ` suggestion , but perhaps you are getting a constant for some other reason .
df.loc['11 : 30 ' : ' 13:00 '] has to be blank , because there is not trading at that time.but after runing these , df add data automatic : f_inx=pd.date_range(start_tradetime,end_tradetime,freq='5s');symdata['last_price '] .asof ( f_inx )
see updated answer .
i did the same thing as yours , but it just run out different result ! i have uploaded the full session of ipython notebook , please check what's wrong , thinks ! #url
i think you are missing the step in line 3 above to convert the remaining ` timestamps ` to ` strings ` , otherwise ` pandas ` will interpolate to create an even-space series .

merge pandas dataframe with multiindex
i got two pandas dataframes with a multi-level index ( the date is the first dimension , hour of the day the second ) .
i would like to add one column from the dataframe ` first ` to the dataframe ` second ` . if i simply try : #code
i only get ` nan ` values for each row in ` second [ " new_col "]` , despite the two dataframes having exactly the same index .
i also tried to merge the frames like so : #code
what am i doing wrong here ?
edit :
i got a solution myself : seems like the datatypes of the indices weren't the same . i typecasted the the index columns of each frame to ` str ` and ` int ` , which solved the problem .
how can i check for the dtypes of the index levels in advance , to avoid such mistakes ?
can you add samples of your dataframes ?
you can try ` df.index.levels ` : #code
#code

rate of change pandas data frame after grouping
i am new to python pandas and have been trying to get the rate of return using pct_change() . i would like to get monthly rate of change by feedcode .
this my current code : #code
i have two issues :
the calculated rate of return is incorrect as it reference prior day feedcode price to calculate the return for the next feed code . for example , feed aav 2015-09-30 should be nan and not - 0.248696
i would like to remove all nan
i am trying to calculate the returns to get something like this : #code
what is the best way of doing this ?
thanks in advance for any help
would be nice having a sample of your original dataset to work with , thanks .
iiuc you need groupby by ` feed ` from multiindex and apply ` pct_change ` . then you can use subset of ` df3 ` , where column ` rate_return ` is ` notnull ` #code
thanks a million !

calculating averages of multiple columns , ignoring nan pandas numpy
i have a basic table of values : #code
i want to work out the average of the three values , ignoring nan , so for the second row it would be ( 5+4 ) / 2 . therefore i can't use the .replace function to put a zero in nan's place . i have searched through some other questions , but can't find anything that covers this . am i missing something obvious ?
pandas takes care of the ` nan ` for you : #code
mind blown again . thanks !

pythonic way to get a zero-record slice of a pandas dataframe
i have a pandas data frame , and i want to get a zero-record slice . that is , a dataframe with the same columns but zero rows . the reason i am doing this , is because i want to have an empty dataframe , to which i add rows from the original dataframe in a loop .
currently if am using : #code
is this the pythonic way ?
` pandas ` provides tons of ways to avoid loops , are you sure you need one ?
well , obvious way to make dataframe with known columns is to do #code
you'll get empty dataframe as desired . but adding rows one by one is not most efficient way of operations
update
there was a discussion quite some time ago , take a look at add one row in a pandas.dataframe
so in my case you are suggesting : empty = pd.dataframe ( columns =d f.columns )
@user if you really need it to do row-by-row , then yes

insert a numpy rec.array to mongodb using pymongo
in an other question some people are trying to insert a pandas dataframe into mongodb using python internal structures ( ` dict ` , ` list `)
insert a pandas dataframe into mongodb using pymongo
i wonder if we can't insert instead a numpy ` rec.array ` ( ` numpy.recarray `) to mongodb using pymongo .
that should probably be more efficient because ` pandas.dataframe.to_dict ` use for loops and that very long to process huge volume of data
see #url #code
but i faced some errors at insert #code
raised #code
this #code
raised #code
this #code
raised #code
any idea ?
odo can do this #code
however it just goes through an iterator of dictionaries ( and so is as inefficient as the other solutions in this regard ) . if you really want to send binary data efficiently over then you should look at monary .
but for loops aren't necessarily the bottleneck here . i highly recommend doing some simple benchmarking to verify that converting to python data structures here is the bottleneck of your application . you may be optimizing prematurely .
i wonder what path ` odo ` use to achieve this ? according what you are saying it's not using monary .
last time i was involved with odo it would have converted to an iterator of python dicts .

pandas - a better way to plot binned x vs y
new to pandas and i'm wondering if there's a better way to accomplish the following -
set up : #code
i would like to group the x values into equal size bins , and for each bin take the average value of both x and y . #code
well that works . but from here , how do i plot x's mean vs y's mean ? i know i can do something like #code
but this feels wrong and clunky as i have to drop the column levels ( which removes useful structure from my data ) and i have to manually rename the columns . is there a better way ?
you can specify the x and y parameters pointing the multi-level columns using tuples : #code
this way , you don't need to rename the columns in order to plot it .

recalculation of a column value based on a previous value of the same column . is vectorization possible ?
i have a pandas dataframe with 2 columns like this : #code
i want to create a new column c in the following way :
c [ i ]= c [ i-1 ] -a [ i ] +b [ i ]
in this question the answer proposes the use of a loop like this : #code
which does the job .
but since the loops are generally slow in comparison to vectorized calculations , i was wondering if there is a vectorized solution for this in pandas . ( and this is the reason for this new question ) .
i tried to use the shift method like this #code
but it didn't help since the shifted c column isn't updated with the calculation . it keeps its original values : #code
and that produces a wrong result .
the problem you have can be vectorized since delta [ i ] = c [ i ] - c [ i-1 ] = -a [ i ] +b [ i ] . you can get delta from a and b first and calculate cumulative sum of delta ( plus c [ 0 ]) to get full c , as follows : #code
thanks @user it works perfectly . its also a useful lesson for the future : if you stuck , do some algebra and rethink your problem .

python / scikit-learn / regressions - from pandas dataframes to scikit prediction
i have the following pandas dataframe , called ` main_frame ` : #code
i've been having trouble to explore the dataset on scikit-learn and i'm not sure if the problem is the pandas dataset , the dates as index , the nan's / infs / zeros ( which i don't know how to solve ) , everything , something else i wasn't able to track .
i want to build a simple regression to predict the next target_var item based on the variables named " input " ( 1 , 2 , 3 .. ) .
note that there are a lot of zeros and nan's in the time series , and eventually we might find inf's as well .
you should first try to remove any row with a ` inf ` , ` -inf ` or nan values ( other methods include filling in the nans with , for example , the mean value of the feature ) . #code
now , create a numpy matrix of you features and a vector of your targets . given that your target variable is in the first column , you can use integer based indexing as follows : #code
then create and fit your model : #code
now you can observe your estimates : #code
in the above example x= main_frame.input1 [: , 1 :] .values and y= main_frame.target_var [: , 0 ] .values , right ?
` df ` is just a generic term for dataframe . you can replace it with ` main_frame ` in your case . use ` iloc ` ( index location ) instead of ` input1 ` and ` target_var ` .
would you mind adding to your answer a way to plot it together with x and y ?
1 ) only one question per post . 2 ) x is an array with 6 columns , what are you expecting ?
actually i'm having trouble to access the regression results along the time , to then plot it all together , but you are right , it was not part of the question . thanks .

pandas : save to excel encoding issue
i have a similar problem to the one mentioned here but none of the suggested methods work for me .
i have a medium size ` utf-8 ` .csv file with a lot of non-ascii characters .
i am splitting the file by a particular value from one of the columns , and then i'd like to save each of the obtained dataframes as an .xlsx file with the characters preserved .
this doesn't work , as i am getting an error : #code
here is what i tried :
using ` xlsxwriter ` engine explicitly . this doesn't seem to change anything .
defining a function ( below ) to change encoding and throw away bad characters . this also doesn't change anything . #code
changing by hand all the offensive chars to some others . still no effect ( the quoted error was obtained after this change ) .
encoding the file as ` utf-16 ` ( which , i believe , is the correct encoding since i want to be able to manipulate the file from within the excel afterwards ) doesn't help either .
i believe that the problem is in the file itself ( because of 2 and 3 ) but i have no idea how to get around it . i'd appreciate any help . the beginning of the file is pasted below . #code
edit
some code ( one of the versions , without the splitting part ): #code
have you already tried ` df.to_excel ( path , encoding='utf8 ')` ?
@user i have , thanks for asking . to be sure , i tried this one more time - just now . still nothing .
what if you save the csv files from pandas and then use win32com to convert to excel . it would look something like this ... #code
try encoding the columns with non-ascii characters as #code
and then save the file to xlsx format with encoding ' utf8 '
thanks for your suggestion , unfortunately this doesn't work . the same error is returned but now it is triggered by the ` .apply ` line .

empty pandas.series does not provide .empty = true
could someone please explain this strange behavior ? #code
you're not creating an empty series ; you're just giving the name ` test_series ` to the series type itself . and the ` empty ` property itself isn't equal to true .
instead , you want to make an instance of series : #code

matrix representation of pandas data-frame in python
i have a data-frame like #code
i want to represent it in matrix format like #code
how to achieve that ..
i have followed python : printing lists as tabular data link.but not getting what i was looking for .
you need to pivot your data . here is an example . #code
for doing fractional calculations before hand , you might use ` groupby() ` then `transform('sum ')` . it is similar to a sql window function sum . #code
@user -in place of values='amt ' can i pass some o / p of a function . actually i need to pass the % of amt field for each from to corresponding to .
you would probably want to first precompute the value before pivoting . let me see ... i remember a really clever way of doing window functions on a dataframe . let me see if i can find it .
added a way to calculate the ` amt / sum ( amt over all from )` . #url
thanks .. david.great help ..
you need to pivot the data frame . see
#url #code
you could also achieve that with ` pivot_table ` : #code

creating a pickeled data file of image data
i have viewed the two questions on this site for creating a pickled data file of my image data ( similar to mnist.pkl.gz ) . even though i understand that pickling is not necessary , i would like help in figuring out why my data is not getting pickled after typing the below code for carrying out the same .
for now i am using 50 .bmp images ( having the names image1.bmp , image2.bmp and so on ) to test if the .pkl .gz file is formed .
i want to divide them into training , validation and test sets .
these images are in the same directory as the code below
the file trainlabels.csv is a single column csv file , whose column name is ' class ' and contains 50 labels for the 50 images . like this :
class
0
1
2
0
0
1
.
. and so on .
the problem here , is that only the csv file ( the image labels ) are getting pickled and not the image data .
these images are in the same directory as the code below . #code
i understand that this does not answer your question , but i would suggest using numpy's [ save ] ( #url ) and [ load ] ( #url ) .
changing the regular expression to * .bmp made it work .

how to use group by and return rows with null values
i have a data set like below on emails and purchases . #code
i want to find the total number of people in the data set , the number of people who purchased and the total number of orders and total revenue amount . i know how to do it via ` sql ` using ` left join ` and aggregate functions but i do not know how to replicate this using ` python ` / ` pandas ` .
for ` python ` , i attempted this using ` pandas ` and ` numpy ` : #code
the problem is - it is only returning the rows with an order ( 1st row and 3rd ) but not the other ones ( 2nd row ) #code
the ` sql ` query should look like this : #code
how can i replicate it in ` python ` ?
is purchaser is ' na or nan ` ? if yes , you could use ' dropna() ' to get the result
welcome to stackoverflow - you can read [ tour ] ( #url ) .
it is not implemented in pandas now - see .
so one awful solution is replace ` nan ` to some string and after ` agg ` replace back to ` nan ` : #code
#code
thank you so much ! solution works perfectly !

replace all commas with an empty space in csv error
i'm trying to replace all the commas with an empty space in a single column in a csv . i tried the following method here : #code
however i got the following error : #code
i did a bit of research on the nature of this error message but i can't figure out where i would be referencing a variable before assignment .
please show your ` df ` .
fwiw this feels like a bug in pandas to me ( even if somehow you've done something you shouldn't have and got something into a strange state ) . if something goes wrong with ` _try_coerce_args ` or ` mask_missing ` and it raises a typeerror or a valueerror , then we fall into the ` except ` and try to use ` mask ` even though it isn't defined .
interesting , @user , i found there was a bug related to this error , but only within the context of pandas datetime . i'll try ' except ' with this and see if it forces a response .
assuming ` df [ " column_name "]` is of type ` string ` , this should work : #code
you can assign the result to the column : #code
thanks mike . i just ran `df['column_name '] .dtype ` and found out that the column is " o " , which i'm assuming is short for " object " . i'm looking into how to change this into a string
`dtype('o ')` is likely fine because python strings are objects from the perspective of pandas . did you try ` df [ " column_name "] .str .replace ( " , " , " ")` ?
perfect ! i just got it to work , thanks so much mike !

read a csv file with column name as one column cell values
is it possible to read a csv file in this format : #code
into a resulting dataframe like this : #code
this is not a csv file . are you going to have multiple records in a file or a single record mentioned as in your response .
@user i have multiple records in my csv file . my csv file contain around 30 rows with different tag names .
yes , see ` dictreader ` #code
try below option . #code
with pandas : #code
now ` df2 ` contains : #code
in case you have first column as header and second column as value , below option could be useful #code

writing pandas dataframe to remote mysql using sqlalchemy
i have a situation , where i want to write the dataframe ex : store_dataframe
to remote mysql database .
store_dataframe contains around 15 columns and 200000 rows . i am using pandas to create this dataframe . #code
my db connection looks like #code
i am writing this data to remote mysql database using following lines of code . #code
issue :
i am not able to write the dataframe to db unless i restart the database .
its weird to restart the database every time i want to write this . what mistake i am making ?
p.s : does not throw any error , the program hangs/doesn't respond .
my application resides in one system which connects to mysql in another system
what do you mean exactly with restarting the database ? what error do you get if you do not do this ? what is ` db_connection ` used for ? ( it's not used in the code snippet below , as the ` engine ` is used for the connection with the database )
sudo service restart mysql in the remote machine . yes you are right about db_connection . i mentioned as i am doing db.commit()
in my snippet .
my application resides in one system which connects to mysql in another system
` db.commit() ` should normally not be necessary ( the engine takes care of that ) did you notice this was needed ?
yes engine takes care of that . but getting rid of db.commit() still doesnt solve my problem .
after writing the dataframe , can you read it ? or execute other commands ?
i would assume there is another connection holding a lock on the table . try ` show open tables ` . #url
@user : yes i can read it and execute all kinds of sql queries

python pandas mean and weighted average
i am new to python pandas .
any help will be much appreciated
this is my raw data : #code
what i would like to get is this :
1 create a new column call mean to calculate average market cap for each feed .
2 find weighted average . #code
this is my current code for mean which i get nan : #code
and for weighted average code : #code
i got error " attributeerror : ' series ' object has no attribute ' value '
` which gives error ` - what error ? can we get the traceback ?
you don't have the ` value ` column in your data frame , yet you reference it in the code .
thks eli , i have reposted . it should be market_cap . i still got the same error
iiuc you can use ` transform ` and ` mean ` .
` weighted average ` is column ` mean ` divided by sum of unique values of column ` mean ` and ` df3 ` is group by column ` sector ` . #code
but doesn't ` sum ( x.unique() )` assume that each mean is a unique value ? what if there are multiple equal mean values for different sectors ?
it is possible , but in this sample work my approach , because each sector has not overlapping ` feed ` . and column ` mean ` depends of column ` feed ` .
try a combination of transform('sum ') , mean #code
.. #code
.. #code

integer slicing in pandas different for rows and columns ?
coming from r i try to get my head around integer slicing for pandas dataframes .
what puzzles me is the different slicing behavior for rows and columns using the same integer / slice expression . #code
we get 3 rows but only 2 columns . in the docs i find that different from standard python , label based slicing in pandas is inclusive . does this apply here and is it inclusive for rows but not for columns then ?
can someone explain the behavior and the rationale behind it ?
using ` iloc ` everything is fine 2x2 matrix
, ix method is primarily label based with fallback to indexing ... from docs online ...
a primarily label-location based indexer , with integer position
fallback .
` .ix [ ]` supports mixed integer and label based access . it is
primarily label based , but will fall back to integer positional
access unless the corresponding axis is of integer type .
` .ix ` is the most general indexer and will support any of the
inputs in ` .loc ` and ` .iloc ` . ` .ix ` also supports floating
point label schemes . ` .ix ` is exceptionally useful when dealing
with mixed positional and label based hierachical indexes .
however , when an axis is integer based , only label based access
and not positional access is supported . thus , in such cases , it's
usually better to be explicit and use ` .iloc ` or ` .loc ` .
so rationale is that it is trying to help you . as with most things where software assume your intent it can have unexpected consequences . where it does find the labels in the named range it does an inclusive selection at both ends as this is what you would normally want when you are analyzing data
you are correct that there is a distinction between label based indexing and position based indexing . the first includes the end label , while typical python position based slicing does not include the last item .
in the example you give : ` x.ix [ 0:2 , 0:2 ]` the rows are being sliced based on the labels , so ' 2 ' is included ( returning 3 rows ) , while the columns are sliced based on position , hence returning only 2 columns .
if you want guaranteed position based slicing ( to return a 2x2 frame in this case ) , ` iloc ` is the indexer to use : #code
for guaranteed position based slicing , you can use the ` loc ` indexer .
the ` ix ` indexer you are using , is more flexible ( not strict in type of indexing ) . it is primarily label based , but will fall back to position based ( when the labels are not found and you are using integers ) . this is the case in your example for the columns . for this reason , it is recommended to always use ` loc ` / ` iloc ` instead of ` ix ` ( unless you need mixed label / position based indexing ) .
see the docs for a more detailed overview of the different types of indexers : #url

time series analysis - unevenly spaced measures - pandas + statsmodels
i have two numpy arrays light_points and time_points and would like to use some time series analysis methods on those data .
i then tried this : #code
this works but is not doing the correct thing .
indeed , the measurements are not evenly time-spaced and if i just declare the time_points pandas dataframe as the index of my frame , i get an error : #code
i don't know how to correct this .
also , it seems that pandas ' ` timeseries ` are deprecated .
i tried this : #code
but it gives me a length mismatch : #code
nevertheless , i don't understand where it comes from , as rdf['light '] and
tdf['time '] are of same length ...
eventually , i tried by defining my rdf as a pandas series : #code
and i get this : #code
then , i tried instead replacing the index by #code
and it gives me an error on the seasonal_decompose method line : #code
how can i work with unevenly spaced data ?
i was thinking about creating an approximately evenly spaced time array by adding many unknown values between the existing values and using interpolation to " evaluate " those points , but i think there could be a cleaner and easier solution .
you will increase the change to get a good answer if you post a [ minimal , complete , and verifiable example ] ( #url ) .
` seasonal_decompose() ` requires a ` freq ` that is either provided as part of the ` datetimeindex ` meta information , can be inferred by ` pandas.index.inferred_freq ` or else by the user as an ` integer ` that gives the number of periods per cycle . e.g. , 12 for monthly ( from ` docstring ` for ` seasonal_mean `) :
#code
to illustrate - using random sample data : #code
so far , so good - now randomly dropping elements from the ` datetimeindex ` to create unevenly space data : #code
running the ` seasonal_decomp ` on this data ' works ' : #code
the question is - how useful is the result . even without gaps in the data that complicate inference of seasonal patterns ( see example use of ` .interpolate() ` in the release notes , ` statsmodels ` qualifies this procedure as follows :
#code

plotting a background image to a matplotlib graph
i'm trying to plot a simple line plot and insert a background image to a plot .
an example pic ( with cat.jpg and dog.jpd ):
at the moment i have a code that plots the line ( from a pandas dataframe ) and places the images into figure . however the images and the line plot do not ' interact ' at all . #code
what kind of interaction are you expecting ?
i meant that i'd like to crop the cat and dog images so that they would go in line with the line plot ( like in the example picture ) .
you can use ` plt.fill_between ` to create a polygon that covers the area between the origin and the line , then use the ` .set_clip_path ` method of each image object to display only the part of the image that falls within the polygon .
for example : #code

replace values by running maximum values in pandas dataframe
i have a dataframe of the format #code
so now , the output should be #code
here , i am grouping by ` id ` and for each ` id ` , the df is sorted by ` time ` . now , i want to replace the values in ` a ` and ` b ` by the maximum value seen thus far . i guess i can apply a rolling max on each group but is there a better way to do this ?
kind of like a cumulative maximum ? :)
#url
you'd have to call it for every group , but it beats a for-loop , which you'd also have to do for every group .
i did `df['a '] = df.groupby('id')['a '] .apply ( pd.cummax )` but this gives an error ` attributeerror : ' module ' object has no attribute ' cummax '` . not really sure how to use cummax ! thanks :)
you can ` apply ` custom function , where find index of first 1 by ` idxmax ` and set rows to the end of group to ` 1 ` : #code
you can indeed just rely on ` .cummax() ` with ` .groupby() ` and ` .apply() ` , using ` lambda ` - see some examples : #code

merge / concat issue
i have two ` dataframes ` like the following :
dataframe 1 #code
dataframe 2 #code
and the desired result will be here #code
looks and seems so simple to me but i can't quite figure it out . i've read the pandas documentation up and down and searched sof for a while .
here's what i have right now . #code
i've come close but it looks like concating on axis 1 still keeps that second ' id ' column at the end .
edit :
so i'm close now , i am using this : #code
but i am having an issue with duplicate rows with the same id because of multiple lines in the dataframe2
so now , here's what i want my end result to be :
dataframe 1 #code
dataframe 2 #code
and the desired result will be here #code
is there a way to just concat the comments onto each other with some sort of character ? i know we are out of the realm of regular sql and panda .
i think merge would work well for your case . #code
it looks like your ` dataframes ` don't have ` id ` set as index , so ` .merge() ` would work as ( see docs ) : #code
your ` pd.concat() ` merges on ` index ` , so it should work fine ( ` axis=1 ` means horizontal , as opposed to vertical combination ) if you did `df.set_index('id ' , inplace=true )` for both ` dataframe ` before merging ( see docs ) .
thanks , that does " work " -- but i'm having an issue with duplicates . i want items to be added to the end of rows in dataframe 1 no matter how many times they appear in dataframe 2 . is this possible ?
this sounds like you want to do a ' left ' merge ? see updated answer . you can specify to do ` inner ` ( default ) , ` right ` , ` left ` , or ` outer ` join , following ` sql ` logic .
that doesn't seem to work : mergedpanda = pd.merge ( indicatorpanda , commentpanda , on='id ' , how='left ')
see updated answer .
updated my question .
i think you should be asking a new question .

how find all parameters for gridsearchcv in python ?
i want to find full set of parameters per each estimator : #code
in test code , i added only few details `'kernel':('linear ' , ' rbf ')` that was found in example : #code
what is the best way to find all possible parameters and their values ?
you can use ` svr.get_params() ` to get a list of all parameters that gridsearchcv can tune . i'm not sure if it's possible to get a list of possible values ( other than reading the docs ) .
the parameters you use will change on depending on the algorithm you are using . random forests will have different parameters than k nearest neighbors and both of those will be different than support vector machines . the above comment shows ho to get the parameters for each
you have to read the docs ( or at least docstrings through ` help ( )` command ) of each method and decide which parameters to fit . in particular , many of them have infinite number of possible values ( such as c ) thus you cannot check all values . you will need some sampling .
in particular noone besides you can decide whether to check many ways of weighting samples in svm or not , whether or not to test multiple stopping tolerance parameters or not .

conditional summing on python dataframe
i'm just getting into pandas and trying to generate a spreadsheet for a car lot . i'm loving pandas but it's slow going and i'm trying to generate some new columns that sum ... #code
i'm getting the totally predictable .... #code
ideally what i'd love to pull off is ..... #code
my knowledge isn't that great of pandas ( yet ) , but i'm guessing it's an " apply " or an agg() function but so far , syntactically , i'm banging my head from the syntax errors , but i appreciate any pointers in the right direction . .. jw
you can prepare two new series ahead of time in the dataframe with auto and manual counts . #code
also a similar approach is to use a pivot table with margins . #code
proof positive david that sometimes i just need to adjust the way i think . marked as the answer !
to use the built-in ` pandas ` methods , you could : set your `'car ' , ' type ' , ' trans '` ` columns ` as index and ` unstack() ` to get the ` total ` for each subgroup , then just sum over the ` columns ` : #code
oh ! can't wait to try this out !

merge rows with same id and time in pandas
i have a dataframe of the format #code
so now , the output should be #code
here , i am basically merging all rows with same ` id ` and ` time ` such the the values in other rows are max of all the values for that ` id ` and ` time ` .
i am currently doing #code
however , this is taking a lot of time ( > 10 mins ) as the number of rows and columns is large . i am wondering if there is a more efficient way to do the same thing !
unlikely . how many rows is ' large ' ?
the number of rows is ~ 1.2m and the number of columns is ~50
you want the max of all 50 columns ?
yes , all columns
then i do not believe there is an easy solution without upgrading your hardware .
you may want to take advantage of ` multiprocessing ` if your hardware allows you to work on several cores simultaneously , and parallelize the calculation of the ` max ` by group :
using a random sample with 25 columns and `['id ' , ' time ']` as ` multiindex ` :: #code
next , grouping by `['id ' , ' time ']` levels and capturing group ids : #code
resulting in 100 groups : #code
finally , setting up a ` pool ` with 8 workers ( # cores ) , and running the 100 groups through the pool via ` functools.partial ` to pass along the ` axis=0 ` parameter : #code
concatenating the result back into ` dataframe ` via ` list comprehension ` : #code

import text file from geonames using pandas python
i downloaded one of the country datasets from geonames and
i used this line to parse the dataset into columns : #code
but for some reason this doesn't parse all of the rows correctly . most of the rows are correctly parsed and about 2k are not . i used this line to be able to see that it's not parsed correctly : #code
i then opened the output.csv in excel and saw that some of the rows are not parsed .
but when i open the original tr.txt dataset on excel and use tab delimiter all the rows are correctly shown as parsed . so i am doing something wrong in my python code but i can't figure out what .
am i outputting the dataset wrong ??
thank you
opening tr.txt in excel as tab-delimited shows at least 7 lines not parsing properly , first one being line 15191 with value 311071 in first column - do you also get this ?
what is size of your ` ram ` ?
always read the readme.txt file .
in this particular case , there are two noteworthy issues going on .
1 ) the elevation ( column 15 ) is expected as an int , but contains blanks . if you specify an int as the datatype , this will generate an error because there is no nan values for ints . the work around is to cast it as a float . if you really want an int , then create a sentinal value for missing fields ( e.g. -99999 ) , fillna() with this value , and then cast as an int .
2 ) some column contain comma separated lists ( e.g. column 3 , alternate names ) . when you used ` data.to_csv ( " c :/ users / documents / output.csv ")` you destroyed the tab delimited parsing . you need to specify ` sep= ' \t '` . #code
i didn't parse the final date column because it is probably not relevant .
i didn't use the dtypes_dict you have here since elevation is recognized as a float when i import and the other variable datatypes are good as is . i didn't know that i had to specify the delimiter when outputting though so thanks so much for that !
specifying the dtypes results in faster reads as the parser doesn't have to guess the datatype .
oh ok , i'll make sure to do that then . thanks a lot !

pandas rank by column aggregate
i want to create a column ` manager_rank ` that ranks a manager by the sum of returns . i have come up with one solution posted below but was hoping if someone else had something more elegant . #code
desired result : #code
one-liner : #code
step by step details :
1 . group by manager with ` sum ` as aggregation function #code
2 . use ` rank() ` assign ranks to managers #code
3 . cast this result to another column #code
4 . join the result of above steps with original data frame ! #code
you can remove ` to_frame ` and add ` name ` to ` reset_index ` : #code
ahh good point re using name in reset_index
yop , but it works only with ` series `
#code
how about extending the method proposed by @user to include the final cumulative return of each manager ( returns don't sum , they compound ) . #code
that's an awesome way to show cumulative return . i should have named the column return_pct :-( .
that's a great answer to a different question .

use of loc to update a dataframe python pandas
i have a pandas dataframe ( df ) with the column structure : #code
this dataframe has data for say jan , feb , mar , apr . a , b , c , d are numeric columns . for the month of feb , i want to recalculate column a and update it in the dataframe i.e. for month = feb , a = b + c + d
code i used : #code
this ran without errors but did not change the values in column a for the month feb . in the console , it gave a message that :
a value is trying to be set on a copy of a slice from a dataframe .
try using .loc [ row_indexer , col_indexer ] = value instead
i tried to use .loc but right now the dataframe i am working on , i had used ` .reset_index() ` on it and i am not sure how to set index and use .loc . i followed documentation but not clear . could you please help me out here ?
this is an example dataframe : #code
i want to update say one date : 2000-01-03 . i am unable to give the snippet of my data as it is real time data .
could you attach a little example of your dataframe ?
@user : the dataframe i am working on is big , i tried to explain the logic here . i will see if i can create any dataframe
you could attach like part of your dataframe with ` df.head() ` or ` df.iloc [: 10 , : 10 ]`
why not just `df['a '] = df.b + df.c + df.d ` ? you need to include sample data to clarify what you are trying to do and produce a mve . [ ask ]
anton and alexander : this is an example dataframe : import pandas as pd
import numpy as np
dates = pd.date_range('1 / 1 / 2000 ' , periods=8 )
df = pd.dataframe ( np.random.randn (8 , 4 ) , index =d ates , columns=['a ' , ' b ' , ' c ' , ' d '])
i want to update say one date : 2000-01-03 . i am unable to give the snippet of my data as it is real time data .
@user for future , it's better to update your question with your data not in the comment
as you could see from the warning you should use ` loc [ row_index , col_index ]` . when you subsetting your data you get index values . you just need to pass for row_index and then with comma col_name : #code
while not being the most beautiful , the way i would achieve your goal ( without explicitly iterating over the rows ) is : #code

python / pandas / numpy : ' stringmethods ' object has no attribute ' capitalize '
an unusual question , in my remote docker environment linux gcc with ` python ` 2.7.6 #code
correctly works and produces my array i am looking for , however in my local python anaconda distribution of the same version of 2.7.6 , i get `'stringmethods ' object has no attribute ' capitalize '`
from what i've read on #url this capitalize string method is " locale-dependent " which i'm not quite sure what that means but i can only conclude this is the reason for the error .
how should i go about error handling this or is there anything i can do to completely replicate the docker environment . thanks for any help .
there were some changes made to the ` string ` methods api in ` 0.16.1 ` - perhaps a ` pandas ` , not ` python ` version conflict ? #url
definitely possible , but the pandas version in both instances are the same
thank you stefan , my pandas was 0.15 , updating solved this for me !
you're welcome - moved comment to answer the issue can be closed .
the error message suggests that the result of the ` str ` ` stringmethod ` as applied to a ` pandas ` ` series ` does not have the attribute capitalize - so i would be looking for issues with the pandas version rather than with the ` python ` ` string ` methods .
the locale dependency refers to location-specific application of these methods , not the availability of methods per se .

dataframe object has no attribute ' sort_values '
#code
what might be the possible reason of the following error message at the last line ? : #code
` pip update pandas `
hello ` sort_values ` is new in version 0.17.0 , so check your version of pandas .
in the previous version you should use ` sort ` . #code
how can i update ` pandas ` on windows ( preferably using ` pip `) ?
@user ` pip install -u pandas ` ?
@user protopopov : thanks . i did ` pip install pandas -- update ` . looks like it's updating ` pandas ` .
my advice on windows is to use the [ anaconda ] ( #url ) distribution since some dependencies of pandas could be tricky to update .
if you want to use the new ` sort_values ` you have to slightly modify the function adding a ` by ` : #code

set some rows equal to other rows pandas
i have a dataframe like this : #code
where column a is the ` name ` with a suffix ` _x ` or ` _y ` , and column b is a value .
i want to make rows with ` _y ` equal to #code
with the same name .
the output should be , #code
sometimes , the dataframe will be #code
and the output should be : #code
non-overlapping part remains ` nan `
how can i solve this problem in a simple way ?
well , what have you tried ?
much of this problem will be solved with ` str.split ` as shown [ here ] ( #url )
at first , i want to add one column " c " , which deletes the suffix of column " a " . it shows like ` [ " name1 " , " name2 " , " name3 " , " name1 " , " name2 " , " name3 "]` . then i ` groupby ` column " c " , and use ` .loc ` to set the name ( i ) _y equal to -1*name ( i ) _x . i think it can solve the problem , but maybe not the best solution .
my suggestion is merge the x data to y data on axis 1 , calculate and rebuild .
this worked for me : #code
separate the ' x ' values from the dataframe and create corresponding ' y ' values . then just concatenate .
i've extended the example dataframe to consider cases where the names do not match . in this case , if the name ends with ' _x ' , a new ' _y ' name variable will be created . if there is a name ' _y ' but no corresponding name ' _x ' , then it will be left unchanged . #code
the first thing to do is to separate the ` name ` part and the ` x ` part by splitting on the ` _ ` : #code
you can now perform operations using the information which was previously locked away in that one field ` name1_x ` .
for example : #code
now it's easy to set ` y ` to be related to ` x ` in any way you wish : #code
this was my original approach , but you run into an issue where , for example , ` name4_y = nan ` . once you unstack , column ' x ' will also be nan . i believe the end result should have ` name4_y = nan ` but should not have ` name4_x = nan ` .
so what should ` name4_x ` * be * in your scenario ? if you mean it shouldn't exist at all you could always drop the rows where ` x ` is ` nan ` .
i don't believe it should be in the result , but i leave it to the op to clarify . i originally thought a missing y name should also be excluded , but the question says to include them .
sorry ! i didn't check carefully : i assumed you * were * the op !

calculating dynamic time warping distance in a pandas data frame
i want to calculate dynamic time warping ( dtw ) distances in a dataframe . the result must be a new dataframe ( a distance matrix ) which includes the pairwise dtw distances among each row .
for euclidean distance i use the following code : #code
i need a similar code for dtw .
thanks in advance .
this question is not really suited for stack overflow . maybe you should try to implement your own algorithm ( maybe following [ this ] ( #url ) blogpost ) and post it for feedback on [ code review ] ( #url ) .
#url

how to convert timedelta to time of day in pandas ?
i have a sql table that contains data of the mysql ` time ` type as follows : #code
i then use ` pandas ` to read the table in :
` df = pd.read_sql('select * from time_of_day ' , engine )`
looking at ` df.dtypes ` yields :
` time_of_day timedelta64 [ ns ]`
my main issue is that , when writing my ` df ` to a csv file , the data comes out all messed up , instead of essentially looking like my sql table : #code
i'd like to instead ( obviously ) store this record as a ` time ` , but i can't find anything in the pandas docs that talk about a time ` dtype ` .
does pandas lack this functionality intentionally ? is there a way to solve my problem without requiring janky data casting ?
seems like this should be elementary , but i'm confounded .
what does pandas.to_datetime give ?
it returns ` 1970-01-01 12:34 : 56 `
found a solution , but i feel like it's gotta be more elegant than this : #code

unable to extract nested json and put into pandas data frame
i have a json file and am trying to extract a list of all teams . i can do it with one iteration , but when it's nested at more than one level , i am unable to do anything . in this scenario , i am trying to extract players into a data frame and write that to json . code is below . starting with json . thanks ! #code
here is my python script . #code
i know in the last line is where the error is and that this is a series . how do i get all players . once i get this i can write this to a csv . any help will be greatly appreciated !
there is no real question here , but i assume you want to know why the error occurs .
` data [ " teams "]` is a ` list ` as you can see in line 2 of your input json ( opening square bracket ) .
so ` some_list [ " players "]` does not make sense and raises ` typeerror : list indices must be integers or slices , not str ` because a list can only be indexed by integers .
you can either get all players of a specific team :
` data [ " teams "] [ n ] [ " players "]` where ` n ` denotes the n-th team .
or you can get all players of all teams :
` [ teams [ " players "] for team in data [ " teams "]]`
edit :
if you want to get rid of the nesting you can use :
` [ player for team in data [ " teams "] for player in team [ " players "]]`
note : using a generator expression might be faster , depending on how pandas works .
my question is how do you i get all players , so that i can write a list of players to a data-frame and eventually a csv . i have no problem when it is nested only at one level , but cannot get at when it is nested two levels like the current example .
thanks . extremely helpful !
assuming no two coaches have the same name , you can use a nested dictionary comprehension . #code

vectorizing haversine distance calculation in python
i am trying to calculate a distance matrix for a long list of locations identified by latitude longitude using the haversine formula that takes two tuples of coordinate pairs to produce the distance : #code
i can calculate the distance between all points using a nested for loop as follows : #code
using a simple function : #code
but this takes quite a while given the time complexity , running at around 20s for 500 points and i have a much longer list . this has me looking at vectorization , and i've come across ` numpy.vectorize ` ( ( docs ) , but can't figure out how to apply it in this context .
possible duplicate #url
thanks , i missed that !
you would provide your function as an argument to ` np.vectorize() ` , and could then use it as an argument to ` pandas.groupby.apply ` as illustrated below : #code
for instance , with sample data as follows : #code
compare for 500 points : #code
` vectorize ` is really just for convienience as far as i know it doesnt typically provide any speedup ( at least any meaningful speedup )
hmmm maybe i stand corrected ... not entirely sure ...
thanks , i needed to see an implementation sample , still new to all this and couldn't figure it out from the docs ...
start by getting all combinations using ` itertools.product ` #code
that said im not sure how fast it will be this looks like it might be a duplicate of python : speeding up geographic comparison
from `haversine's function definition ` , it looked pretty parallelizable . so , using one of the best tools for vectorization with numpy aka ` broadcasting ` and replacing the math funcs with the numpy equivalents ` ufuncs ` , here's one vectorized solution - #code
runtime tests -
the other ` np.vectorize based solution ` has shown some positive promise on performance improvement over the original code , so this section would compare the posted broadcasting based approach against that one .
function definitions - #code
timings - #code
thanks , this is quite a bit better and very helpful !

issue with cross_validation in pandas_ml
i try to complete cross validation with ` pandas_ml ` library #code
however , i got unexpected error
raise valueerror ( " {0 } is not supported " .format ( y_type ))
valueerror : continuous is not supported
any tricks here ?
this is because the accuracy metric is for scoring classification models only . it measures the fraction of correct predictions . instead of what fraction of your predictions are correct , in regression what you want to know is how far away from the correct answer your predictions are - it's unlikely that any of them will be exactly correct so that's not a meaningful metric .
regression metrics include ` r2 ` , ` mean_squared_error ` , ` mean_absolute_error ` , etc .
you could use something like : #code
are there any others approaches for regression models ?
#url - i see that ` cross_val_score ` was used with ` randomforestregressor `
that's using the ` r2 ` scoring metric . there's no problem using ` cross_val_score ` , you just need to use an appropriate scoring metric .

python pandas : determining which " group " has the most entries
let's say that i have pandas dataframe with a column called " fruit " that represents what fruit my classroom of kindergartners had for a morning snack . i have 20 students in my class . breakdown would be something like this .
oranges = 7 , grapes = 3 , blackberries = 4 , bananas = 6
i used sort to group each of these fruit types , but it is grouping based on alphabetical order . i would like it to group based on the largest quantity of entries for that class of fruit . in this case , i would like oranges to turn up first so that i can easily see that oranges is the most popular fruit .
i'm thinking that sort is not the best way to go about this . i checked out groupby but could not figure out how to use that appropriately either .
thanks in advance .
to sort by name : ` df.fruit.value_counts() .sort_index() `
to sort by counts : ` df.fruit.value_counts() .sort_values() `
i did this as recommended : df.fruit.value_counts() .sort_index()
i was given a result like this , which discovered is a series :
oranges 5
bananas 13
apples 2
i want to return " bananas " in the function that i created because bananas has the highest count at 13 , i am puzzled as to how to do this . if it was a data frame , my thought was i could sort by the integer column and then return the first string in the fruit-based column .
i'm new to pandas and python , i'm not quite sure how to convert the series to a data frame the way i mentioned or if that's even necessary ?
in pandas 0.17 : #code

pandas write to csv specify dtype
is there a way to prevent write a pandas dataframe as a csv without converting numbers that are formatted as strings to numerics ?
say i have the following dataframe : #code
i can confirm that my index is not a numeric : #code
but when i write to csv using `d.to_csv('myfile .csv ')` the index is converted to an integer and i lose the leading 0 . any suggestions ?
are you sure the zeroes are lost when writing to csv ? i replicated your results and examine my csv file , and all of the zeroes are there . my guess is that they are lost by whatever program you then use to read the new file .
agreed . i replicated the code . when i open the csv file in a txt editor , i get the leading zeros . if you open the file in a spreadsheet program ( or read it back into a pandas dataframe ) the zeros are dropped at that point .
ah thank you . so the issue is actually when i read in the csv not when i write it . and i can specify the dtype when reading in a csv , so problem solved .
there is no problem with the `d.to_csv('myfile .csv ')` . i have all the zeros in my .csv file by running your code .
if you use ` d1 = pd.read_csv('myfile .csv ' , index_col=0 )` , the leading zeros will be dropped in the ` d1 ` .

set index as integers with mixed types
given : #code
if 1 and 2 are floats and i set ' col1 ' as the index , i get : #code
how can i manipulate the new index such that it only contains integers with no decimals ( 1 and 2 ) with the string ( ' a ') in a mixed-type column ?
thanks in advance !
dumb question ( not ` pandas ` user ): since when is `'a '` an integer ? also , would it not make sense to generate the index column from the mixed type column so it can be converted to uniform type and indexed properly , while preserving the original values of the mixed type column ?
hi , i just modified the question for clarity . i meant to say that i need a mixed-type column with the numbers being integers yet with the string still present .
so , you want to convert to ints in cases it is possible , you should probably use try and except , like so : #code
thank you so much !

error with corr function in pandas
i had a csv file of 2 stock prices data which i did some native python coding to create 2 single lists of prices ( in decimals ) of 2 stock counters .
i then converted them to 2 pandas dataframe , used the .pct_change() function then applied the a.corr ( b ) function in an attempt to get the correlation .
this is the error msg that i got . anyone can advise on the issue here ? #code
the head() of the 2 dataframes look like this :
price
0 nan
1 0
2 0
3 0
4 0.00021853
price
0 nan
1 0
3 0.00051667
4 0.00003361
2 0.00005167
you can't use ` corr ` that way , with two dataframes . what are you trying to accomplish ?
i'm just trying to find the correlation between 2 stock prices as mentioned .
each dataframe contain only a single column of price data .
ok i realised it works if i parse the list into a pandas series rather than a dataframe change my initial prices from string to float rather than to decimals .
why doesn't the correlation function work with decimal data types though ? #code

dataframe append a row
how do i append only one row to a new dataframe ? i have only seen examples appending a whole dataframe . i am using iterrows to so i have the index of the row i want to append to a new dataframe . #code
inside this if statement i want to append that row .
do you wan to append one of the rows from your dataframe to the same dataframe ?
if you wan to append a row to the dataframe you have to assign it to a variable first . for example if you want to append the first row in your dataframe to the same dataframe : #code
i have a dataframes from pandas : #code
output : #code
then , you can append a specific row in a new dataframe . #code
output : #code
i recommend to see this answer about dataframe iterating speed .
to add one row of data to existing dataframe you should provide the data of one row compliant to existing dataframe . #code

python pandas combine the second row if the first row ids are the same
we are using python 2.7
we have a simple table below : #code
it generates #code
is there any pandas way to match the ` id ` in the column ` a ` ? for example , if the ` id ` in the column ` a ` is the same , then concatenate the second row into a dictionary or a list . for example : #code
thank you !
your desired output is a little vague -- you mention dictionaries and lists , and then give a set containing strings -- but if you just care about the matched information and not the format it's in , then iiuc , you could use ` groupby ` . if b consists of strings ( as it does in your example frame ) , it's easy : #code
if b is made up of integers instead , we'd have to convert them to strings , maybe using something like #code
i recommend reading the documentation on the split-apply-combine pattern .
after your answer i just realized sum concatenates strings :)
you could groupby aggregate to list and join the list as below . #code

how to use correlation functions in pandas & numpy with decimal dtypes ?
when i try to use the ` a.corr ( b )` function in pandas or the ` corrcoef ( a , b )` function in numpy ,
i get the error : #code
do they only work with float or is there a way around it ?
why are you using ` decimal ` s rather than a floating point dtype ?
` numpy ` , and by implication ` pandas ` , do not provide support for a ` numeric ` ` dtype ` ` decimal ` ( see docs and docs ) , and uses a generic ` dtype ` ` object ` instead .
hence the need to cast ` decimals ` ` astype ( float )` : #code
` decimals ` as ` dtype ` ` object ` : #code
and unless cast as ` float ` , the following operation throws the error you mention : #code

python 2.7 pandas to fill missing number / series
here is a shorten table . #code
the goal here is to output the " device " pattern for each unique " id " . there should be in total 4 number : 101,102,103,104 in the " number " column . if any " number " column is missing , we fill in " n " in the " device " column . for example , number 101 and 102 are missing in the " a " id , so we wanna fill in the following fashion : #code
the goal here is to output the " device " pattern for each unique " id " . therefore , by filling the necessary " n " in the " device " column , we are hoping to get outputs like this by using " groupby " :
{ " a , nnll " , " b , ldnn " , " c , vvll " }
could any guru help regarding the first missing value filling ? python pandas way is more welcome !
here's a one liner ( actually eight lines , but broken for readability ) . the output of what you requested is not valid : it is neither a list nor a dictionary . i have exported the contents as a dictionary , but could do a list format if you prefer . #code
the main part of the solution is the pivot table , where we fill missing values with ` n ` . #code
this basically gets the data we want , now we just have to restructure it . if we call the output above ` df2 ` , then : #code
this , in turn , can be joined together using a dictionary comprehension : #code
alternatively , you can use a list comprehension : #code
oh my budda , this is dirty !
what's dirty about it ? it is just a condensed version of the longer explanation .
meaning it is a very clean solution !
i really like this pivot table solution . elegant and fast !

how to find the distribution in python ?
i have data set from goodreads with number of books read over a year with details like number of pages of the each book , date read etc . i want to know in which month / week i have read most / least . how do i do that in python ?
you should probably use groupby , check the docs . it should look something like : data.groupby(['year','month ']) .min()
could you post sample of dataframe ? #url

creating binned histograms in spark
suppose i have a dataframe ( df ) ( pandas ) or rdd ( spark ) with the following two columns : #code
in pandas , i can create a binned histogram of different bin lengths pretty easily . for example , to create a histogram over 1 hr , i do the following : #code
moving to pandas df from spark rdd is pretty expensive for me ( considering the dataset ) . consequently , i prefer to stay within the spark domain as much as possible .
is there a way to do the equivalent in spark rdd or dataframes ?
spark rdd or dataframe do not have index sadly and spark do not provide low level operation as pandas definitely no re sampling of ts .
there is a recent cloudera spark package on time series , it also has python doc [ here ] ( #url ) . i don't know if it is what you are looking for , but it does say it's pandas-like functionality for time series .
woodchopper : what do you mean by ' do not have index ' ? are you referring to ' set_index ' functionality as available in pandas ?
in this particular case all you need is unix timestamps and basic arithmetics : #code
example data from ` pandas.dataframe.resample ` documentation .
how do i convert 12345.0 ( long ) to the correct time format in spark ?
what does it represent ?
timestamp is a long data type . how do i convert it to the correct time format so that spark can interpret it correctly as described in your example ?
if it is in seconds you can pass it directly to ` resample_to_minute ` / ` resample_to_hour ` . if you want to convert it to datetime you can simply use cast as shown above .
if i pass df['timestamp '] directly , i get " typeerror : unsupported operand type ( s ) for /: ' str ' and ' int ' " i.e resample_to_minute ( " timestamp ")
i cannot reproduce the problem . could you show ` df.printschema() ` ?
root
| -- timestamp : long ( nullable = false )
i see , you use ` resample_to_minute ( " timestamp ")` not `resample_to_minute(df['timestamp '])` . you should use the latter . function operates on columns not names .
let us [ continue this discussion in chat ] ( #url ) .

pandas to_datetime valueerror : unknown string format
i have a column in my ( pandas ) dataframe : #code
when i convert it into a date format ( as follows ) i am getting the error valueerror : unknown string format #code
what am i missing here ?
what is the exact data type of the elements in the column ?
you can pass `errors='coerce '` ( or ` coerce=true ` in earlier versions ) , and then see which item is converted to nat ( and so could not be converted to a datetime )
the type is object . by that i guess its considered to be string ?
this is probably indeed string ( but can also be mixed ) . did you try my suggestion ?
i think problem is in data - some problematic string . so you can try check length of string in column ` start date ` : #code
or you can try find these problematic row different way e.g. read only part of datetime and check parsing datetime : #code
but this is only tips .
edit :
thanks joris for suggestion add parameter `error='coerce '` to ` to_datetime ` : #code
the data is from a csv file . when i had a look at the data on excel , it seems fine , like a regular date format . the length of the dates are not unusual .
how it works ? is error found ?
upon drilling down each record . i realised there were some encoding whitespaces issues , which needed trimming . turns out the ' length ' way of verification helped this case . thanks

what is the pandas idiom to convert a time column into datetime format ?
i'm trying to convert time in a column to datetime : #code
thanks .
what are the extra fields ? ` datetime ( year , month , day [ , hour [ , minute [ , second [ , microsecond [ , tzinfo ]]]]])` the last two values in your tuples do not correspond to fields in the datetime constructor .
( tm_year=2000 , tm_mon=11 , tm_mday=30 , tm_hour=0 , tm_min=0 ,
tm_sec=0 , tm_wday=3 , tm_yday=335 , tm_isdst=-1 ) #url
use ` datetime.datetime ` to convert the data : #code

strip timezone info in pandas
i have been struggling with removing the time zone info from a column in a pandas dataframe . i have checked the following question , but it does not work for me :
can i export pandas dataframe to excel stripping tzinfo ?
i used tz_localize to assign a timezone to a datetime object , because i need to convert to another timezone using tz_convert . this adds an utc offset , in the way " - 06:00 " . i need to get rid of this offset , because it results in an error when i try to export the dataframe to excel .
actual output #code
desired output #code
i have tried to get the characters i want using the str() method , but it seems the result of tz_localize is not a string . my solution so far is to export the dataframe to csv , read the file , and to use the str() method to get the characters i want .
is there an easier solution ?
doesn't `df['datetime '] .dt .tz_localize ( none )` work ? replace ` datetime ` with what ever your column name is
thanks . it doesn t work . the format of the date is " 2015-12-01 00:00 : 00-06 : 00 " . i used " to_datetime " to convert the original date format to a datetime object , in order to apply " tz_localize " to convert to another time zone . it seems tz_localize adds that offset and i have not found how to get rid of it .
if it is always the last 6 characters that you want to ignore , you may simply slice your current string : #code
maybe help strip last 6 chars : #code
thanks ! that did the trick . i had to convert it to a string first .
thanks for accepting , you can upvote too . it is up to you .

how compute rolling difference between open year stock price and closing price of that stock with a frequency one day
i have stock data for over 30 years . what am trying to do is calculate whether the stock rose or fell during an year using ` rolling_apply() ` .
the frequency is one day an the window is 252 with ` min_period ` of 2 days .
due to the massive amount of data am trying to avoid a for loop since it greatly slows the execution and pandas seems to be the best bet .
here an image of the sample data .
the data
what i want to achieve is for example after entering the date e.g 2015-12-22 , the rolling function should compute whether through the year starting from 2015-12-22 to 2014-12-22 the value ` open_price ` ( at 2015-12-2 ) - ` close_price ` ( 2014-12-22 ) increased or reduced and return the value then from 2014-12-22 to 2013-12-22 all the way to 1997 . then do the same for 2015-12-23 , all the way to 2015-12-31 .
the value returned should be the number of years the stock had risen . for example given for the first date e.g 2015-12-22 the opening price was at ` 663.xx ` and at 2014-12-22 the closing price was ` 660.00 ` , the stock had risen hence increment a counter . then if the stock had risen from 2014-12-22 to 2013-12-22 it should increment the counter until the last year in the data is reached , i.e. if there were 30 yrs and it rose for 14 value returned is 14 for that date then it should roll to the other dates and do the same . #code
could you post your dataframe to work with instead of pic ?
just added the code to show how i obtained the data
is this what you're looking to do ? #code
edited in response to comment #code
thanks but not quite the answer . the value returned should be the number of years the stock had risen .for example given for the first date e.g 2015-12-22 the opening price was at 663.xx and at 2014-12-22 the closing price was 660 the stock had risen hence increment a counter . then if the stock had risen from 2014-12-22 to 2013-12-22 it should increment the counter until the last year in the data is reached . i.e if there were 30 yrs and it rose for 14 value returned is 14 for that date then it should roll to the other dates and do the same .
maybe say that in the question then .

why recast a pandas groupby object as a dataframe to write to excel ?
if i read a csv file into a pandas dataframe , followed by using a groupby ( pd.groupby ([ column1 ,... ])) , why is that i cannot call a to_excel attribute on the new grouped object . #code
can someone explain why pandas needs to go through the whole process of converting from a dataframe to a series to group the rows ?
i believe i was unclear in my question .
re-framed question : why does pandas convert the dataframe into a different kind of object ( groupby object ) when you use pd.groupby() ? clearly , you can cast this object as a dataframe , where the grouped columns become the ( multi-level ) indices .
why not do this by default ( without the user having to manually cast it as a dataframe ) ?
please show the output from ` data.head() ` .
because all you have is a ` groupby ` object with no aggregations performed , the error shows that ` series ` objects have no ` to_excel ` method this is expected operation , it looks like you want to set the index to those columns and then export to excel
as mentioned in the docs , ` pandas.dataframe.groupby ` returns a ` groupby ` object .
to illustrate : #code
this ` object ` has its own set of attributes ( omitting internals ) - and no ` to_excel ` : #code
you can save each group individually to excel like so : #code
otherwise , sorting and storing to excel would be the way to go .

select date index with pandas datetimeindex
i have following pandas dataframe ` selected_symbol_log ` : #code
what i want is to select rows using variable last_cover containing pandas ` datatimeindex : #code
i do : #code
this use to work before , but perhaps after pandas update i get error : #code
what i found out is that #code
is working . but why my original condition stopped working and what is the way to get it working ?
you need to compare with one value . use ` last_cover [ 0 ]` : #code
great that it helped . btw , you have the privilege to [ upvote ] ( #url ) . ;)

python string of numbers to date
i have am trying to process data with a timestamp field . the timestamp looks like this :
' 20151229180504511 ' ( year , month , day , hour , minute , second , millisecond )
and is a python string . i am attempting to convert it to a python datetime object . here is what i have tried ( using pandas ): #code
so i add milliseconds : #code
so tried using the dateutil.parser : #code
also tried converting these entries using the pandas function : #code
i've made sure that whitespace is gone . converting to strings , to integers and floats . no luck so far - pretty stuck .
any ideas ?
p.s. background info : the data is generated in an android app as a the java.util.calendar class , then converted to a string in java , written to a csv and then sent to the python server where i read it in using pandas ` read_csv ` .
just try :
` datetime.strptime ( x , " %y%m%d%h%m%s%f ")`
you miss this :
%b : month as locale s abbreviated name .
%m : month as a zero-padded decimal number .
might be worth highlighting that the key difference here is the use of ` %m ` for the month ( month as a zero-padded decimal number ) instead of ` %b ` ( month as locale s abbreviated name )
yes , you're right , bad manipulation , sorry .
* groan* . yes it was that . just when i'd convinced myself this wasn't a stupid question . oh well , will leave it there for the next lost datetimer .
` %b ` is for locale-based month name abbreviations like ` jan ` , ` feb ` , etc .
use ` %m ` for 2-digit months : #code

python function to calculate distance using haversine formula in pandas
( ipython notebook )
( bus statistics )
summary.head()
i need to calculate distance_travelled between each two rows , where
1 ) row['sequence '] ! = 0 , since there is no distance when the bus is at his initial stop 2 ) row['track_id '] == previous_row['track_id '] .
i have haversine formula defined : #code
i am not exactly sure how to go about this . one of the ideas is use itterrows() and apply harvesine() function , if rows ' sequence ' parameter is not 0 and row's ' track_id ' is equal to previous row's ' track_id '
[ edit ] i figured there is no need to check if ' track_id ' of row and previous row is the same , since the haversine() function is applied to two rows only , and when sequence = 0 , that row's distance == 0 , which means that the track_id has changed . so , basically , apply haversine() function to all rows whose ' sequence ' ! = 0 , ie haversine ( previous_row.lng , previous_row.lat , current_row.lng , current_row.lat ) . still need help with that though
[ edit 2 ]
i managed to achieve something similar with : #code
where previous_row should actually be previous_row , since now it is only a placeholder string , which does nothing .
isn't this a a dupe of this : #url
iiuc you can try : #code
if performance matters , calling ` .apply ( haversine , axis=1 )` will be much slower than writing ` haversine ` to take numpy arrays and doing `summary['distance_travelled '] = haversine(summary['lng '] , summary['lat '] , summary['lngp '] , summary['latp '])`

how to remotely query sas7bdat file in python
i am trying to query a database stored in sas's sas7bdat file format using python ( i don't have sas ) . the sas7bdat file is stored on a remote server that i can access through an ssh connection with paramiko , but is too large to simply read with python's sas7bdat module or to download locally .
can anyone think of a way to use the ssh connection to query the sas7bdat file , and load the query results into python ( pandas dataframe , etc ) ? or perhaps a completely different approach ?
do you have permissions to write scripts on the server ?
yes , and the server ( which is old ) has python 2.6.4 installed .
but it does not have the sas7bdat python module installed , nor do i have permission to install it .
do you have sas / share licensed ? if so , is this available via a share library ?
no , i don't have any sas products .
how was the sas data set generated , then ? the point of my question is that if the sas server has a sas / share or sas integration technologies license , then there are options for you to access the data ( jdbc , a c library proved by sas , etc . ) .
the sas data set was generated by another user and placed on the server ( a long time ago ) . the server itself doesn't have a sas implementation .

pandas datareader fails to obtain data for mutual funds and indexes from google
i have attempted to obtain financial data from google for certain financial indexes not available elsewhere . the data extraction failed and has left me wondering if certain categories of google financial data cannot extracted using pandas datareader . i have googled the issue and cannot find any discussion of this issue . is there a limitation of obtaining data from google finance ?
here is the problem . i received the following error when i attempted to obtain data from google finance using the pandas's datareader :
oserror : after 3 tries , google did not return a 200 for url ' #url q=vfinx startdate=jun+02%2c+2003 output=csv '
the pandas's statement causing the error message was : #code
i imported the following libraries : #code
the error occurs also when i attempt to obtain data for a mutual fund , such as vfinx ( vanguard s p 500 ) , or an index , such as dwcpf ( dow jones completion index ) . the obvious work around for mutual funds is to use yahoo . however , the above statement works fine when i obtain data for a stock , such as c ( citibank ) . this leads me to believe that google finance data for mutual funds and indexes are not available thru the pandas data reader .
unfortunately , the historical data for index dwcpf is not available from yahoo . to obtain the data from google i web scraped google by modifying the url ,
#url startdate =d ec+26%2c+2014 enddate =d ec+25%2c+2015 num=200 ei=tvv9vohsomwsmagax7ewcg
clearly , web scraping is more work than using simply the data reader .
i am using python 3.4 ( and 3.5 on another computer ) , pandas version 0.17.1 , and recently upgraded to pandas datareader .
it looks like google finance does not support csv output for mutual funds . so this url works : #url but adding &output=csv with any &startdate or &enddate does not
this could be because mutual funds and indexes do not have trading prices ( they have navs and levels , respectively ) , which google may not have permission to provide for download . just a guess though .
google lists the data providers for the various securities and exchanges , and so does yahoo . while stock data come delayed from exchanges , mutual fund data come from ` morningstar ` and ` vickers-stock.com` , respectively .
the latter being commercial data providers ( and so is dow jones ) rather than stock exchanges , they will likely be more restrictive on the data usage beyond display on google / yahoo websites , and will likely have terms in the contracts to prevent large-scale downloads via api .
so in essence , you are most likely running into commercial limitations of the google finance api rather than technical limitations of the ` pandas ` ` datareader ` .
thank you for the response . i suspected that it was not a problem with the pandas datareader .
happy to help . did this answer your question ?
yes , you did answer my question . i appreciate your time effort to respond .

speeding up past-60-day mean in pandas
i use data from a past kaggle challenge based on panel data across a number of stores and a period spanning 2.5 years . each observation includes the number of customers for a given store-date . for each store-date , my objective is to compute the average number of customers that visited this store during the past 60 days .
below is code that does exactly what i need . however , it lasts forever - it would take a night to process the c.800k rows . i am looking for a clever way to achieve the same objective faster .
i have included 5 observations of the initial dataset with the relevant variables : store id ( store ) , date and number of customers ( " customers ") .
note :
for each row in the iteration , i end up writing the results using .loc instead of e.g. row [ " lagged no of customers "] because " row " does not write anything in the cells . i wonder why that's the case .
i normally populate new columns using " apply , axis = 1 " so i would really appreciate any solution based on that . i found that " apply " works fine when for each row , computation is done across columns using values at the same row level . however , i don't know how an " apply " function can involve different rows , which is what this problem requires . the only exception i have seen so far is " diff " , which is not useful here .
thanks .
sample data : #code
code that works but is incredibly slow : #code
why do you take the first 100 rows of ` data ` ? ( ` data [: 100 ]`)
@user ` time_condition ` is just a mask that selects the right time window , which is subsequently used to create sub_df ; @user z i take 100 rows to not spend the night waiting for the output but the objective is to have the output for the entire dataset
in that case i think it's best to leave the ` [: 100 ]` out of the code sample in your question . after all , your sample code is supposed to illustrate the problem you're having - in this case , being too slow . it's not a big deal , but if you find yourself editing again for some other reason , you might as well consider taking it out .
@user just updated
given your small sample data , i used a two day rolling average instead of 60 days . #code
by taking a pivot of the data with dates as your index and stores as your columns , you can simply take a rolling average . you then need to stack the stores to get the data back into the correct shape .
here is some sample output of the original data prior to the final stack : #code
after `.stack('store ')` , this becomes : #code
assuming the above is named ` df ` , you can then merge it back into your original data as follows : #code
edit :
there is a clear seasonal pattern in the data for which you may want to make adjustments . in any case , you probably want your rolling average to be in multiples of seven to represent even weeks . i've used a time window of 63 days in the example below ( 9 weeks ) .
in order to avoid losing data on stores that just open ( and those at the start of the time period ) , you can specify ` min_periods=1 ` in the rolling mean function . this will give you the average value over all available observations for your given time window #code
to more clearly see what is going on , here is a toy example : #code
the window is four observations , but note that the final value of 5.5 equals ( 5 + 6 ) / 2 . the 4.0 and 4.5 values are ( 3 + 4 + 5 ) / 3 and ( 4 + 5 ) / 2 , respectively .
in our example , the nan rows of the pivot table do not get merged back into ` df ` because we did a left join and all the rows in ` df ` have one or more customers .
you can view a chart of the rolling data as follows : #code
thanks i had never seen this before :) rolling_sum could also be useful for other similar problems i had in the past . i will try it asap on the full dataset to test speed .
i just tested it on the 5.66mb training data file . less than half a second . you then need to merge it back it .
i was working on a solution like this as well , but it doesn't seem to do quite the same thing as the reference implementation in the question . that code , when taking the mean , divides by the number of dates within the past 60 days for which there is actually data in the data set - a number that is typically less than 60 . ( this number is placed in the " no of dates " column ) ` rolling_mean ` appears to actually duplicate data into the empty rows and then divide by 60 , or something like that . in any case , my tests show that the results are not quite the same .
@user : using min_period to fill in nan is good if e.g. one wants to remove seasonality in data . i , however , want to use the rolling mean to create a feature to feed into a ml model . i can't use rolling_mean for store-dates not in the initial dataset , which min_period does . therefore , i loop over the 1115 stores and apply your solution ( without min_periods ) , which is still much faster than my initial attempt . thanks for your help

pandas to_sql trying to index nullable column
i want to set up a job that dumps data into a sql table every day , overwriting the existing data . #code
however behind the scenes sqlalchemy is trying to create the table with idcolumn varchar ( max ) , and being nullable . so sql throws an error when it tries to create the index .
it's pretty trivial to truncate the table before i write the data to it , but i feel like there should be a more elegant solution to this problem .
consider using the dtype argument which takes a dictionary mapping data frame column names to specified sqlalchemy data types . you can try ` varchar ` : #code
or generic ` string ` type , specifying a length : #code
if you want the write the index to the sql table as a normal column , you can do a ` reset_index ` before the ` to_sql ` call : #code
the only problem is the name of that column , if you want a custom one you first have to set the index name ( ` df.index.name = ' idcolumn '`) or rename after the reset_index .

copying values between columns in pandas
i have a data frame with the columns ` name ` and ` name ` .
and i'm doing this : #code
but i get this warning :
a value is trying to be set on a copy of a slice from a dataframe
and also it takes a while to run , it's that related with the warning ?
how should i do it ?
you can try ` loc ` : #code
and very nice explanation of ` settingwithcopywarning ` .
so generally : #code
you don't want to use chained assignments . you should use ` .loc ` : #code
refer to this so post for more information .
and this is the link to the relevant pandas documentation " why does the assignment when using chained indexing fail ! " .

pandas rolling_apply typeerror : int object is not iterable "
i have a function saved and defined in a different script called techanalisys.py this function just outputs a scalar , so i plan to use pd.rolling_apply() to generate a new column into the original dataframe ( ` df `) .
the function works fine when executed , but i have problems when using the ` rolling_apply() ` application.this link passing arguments to rolling_apply shows how you should do it , and that is how i think it my code is but it still shows the error " typeerror : int object is not iterable " appears
this is the function ( located in the script techanalisys.py ) #code
and now this is the main script : #code
the following works : #code
this doesn't work : #code
whats wrong in the code ?
it's possible that there are other problems with your code , but at least one stands out to me . the documentation for ` rolling_apply ` states :
args : tuple passed on to func
however , in your code you use #code
note that ` ( 50 )` is not a tuple ; you probably meant ` ( 50 , )` .
thanks @user tavory , i tried that before asking but it gives the error df2['ret '] = 1 - df.px_last / df.px_last.shift ( 1 )
attributeerror : ' numpy.ndarray ' object has no attribute ' px_last '
@user actually , it seems to have been a bona-fide bug with your code , i must say . the accepted answer corrected this as well , and your comment above simply indicated that once it was corrected , the next bug sprang up .
if you check the ` type ` of ` df ` within the ` hurst ` function , you'll see that ` rolling_apply ` passes it as ` numpy.array ` .
if you create a ` dataframe ` from this ` numpy.array ` inside ` rolling_apply ` , it works . i also used a longer window because there were only 15 values per array but you seemed to be planning on using the last 50 days . #code
thanks for the answer !

multiplying multi index pandas series and dataframe
i have the following dataframe : #code
and i have the following series : #code
i want to multiply each element in the series by the appropriate element in the series .
right now i can do that by converting the series to a dataframe and then joining : #code
is this the best way to do this ? is there a way to do it without having to use the join ?
you can use ` mul ` with parameter ` level ` : #code

python / scikit-learn - how to actually predict ?
i have the following dataframe , which i call main_frame : #code
i've built the following model : #code
i know that to predict , i should use now model.predict() , however i'm having a hard time to understand how does the arguments of the predict function work . i'm trying to use : #code
but that keeps getting me an error , i believe that i'm misunderstanding something related to the argument .
how do i set up the command for the prediction to work ?
edit
i'm adding the traceback #code
edit 2
trying to put my question in other words so it gets better to be answered :
considering the above model , how do i find out what is going to be the predicted value for the next period for the target variable ?
print prediction ? ( predict is not defined )
don't just say " an error " . always include the complete error message in your question .
your shouldn't predict based on data from ` target ` but from new samples ( what you called ` predictor ` before )
you are passing the wrong argument into the predict function . try this : #code
note that the model has been trained using the " predictor " variable . so you can only predict data that have the exact same amount of columns as the " predictor " variable .
thanks . however prediction is getting me a long array and all i want is to find out the predicted value for the n+1 period . how do i find that out ?
you will need to pass in the predictor values for the n+1 period . so you will need to get the column values of the second value column , 1lag , 2lag , 3lag and 4lag columns . then the model can give you the predicted value for the first value column .
so with the data in your example , you should train the model with all the rows except for the last one ( so everything except period 2006-02-01 ) . then you use your model to only predict that last row .
thanks a lot , that was the missing detail i couldn't find anywhere . so that long array that i was getting inside " prediction " is the prediction value for each period considering the regression that the fit has created , right ?

finding highest values in each row in a data frame for python
i'd like to find the highest values in each row and return the column header for the value in python . for example , i'd like to find the top two in each row : #code
i'd like my for my output to look like this : #code
i'm not sure i understand . are there numbers missing from the output ? you might be looking for the ` nlargest ` method .
the output i'd like to show is the corresponding column header . so , 9 and 8 in the first row of the input are columns b and c . hope this makes sense . i've messed around with the nlargest method but haven't figured it out yet . i may be doing it wrong though .
you can use a dictionary comprehension to generate the ` largest_n ` values in each row of the dataframe . i transposed the dataframe and then applied ` nlargest ` to each of the columns . i used ` .index .tolist() ` to extract the desired ` top_n ` columns . finally , i transposed this result to get the dataframe back into the desired shape . #code
wow , that works perfectly . thank you very much .

using python csv to recreate pandas data frame functionality
i'm using pandas to work with csv files , and my code to handle the csv contents looks like this : #code
with these 2 lines of code i get a key : value pair for each column in my csv ; the key being the header and the values being the remaining rows placed in a list .
how would i do this using the python csv module ?
it seems much less straightforward using the csv module when working column by column , though it's just as likely that i just don't understand how to use it otherwise ....
there are already lots of questions about extracting columns from csv files using the ` csv ` module . did something go wrong when you tried something similar ?
sorry your right , it is a duplicate . my first question and already i'm in the negative ! not sure if the linked post above answers it or not , as far as i can tell it's answering something different . but this answer helped me : #url thank you !

lookup table errors
i created a dataframe for conforming mortgage limits by year and unit type . #code
imported using the following code : #code
however , when i attempt to perform a lookup : #code
or #code
i get the following error messages , respectively : #code
and #code
i am hoping to get the value " 240000 " . i know this may be a trivial question but any assistance you can offer would be greatly appreciated .
i think you need .get_value instead of .lookup
from docs :
dataframe . lookup ( row_labels , col_labels )
akin to : #code
in first argument you should pass row_labels , i.e. index . you should have your ` year ` column as index or you could do that with `set_index('year ')` : #code
in ` lookup ` method you passing your labels to ` zip ` function and they should be iterable . you could do that if you'll pass ` list ` or ` tuple ` . #code
btw , if you just need to find value in column ` one_unit ` with ` year ` column equals to 1999 you could do following ( if your ` year ` column isn't an index ): #code

convert integer series to timedelta in pandas
i have a data frame in pandas which includes number of days since an event occurred . i want to create a new column that calculates the date of the event by subtracting the number of days from the current date . every time i attempt to apply ` pd.offsets.day ` or ` pd.timedelta ` i get an error stating that series are an unsupported type . this also occurs when i use ` apply ` . when i use ` map ` i receive a runtime error saying " maximum recursion depth exceeded while calling a python object " .
for example , assume my data frame looked like this : #code
i want to create a new column with the date of the event , so my expected outcome ( using today's date of 12 / 29 / 2015 ) #code
i have attempted multiple ways to do this , but have received errors for each .
one method i tried was : #code
with this i received an error saying that series are an unsupported type .
i tried the above with ` .map ` instead of ` .apply ` , and received the error that " maximum recursion depth exceeded while calling a python object " .
i also attempted to convert the days into timedelta , such as : #code
this also received an error referencing the series being an unsupported type .
post code , issues could be anything
first , to convert the column with integers to a timedelta , you can use ` to_timedelta ` : #code
then you can create a new column with the current date and substract those timedelta's : #code

what is a proper idiom in pandas for creating a dataframes from the output of a apply function on a df ?
edit --- i've made some progress , and discovered the drop_duplicates method in pandas , which saves some custom duplicate removal functions i created .
this changes the question in a couple of ways , b / c it changes my initial requirements .
one of the operations i need to conduct is grabbing the latest feed entries --- the feed urls exist in a column in a data frame . once i've done the apply i get feed objects back : #code
so , now i'm stuck with the feed entries in the " entries " column , i'd like to create a two new data frames in one apply method , and concatenate the two frames immediately .
i've expressed the equivalent in a for loop : #code
i realize i could do this in a for loop ( and indeed have written that ) , but i feel there is some better , more succinct pandas idiomatic way of writing this statement .

getting boolean nullable pandas column
how can i create pandas dataframe with column dtype being bool ( or int for that matter ) with support for nan / missing values .
when i try like this : #code
column ` one ` is implicitly converted to object . likewise similar for ` ints ` : #code
` one ` is here implicitly converted to ` float64 ` , and i'd prefer if i stayed in ` int ` domain and not handle floating point arithmetic with its idiosyncrasies ( always have tolerance when comparing , rounding errors , etc . )
pandas doc explains why it's not possible to do what you're asking : #url
could you use an int flag ( -999 ) or some other approach rather than nan ? ( what are you trying to achieve ? )
ah the venerable -999 . frequent in scientific datasets , scourge of naive grad students .

python flatten dataframe with multiple columns all n-length
i am looking to flatten out a ` dataframe ` where there are multiple groups ( below : `['a ' , ' b ' , ' c ']`) of columns , each n columns long ( below : n=2 ) . there is also stagnant data which does not need to be flattened ( below : [ ' misc ' , ' year ']) . below is an example ` dataframe ` : #code
produces the following : #code
i want the output to be : #code
so ` [ ai , bi , ci ]` moves to a single ` row ` while keeping [ misc , year ] . i am working with thousands of 20,000 row datasets so performance is a big issue . i currently am looping per row to separate them , but was hoping there is a better python function for flattening . i have seen panda's ' melt ' function but it seems to only work if there is a single group .
ultimately i want to create a helper function which would accept an arbitrary number of ' group ' columns , ' stagnant ' columns , and value for ' n ' .
i am currently using pandas but am open to other solutions as well . thanks for the help ! :)
this is not a typical application for reshape / melt type functions , so you're probably going to have to roll your own . here's a solution that should be relatively performant provided ` ( # groups ) *n ` is not too big :
make two dataframes , one with columns ` [ misc , year , a1 , b1 , c1 ]` , the other with ` [ misc , year , a2 , b2 , c2 ]` and concatenate them vertically .
this can be automated for arbitrary numbers of groups and n values , provided that the column names have a uniform convention like ` letter number ` as in your example . you'll have to do a bit of regex parsing on the column names to determine which columns go together in each data frame .
make a list called ` subframes ` of all these dataframes and concatenate them together with ` pd.concat ( subframes )` .
indeed looks like a ` pd.concat ` could be helpful . this solutions allows for more than 2 ` columns ` per letter and number sequences of different length . starting with : #code
parse ` column ` names ( may not need this if you know the structure beforehand ) #code
build dictionary to rename ` columns ` for proper application of ` concat ` : #code
and concatenate the ` dataframe ` slices - using ` .filter ` for some error tolerance in case not all letters come with the same numeric sequence : #code
to get : #code
how's about this : #code
you could do this as a comprehension , or function , more generally : #code
if you want to eke out some more performance , you're going to want to do this concat in numpy and then repeat the index ( though i'm not convinced it's worth the small gain that will give you ) .
thanks andy ! what if i want to preserve the ' ith ' column index ? is it possible to create a brand new column , say ' n_index ' , within the concat function and assign it the value of str ( i ) ?
@user iiuc i would ( lazily ) do this with ` .sort_index() ` , then ` .reset_index() ` . in some sense the index you wanted wasn't really coming from anywhere and i don't think it can be done during concat .
well i actually intend to perform some logic on it . i want to use the ' year ' column and the ' ith ' number to get a year-month combination . for example : i=2 year=1992 would get put into a ' date ' column as ' feb-1992 ' . any suggestions ?
i figured it out . i created each dataframe in a for loop , appending each to a list , then concatenating them all after the for loop .

python : import csv files , set dict name as filename , headers as keys , and columns as values
i have a directory containing folders and inside each folder is about 20 csv files , each with a different file name ; files contain columns a , b , c , d , e and each column contains numerical data ( some entries are nan ) . what i want is each filename to be the name of a dictionary , where a , b , c , d , e are keys in that dictionary , and the data below are the values to those keys .
as an example : #code
the numbers in these columns are not necessary int , if that matters .
i think i figured out how to set the dict name as filename with a for-loop , that is : #code
i found a way to read in the values from the csvs using #url
that is , #code
ideally this will read in the csvs one at a time .
but i'm not sure how this will help me . i took a look at the pandas dataframe.to_dict() but i don't think the above code reads into a dataframe ( or , if it does , i don't understand the documentation well enough ) . it looks like it'll only store one value per key at a time . another thread i was reading says it's possible to store more than one value per key , though ( using .append() ) but i don't know how to apply it to this situation .
any help is appreciated , thank you
this should get you started : #code
you can't really do the first part - " each filename to be the name of a dictionary " - without some really dark magic . and no , your first code sample does not actually work :) and if it worked it would be a really bad idea - consider a directory containing files named ` sys ` , ` os ` etc . :)
but you can nest dictionaries inside dictionaries , which more or less achieves what you want , only with slightly different syntax : #code
( for the " how to read a csv file " part refer to the @user ' s answer who posted a bit quicker than me )
supposing the directory would never contain files with those names ( it wouldn't ) ... is it just really bad practice to do something like this ?
would there be any issues with ram if i did something this way ? the csvs are really huge .
what if i named each dictionary ticker_fname , where fname is whatever file python is trying to read in on that loop instance ? i heard eval() can do some dark magic and renaming like this should avoid any conflicts .
i tried
eval('ticker_ %s ' % fname = {} )
but obviously it doesn't work ;)
yes , it would be a really bad , unexpected and unusual practice , and yes , you can use eval to do this ( i guess ) , and no , it would not affect performance or memory consumption too much
i'd rather not have to load in that massive dictionary every time i wanted to access it , though ; it would be easier for me just to call individual dictionaries . i read up on eval() and exec() and i can see how there can be dangers in using it . i appreciate the help !
the code
this works : #code
walk down your paths with ` os.path.walk ` and use pandas .
you need to create the full path , otherwise pandas cannot find the file because the file is not in the current working directory .
you need to tell ` pd.read_csc() ` to use white space as delimiter . the method `to_dict(orient='list ')` converts the data frame into a dictionary with th column names as keys and a list of float values . pandas takes care of nans .
if you get deeper into pandas , you might want to keep the data frames without converting them to dictionaries . for example , this line : #code
gives you this data frame :
it allows a lot of sophisticated operations .
the sample data
` file1.txt ` : #code
` file2.txt ` : #code
the output
now ` data ` contains : #code

performing statistical test on every possible file combination in a folder
i have a folder with about 100 csv files . i want to use a two sampled kolmogorov-smirnov test on every possible file combination . i can do this manually like this : #code
but i don't want to manually assign all the variables . is there a way to iterate through the files and compare all the possible combinations using the statistical test ?
yes , it's called a " nested loop " .
sounds like you want to get the cartesian product of a list of filenames with itself .
cartesian product of lists in python
in your implementation , you should have a list of all the filenames in a list , and then call #code
in the documentation for ` itertools.product ` it mentions that it is the same as #code
sounds just like what i am looking for , i will read the docs about it , thanks .
you can also gather the file objects in a dictionary . #code
to access the files , just call their key #code

pandas dataframe column with both strings and floats
i have a dataframe where one of the columns holds strings and floats .
the column is named ' value ' has values like " aaa " , " korea , republic of " , " 123,456 .78 " and " 5000.00 " .
the first two values are obviously strings , and the last is obviously a float . the third value should be a float as well , but due to the commas , the next step of my code sees it as a string .
is there an easy way for me to remove the commas for those values that are really floats but keep them for values that are really strings ? so " korea , republic of " stays , but " 123 , 456 , 78 " converts to " 123456.78 " .
thanks .
show the date dataframe and your code .
all your values are strings . one column in pandas can only have one data type . you need two columns one for strings and on for floats .
to begin with , your pandas column does not contain strings and floats , since columns contain homogeneous types . if one entry is a string , then all of them are . you can verify this by doing something like ( assuming the dataframe is ` df ` and the column is ` c `) : #code
and noticing that the type should be something like ` object ` .
having said that , you can convert the string column to a different string column , where the strings representing numbers , have the commas removed . this might be useful for further operations , e.g. , when you wish to see which entries can be converted to ` float ` s . this can be done as follows .
first , write a function like : #code
then , you can do something like : #code
again , it's important to note that `df.c`'s type will be string .

the most elegant way to get back from pandas.df_dummies
from a dataframe with numerical and nominal data : #code
get_dummies convert categorical variable into dummy / indicator variables : #code
what's the most elegant back_from_dummies way to get back from df_dummies to df ? #code
get back to df ? not sure what you mean exactly .
i just specify get back / revert
thanks . just wanted to make sure .
` idxmax ` will do it pretty easily . #code
firstly , seperate the columns : #code
this allows you to slice into the different frames for each dummied column : #code
now we can use numpy's argmax : #code
* note : pandas idxmax returns the label , we want the position so that we can use categoricals . * #code
now we can put this all together : #code
and putting back the non-dummied columns : #code
as a function : #code

pandas : apply returns list
my dataframe is as follows : #code
i split it into groups with : #code
i have the following function that i want to apply to each group : #code
calling #code
results in the following : #code
how do i make the results of the apply operation the values of the " mean_to_date " column ? that is , the mean_to_date for player 200 , season 21999 would be 0 and 10 , then for player 200 , season 21200 it would be 0 , 10 , and 15 , and so forth . note that the mean_to_date value represents the mean prior to the game , so before the 1st game it is zero , and before the second game it is the total from the first game .
also , the " previous_mean " function is ugly and there is probably a more efficient way to accomplish the same end , but i couldn't figure it out as i am new to using pandas . thanks for any suggestions !
you're getting lists back because your function ` previous_mean ` , when fed a dataframe , returns a list -- it has nothing to do with ` apply ` .
also you need to post workable code . right now ` avgs ` is used before you define it in ` previous_mean `
i fixed the code by adding the empty dictionary . so should the function return a pandas series instead of a list ?
you said in another comment that you want one mean per row , therefore the function you apply should return a single value .
iiuc you can use ` expanding_mean ` , shift data by ` shift ` to ` 1 ` , fill ` nan ` to ` 0 ` by ` fillna ` and return column ` mean_to_date ` : #code
if solution is helpful , you can upvote and accept . thanks .
this is helpful but i need one mean per row , rather than a list / series of values .
ok , solution is modified . thanks .
thank you - i accepted this as the answer .

accessing data from cartesian list of files
i have a numerous csv files in a folder that i have turned into a cartesian list as such : #code
the contents of result are like this : #code
how would i even do something as simple as printing out the contents of one item in ` result ` ? as of now they are just recognized as strings from what i can tell .
do you want read the file content ?
exactly , eventually i want to do a statistical test on each item in ` result ` but i don't know how to recognize the files for what they are and not strings
what type are these files ? csv ?
yes they are csv's .
iterating on each item may help #code
as mentioned in the comments , there are probably better ways to achieve your goals , depending on what they are .
e.g use one or more specific libraries like ` csv ` , ` numpy ` , ` pandas ` , etc .
iiuc you need to print content of the files which are in csv format . you could do that with ` pandas ` ` read_csv ` : #code
note : you need to pass full path to the files in ` read_csv ` function . you could do that with ` os.path.abspath ` to the each element e.g. ` pd.read_csv ( os.path.abspath ( files [ 0 ]))`

query pandas dataframe with integer in first level of multiindex
i'm having trouble with pandas multiindex , if the first index is an integer .
i could not find this question , so maybe i'm doing something wrong here ?
i use pandas version ' 0.16.2 '
example :
in : #code
out : #code
but with integers in the first index-level it doesn't seem to work : #code
out : #code
use ` loc ` instead of ` ix ` : #code
for ix it's strange why it's not working because from docs :
however , when an axis is integer based , only label based access and not positional access is supported
thus , in such cases , it s usually better to be explicit and use .iloc or .loc .
you index values are integer so they are should be analysed as labels : #code
but they recommended to use ` loc ` in such cases to be more explicit .
thanks anton , but do you know why is it different with integers ? in my opinion this is pretty confusing ...
@user ` .ix ` only works with labels when the index is integer-based . that is indeed confusing and the source of the motivation that created ` loc ` and ` iloc ` .
@user : i see - so ` .ix ` it works fine with ` data =p d.dataframe ( ... index = [ 1 , 1 , 1 , 1 , 2 , 2 , 2 , 2 , 3 , 3 ])` but not with multiindex's . are we supposed to use ` .ix ` at all ?
i never use it . my understanding is that it was retained for backwards compatibility , but not much else . @user
@user ok thank you very much !

weird error while reading xls file in python xlrd and pd read excel
i am trying to read an excel ( .xls ) file from my local drive of windows machine , which will be saved from a mail attachment .
the modules in python ( 2.7 ) , used for this purpose is xlrd ( 0.9.3 ) and pandas ( 0.17.0 ) . below is the sample code used . #code
when first ran , both the read_excel and open_workbook statements gave some error like this .
compdocerror : workbook corruption : seen [ 2 ] == 4
what is weird to me though is that , i manually opened that xls file , saved back in the same location , and when i run the same xlrd pandas commands , they are working fine .
does it have something to do with the origin of the file type . the file and code which is shown here is just sample . the actual file will be saved in the local machine from a mail attachment normally from outlook . ms excel 2010 is installed in my machine .
thanks in advance ,
bala

keeping just the hh : mm : ss from a time delta
i have a column of timedeltas which have the attributes listed here . i want the output in my pandas table to go from : #code
to : #code
how can i drop the date from this output ?
not really , i am just looking for hour and smaller for these calculations .
you could get seconds of the day with ` dt.seconds ` and then pass that to ` pd.timedelta ` : #code
this seems like a promising solution . however , i have some timedeltas that are nan - is there a better way of getting around these than error exception ?
you could use ` dropna ` of ` fillna ` methods to drop or fill with values what you like
you could subtract the number of days from each timedelta : #code
yields #code
column ` a ` shows the original timedelta . column ` b ` shows the result after subtracting the whole days . the ` truncated ` and ` rounded ` columns shows the result after dropping or rounding off fractional seconds .
calling `astype('timedelta64 [ d ]')` truncates the numpy timedelta64s to whole days .
similarly , calling `astype('timedelta64 [ s ]')` truncates the numpy timedelta64s to whole seconds . see the numpy docs for more on datetime64 / timedelta64 arithmetic .
an alternative way to subtract the days is to use : #code
but this turns out to be slower : #code
an alternative way to round to the nearest second is : #code
but again this is slower : #code

create open bounds indicators from pandas get_dummies on discretized numerical
from a numeric age pandas column , discretize as aged with qcut , we create open bounds from the qcut bounds : #code
from index ([ u ' [ 5 , 30 ]' , u ' ( 30 , 70 ]'] , dtype='object ') we make bopens : #code
then we convert categorical variable into dummy / indicator variables with get_dummies : #code
i want to enrich the data frame with the open bounds columns , df.shape will be
quite big , ~ ( 10e6 , 32 ) . what is the best way to make for each line the 6 bopen cols ?
the target df will look like this one : #code
ps : the get_open_bounds used to make bopens : #code
iiuc , you can do this with a bit of broadcasting : #code
gives me #code
note #1 : by adding ` retbins = true ` i can get the bins themselves and avoid some awkward string parsing .
note #2 : by doing an implicit " right = 1 - left " , i'm assuming that no ages are nan and so one of > = or must be true ; if that's not certain , you could do ` right = ( df [ " age "] .values bins [: , none ] .t .astype ( int )` instead . )
note #3 : really i should be passing the frame constructors df.index as well -- while your example had the canonical index , that may not be true in your real data .
what does mean passing the df.index to the frame constructor ?
@user : dataframe takes several arguments , including ` data ` ( what it's going to be filled with ) , ` columns ` ( the names of the columns ) and ` index ` . i didn't pass ` index ` , so ` dl ` and ` dr ` get indices 0 , 1 , 2 , 3 .. if your ` df ` doesn't have 0 , 1 , 2 , 3 .. etc , then the concatenation will give unexpected results because the indices won't match .

creating column with difference in two rows in pandas
i have a pandas dataframe with a hash in one column and dates in another column . i want to create a new column with age , i.e. , difference between the first date with a particular hash and the current date . as an example , the dataframe ` inf ` contains #code
i add a column called age to this by #code
now i want to convert this to #code
where the last column is the difference between a particular id's first date and the date in that row . i am currently using this to perform this : #code
this is working but is disgustingly slow . took over an hour for 100,000 records . is there a better way to perform this ?
by current date are you saying today date like ` datetime.now() `
i meant the date in that particular row . have edited question accordingly .
you could use ` groupby ` method for your hashes , then ` transform ` method for ` date ` column and ` iloc ` to get the first element . you also will need to convert your date column to datetime with ` pd.to_datetime ` : #code
i'm not sure why it printing from the begining for 1970-01-01 but you could fix that with substracting from that `pd.timestamp('1970 - 01-01 ')` #code
if you need only values you could use ` dt.days ` and then pass it to your new column ` age ` : #code
that works way faster . thanks !
similar to anton's , slightly different flavor : #code

how to count categorical timeseries data in pandas
this week i decided to dive a bit into pandas . i have a pandas dataframe with historical irc logs that looks like this : #code
there are roughly 5.5m rows and i'm trying to make some basic visualizations like rank over time for the top 25 nicks and that sort of thing . i know i can get the top 25 nicks like this : #code
what i want is a rolling count like this : #code
so that i can plot an area graph of messages from the beginning of time for the top 25 nicks . i know i can do this by just iterating over the entire dataframe and keeping a count but since the whole point of doing this is to learn to use pandas i was hoping there would be a more idiomatic way to do it . it would also be nice to have the same data but with ranks rather than running counts like this : #code
iiuc you need ` crosstab ` and ` cumsum ` : #code
i dont understand last rank dataframe . can you explain it ?

time series manipulation
so i have a dataframe that i dump a time series into . the index is the date . i need to do calculations based on date .
for eg . i have { #code
how would i go about doing say ... percentage change of beginning to end of the month ? how would i construct a loop to cycle through the months ?
any help will be met with huge appreciation . thank you .
first create year and month columns : #code
group by them : #code
define the function you want to run on the groups : #code
apply the function to the groups : #code
let me know if it works .

comparing dtyped [ float64 ] array with a scalar of type [ bool ] in pandas dataframe
i have a pandas df where i need to filter out some rows that contains values == 0 for feature ' a ' and feature ' b ' .
in order to inspect the values , i run the following : #code
which returns the right values . similarly , by doing this : #code
i can see the 0 values for feature ' b ' .
however , if i try to combine these 2 in a single line of code using the or operand : #code
i get this : #code
what's happening here ?
you can transform either column ' a ' or ' b ' so they are both either float64 or bool . however , an easier solution that preserves the data type of your features is this : #code
a common operation is the use of boolean vectors to filter the data . the operators are : | for or , for and , and ~ for not . these must be grouped by using parentheses .

how to apply a function ( bigramcollocationfinder ) to pandas dataframe
i am not very used to programming and need some help to solve a problem .
i have a .csv with 4 columns and about 5k rows , filled with questions and answers .
i want to find word collocations in each cell .
starting point : pandas dataframe with 4 columns and about 5k rows . ( id , title , body , body2 )
goal : dataframe with 7 columns ( id , title , title-collocations , body , body_collocations , body2 , body2-collocations ) and applied a function on each of its rows .
i have found an example for bigramm collocation in the nltk documentation . #code
i want to adapt this function to my pandas dataframe . i am aware of the apply function for pandas dataframes , but can't manage to get it work .
this is my test-approach for one of the columns : #code
but if i print that out for an example row i get #code
i am not even sure if this is the right way . can someone point me to the right direction ?
you could print the variables of your lambda function in order to find out if it is in the proper format for your function . maybe you have to do something like ' x.values '
can you add [ minimal , complete , and verifiable example ] ( #url ) ?
i will keep that in mind for the next time and edit this later . thx everybody .
if you want to apply ` bigramcollocationfinder.from_words() ` to each ` value ` in the ` body ` ` column , you'd have to do : #code
in essence , ` apply ` allows you to loop through the ` rows ` and provide the corresponding ` value ` of the ` body ` ` column ` to the applied function .
but as suggested in the comments , providing a data sample would make it easier to address your specific case .
thx , for the answer . i guess the question i asked was not perfectly phrased . but your answer still helped me to find a solution . sometimes its good to take a short break :-)
if someone is interested in the answer . this worked out for me . #code

matplotlib style not working in pandas bar plot
the code i used to produce the bar plot as per pandas visualization tutorial . #code
i get :
i was expecting to get the bar colours as in the tutorial ( vermilion ) , instead the bars are the default matplotlib blue colour .
i suspect the problem is in pandas . colours are correct when pandas is not used . #code
i created my environment using conda . #code
how can i get pandas plot the right colours ?
well , i don't get the ` default matplotlib blue ` color when i run your code on my machine .
what version of matplotlib are you using ? it sounds like [ this bug in pandas ] ( #url ) , which affects matplotlib 1.5.0 .
fwiw , i do get a blue bar plot . mpl 1.5.0 , pd 0.17.1 . and yes , that bug report looks like your problem . it does show a solution in one of the replies ; hope that works for you .
@user mpl 1.5.0 . thank you for the link - i found the solution .
@user can you please write up an answer to your own question ?
@user done . i hope it helps :)
as ali_m pointed out in the comments , it is the bug in the latest version of pandas ( 0.17.1 ) when using matplotlib version 1.5.0 .
the fix will be in the next pandas release .
until the new release is out , i ran #code
in my conda environment to get the latest fixes .

matplotlib key error : 0 when annotating
in this bit of code , i am trying to create a bar chart with one bar ( as the " total " bar which will be placed next to other bars ) with a slice of a data frame . it works for the other slice of the data frame , but for the " total " slice ( which is just one row ) , i keep getting this " key error : 0 " : #code
... #code
here's the error : #code
i'll send a complete example if needed , but i just wanted to see if the answer is obvious enough to not do so .
thanks in advance !
checkout x_4t [ 0 ] and ind_4t [ 0 ] and see which one of them ( if any ) causes the problem
tl ;d r ; use ` iloc ` #code
the problem is that ` pd.series ` use index not position indexing via ` [ ]` . this is a powerful idea / bit of syntax as frequently when you are working with a ` series ` the value of the index is the interesting thing , not the position in the ` series ` ( ex a date index ) , however if you expect that ` series ` / ` dataframes ` will behave exactly like a numpy array you will have issues ( i speak from experience here ) .

converting tuple into dataframe
i have a tuple like this : #code
and i want to turn it into a dataframe like this : #code
i have been trying to use this : #code
but this error is returned : #code
you can use ` list ( sum ( x , ( )))` to flatten your ` tuple ` of ` tuples ` : #code
#code

pandas i want to drop rows with nat based on another column's value
i have a df that looks like : #code
i only want where the type equals 1 and where the end time isn't nat , so all that should be returned is : #code
what is the best way to go around this ? the final step that i want to implement is that the rows that fit this criteria will have their start time amended . eg this row above will then be transformed to : #code
what does mean ` have their start time amended ` ? can you add sample ?
amended . i was thinking that would probably be within a if statement to for loop but i know with pandas that there are easier ways to do this .
you can try ` notnull ` : #code
iiuc then you can : #code
or if you want custom ` timedelta ` : #code
if my solution was helpful , you can upvote and accept . thanks .
#code
` notnull ` is more readable than a negated ` isnull ` imo

pandas groupby slice of a string
i have a dataframe where i want to group by the first part of an id field . for example , say i have the following : #code
how can i group by the first letter of the id field ?
i can currently do this by creating a new column and then grouping on that , but i imagine there is a more efficient way : #code
you will need to create a grouping key somehow , just not necessarily on the dataframe itself , for eg : #code
thanks , exactly what i was looking for . didn't realize you could group on a series like that .
or ` .str [ 0 ]` as you just want the first
@user don't forget that there's a distinct difference between ` .str [ 0 ]` and ` .str [: 1 ]` in the case of empty strings ...

python pandas compare 2 large dataframes of text for similarity
i have two large dataframes i want to compare . i want a comparison result capable of a column and / or row wise comparison of similarities by percent . this part is simple . however , i want to be able to make the comparison ignore differences based upon value criteria . a small example is below . #code
my problem is that for when `value1='aa ' and value2 = ' -- '` i want them to be viewed as equal ( so when one is `' -- '` basically always be true ) but , otherwise perform a normal boolean comparison . i need an efficient way to do this that doesn't include excessive looping as the datasets are quite large .
hmmm , is possible replace ` -- ` to ` aa ` and then compare ?
i think loop comprehension should be quite fast : #code
this outputs the full true / false df .
below , i'm interpreting " when one is ' -- ' basically always be true " to mean that any comparison against `' -- '` ( no matter what the other value is ) should return true . in that case , you could use #code
to find every location where either ` df1 ` or ` df2 ` is equal to `' -- '` and then use #code
to update ` comparison ` . for example , #code
thanks unutbu ! that worked well . i had two dataframes , each with 1326 columns and 62831 rows . the code ran in 12.14 seconds and gave the correct results ( i previously had a very loop intensive base python script which took over 3 hours to compute ) .

slicing part of a column pandas
i have a dataframe with column ' a ' as a string . within ' a ' there are values like
name1-l89783
nametwo-l33009
i would like to make a new column ' b ' such that the ' -lxxxx ' is removed and all that remains is ' name1 ' and ' nametwo ' .
initialize ` dataframe ` . #code
` apply ` function over rows and put the result in a new column . #code
awesome ! i know this wasn't this original ask , but is there any way to make this work dynamically ? without creating the second column ?
sure ! if i get your meaning you'd just do : ` df [ 0 ] = df [ 0 ] .apply ( lambda x : x.split ( ' - ') [ 0 ])`
i get this warning : is this going to affect the program at all ? if not , how can i suppress it so the user doesn't see it ? ------------- a value is trying to be set on a copy of a slice from a dataframe .
try using .loc [ row_indexer , col_indexer ] = value instead
what version of pandas are you on ? the above code is tested to work on 0.17.0 with no warnings or errors .
newest version , 0.17.0 using anaconda as editor
use vectorised ` str.split ` for this and then use ` str ` again to access the array element of interest in this case the first element of the split : #code
like with all good questions , post raw input data , code to reproduce your df , your pandas , python , and numpy versions and the erroneous output
@user : i don't see how that can work . shouldn't it be ` .str .split ( " - ") .str [ 0 ]` ?
@user damn , you're right , i knew i overlooked something , will update

python max function comparing calculation of pandas columns and integer fails
i'm trying to create a new pandas dataframe column by subtracting an existing dataframe column column from another . however , if the result is a negative number , the new column value should be set to zero . #code
if i create a new dataframe column ' c ' by subtracting ' b ' from ' a ' , i get the right result . #code
however , if i utilize the max function to avoid results with a negative number , i get " valueerror : the truth value of a series is ambiguous . " #code
the expected output is : #code
what am i doing wrong ?
what is recommended output ?
in [8 ]: df
out [ 7 ]:
a b c
0 1 3 0
1 2 2 0
2 3 1 2
you need to use ` np.maximum ` to do element-wise maximum comparison : #code
the problem is ` max ` is that it essentially checks `(df['a '] - df['b ']) 0 ` . this returns an array of boolean values ( not a boolean ) , hence the error .
that works ... thank you .
use ` np.where ` : #code
the in-built ` max ` function operates on scalars not array-like structures hence the error
you could also do apply : #code

pandas.read_csv in spark environment ( ibm bluemix )
i'm using ` ipython ` in a ` spark / bluemix ` environment
i have a csv uploaded to the the object store and i can read it ok using ` sc.textfile ` but i get ` file does not exist ` when i use ` pandas pd.read_csv `
` data = sc.textfile ( " #url )`
` import pandas as pd `
` data = pd.read_csv ( ' #url )`
` ioerror file #url does not exist `
why is this ?
how can i read a csv file to a ` pandas ` dataframe ?
pandas reader supports only local filesystems . why do you need this ?
it relates to a big data analytics course project and demonstrating the use of spark / bluemix and map / reduce is a requirement . even though the file starts out on a local file system - i have to process it in spark / ipython .
just skip pandas and load data directly to spark : #url
but then i miss out on all the pre canned data analysis capabilities of pandas dataframes
spark and pandas are two completely different worlds . if your requirement is spark and distributed processing then pandas won't work . running pandas in the same interpreter doesn't make it distributed .
i managed to make some progress with .todf() on the file loaded with sc.textfile . i can do some distributed processing on this and then convert to a spark dataframe and then topandas() which has me back in familiar territory . i take your point about the two different purposes . thanks

how does one make lines thicker in pandas subplots
i am using pandas to plot 3 subplots in a single figure . the code below accomplishes this . however , i am having trouble making the data lines in the subplots thicker . anybody know how to do this ? #code
not uure if it works with pandas , but in pyplot you use ` linewidth= 2.0 ` #url
specify a line width ( ` lw `) parameter : #code
thanks for the help
if this worked for you , would you mine marking it as the accepted answer ?

how to count nan values in a pandas dataframe ?
what is the best way to account for ( not a number ) nan values in a pandas dataframe ?
the following code : #code
outputs : #code
while the desired output is : #code
i am using pandas 0.17 with python 3.5.0 with anaconda 2.4.0 .
tell ` value_counts ` not to drop nan values by setting ` dropna=false ` ( added in 0.14.1 ): #code
this allows the missing values in the column to be counted too : #code
the rest of your code should then work as you expect ( note that it's not necessary to call ` sum ` ; just ` print ( " nan : %d " % dfv [ np.nan ])` suffices ) .
and after using the method above
dfv.values.sum()
counts all the values , i.e .
6
thanks . ;)
no problem ! yep , that works . in fact , you could just write ` dfv.sum() ` to count all the values . or even more efficiently , just check ` len ( dfd )` .
to count just null values , you can use ` isnull() ` : #code
here ` a ` is the column name , and there are 2 occurrences of the null value in the column .

python pandas groupby multiple groups with binary class
i have a dataframe as following : #code
i want to group it into 3 groups , g1 : a , b , g2 : c , d , g3 : e , f .
is there a way to do so with looping over all the rows to assign a new class for each id ?
can you post the desired output so we can understand your question better ?
you can use ` diff ` , ` astype ` and ` cumsum ` : #code
testing performance :
these timings are going to be very dependent on the size of df as well as the number ( and position ) of ` 0 ` and ` 1 `) : #code
test ` len ( df ) = 10 ` : #code
test ` len ( df ) = 10000 ` : #code
iterate through ' class ' and start a new group every time the class is not the same as the previous one , for the example :
crete the df : #code
iterate through ' class ' to create the groups index : #code
add the group_index to the df : #code
and the output should be : #code
?
here is a one-liner code . :p
it utilizes differential information of adjacent rows and cumulative summation to assign group ids for each row . #code
now , you can use groupby() to obtain groupy object . #code
the complete code is attached . #code
output : #code

printing all rows in each level of multiindex pd.dataframe in one row
i have a dataframe which was converted into a multiindex dataframe after doing ` groupby() ` and aggregation . #code
for each player on a team , find the total number of trips and total time spent traveling . this returns a multiindex dataframe . #code
desired output :
i want to print the output such that all players on a team are on the same line . #code
this question was noted as too broad so i took the liberty of splitting the pandas dataframe creation aggregation from the output printing .
iterate through the 0th level ( team ) using ` groupby() ` . #code
for example at the second iteration , it will return a dataframe for ` team2 ` : #code
use ` reset_index() ` to drop the team index column and make the player index column as part of the dataframe . #code
convert that dataframe into a list of lists so we can iterate through each player . #code
when printing we have to map the integers to a string , and use the end parameter of the print function to print a semicolon instead of printing a new line at the end . #code
the full solution : #code
output : #code

how to round dates to week starts in pandas
i'm having a dataframe with a date column . how can i map each date ` d ` to the start day of the week containing ` d ` ?
#code

data frame in panda with time series data
i just started learning pandas . i came across this ; #code
i have understood what is the above data means and i tried with ipython : #code
is it correct way of creating a data frame ?
the next step given is to : return a series where the absolute difference between a number and the next number in the series is less than 0.5
do i need to find the difference between each random number generated and store only the sets where the abs diff is 0.5 ? can someone explain how can i do that in pandas ?
also i tried to plot the series as histogram with ; #code
the graph display the x as random number with y axis 0 to 18 ( which i don't understand ) . can some one explain this to me as well ?
given the date_range , r , that data frame can be created as : ` dataframe ( index=r , data=randn ( len ( r )) , columns=['random number generated '])` .
whats is the difference between the way i created and the way u suggested . can you explain me pls im new to pandas .
i was just pointing out that creating the series , s , isn't needed .
thanks :) dithal . can you please clarify me if you have any idea on the abs different thing
i don't really understand that part of you question .
ok . can you tell me the histogram pic i got . why it shows as 0 to 18 in y axis
the y-axis values of 0 to 18 are the bin counts . the bins are on the x-axis . to see what's going on , try ` hist ( bins=1 )` , you should get 72 on the y-axis , the length of the data .
to give you some pointers in addition to @user ' s comments : #code
as commented by @user , you can simplify the creation of your ` dataframe ` randomly sampled from the normal distribution like so : #code
to show only ` values ` that differ by less than ` 0.5 ` from the preceding value : #code
can simplify using ` .dropna() ` to get rid of the missing values .
the ` pandas.series.hist() ` docs inform that the default number of ` bins ` is ` 10 ` , so that's number of ` bars ` you should expect and so it turns out in this case roughly symmetric around zero ranging roughly ` [ -4 , +4 ]` .
series.hist ( by=none , ax=none , grid=true , xlabelsize=none , xrot=none , ylabelsize=none , yrot=none , figsize=none , bins=10 , ** kwds )
diff.hist()
thanks for the explanation stefen :) i tried the same but when i tried to create the bar : df_new = diff[abs(diff['random number generated ']) < 0.5 ] , df_new.diff() .hist ( bins =10 ) . i am getting different graph
am i missing something ?
do i need to specify the axis explicitly ?
i dont have the symmetry like your graph what should i do ?
that's because you are reducing the number of valid observations to around 20 , so with ` bins=10 ` , that is 2 values per bin on average , you can get all sorts of shapes . my chart above was for the 72 values , ie the full series , before dropping the values with ` abs ( diff < 0.5 )` .
yeah .. got it :)
just let me know if this answered your question or if you need anything else .

dataframe : shift expanding mean with groupby
i want to have expanding mean give the result excluding the current item , ie , the items prior average . here's what i'm looking for : #code
result : #code
the number highlighted should be ` 0.5 ` as : ' a ' won 1 of 2 games home prior to this entry at index = 6 . instead the result includes this game yielding ` 0.66 ` . actual output should be : #code
things i've tried include adding ` .shift ( 1 )` and trying to slice ` [: -1 ]` in the groupby but i can't get it to work . also thought of introducing a helper column but couldn't work out how to keep the original index .
i asked a related question here but i prefer this method to the group-apply-split routine . any help appreciated .
is this what you're looking for ? calculates the ` expanding_mean ` and ` shifts ` the result . #code
dear @user thank you for your answer , this works perfectly ! before i had : `df2['homewin_at_home '] = df2.groupby('home')['hw '] .apply ( pd.expanding_mean ) .shift ( 1 )` which didn't shift the calculation . i know the purpose of lambda so i think my error was putting the ` shift() ` in the wrong place ? am i correct ?
indeed , by pulling ` .shift() ` inside ` .apply() ` , the anonymous function represented by ` lambda ` shifts the result of ` expanding_mean ` for each group , as opposed to the result for all groups .
understood , thank you kindly .

divide multiple columns by another column in pandas
i need to divide all but the first columns in a dataframe by the first column .
here's what i'm doing , but i wonder if this isn't the " right " pandas way : #code
is there a way to do something like `df[['b','c ']] / df['a ']` ? ( that just gives a 10x12 dataframe of ` nan ` . )
also , after reading some similar questions on so , i tried `df['a'].div(df[['b ' , ' c ']])` but that gives a broadcast error .
i believe `df[['b','c ']] .div ( df.a , axis=0 )` and ` df.iloc [: , 1 :] .div ( df.a , axis=0 )` work .

rolling mean of time series pandas
i am trying to learn about rolling statistics . i created a data frame for : #code
as : #code
now i am trying to find the rolling mean of the series over the last 3 hours in a new column on a dataframe . i tried to find the rolling mean first : #code
am i correct ? but the result doesn't look like mean . can someone explain me this one please .
what do you mean " the result doesn't look like mean " ? can you be more specific ?
i have nan for first 3 columns
columns , or rows ?
i have rerun your code and could not find any problems . it seems to work .
if you want to take the rolling mean over the last 3 hours , ` rolling_mean ( df_new , window=5 )` should be ` rolling_mean ( df_new , window=3 )`
here is my code for the verification . #code
output : #code
the results by ` rolling_mean ` is consistent with manually calculated rolling mean values .
another way to confirm the validity is looking at the plots of calculated rolling mean . pandas.dataframe prepares ` plot ` method to draw graph easily . #code
i was confused to see nan as mean . tats why asked for verification . thanks :)
that's because of the definition of the rolling mean . you cannot get a value from ( x_{ t-2 } + x_{ t-1 } + x_t ) / 3 for t=1 and t=2 since the time series does not have x_{ t-2 } and x_{ t-1 } . that's how they return nan as outputs .
as long as your index is a timestamp ( as it currently is ) , you can just use resample : #code
when you use random numbers , it is best to set a seed value so that others can replicate your results . #code

average time series with missing time value
i have two series date sets , as read into pandas like this ( first column is row index , second is time and third is flux ) . #code
i need to average the flux and time of these two data sets according to the time . however , as you can see , the time is not aligned . some of them are missing . they are roughly separated by 0.02 . i am thinking of re-sample all the time columns into uniformly separated ones based on the starting point . i want something like this . #code
and then fill the nan with the one after or before it . in this way i can directly average the two data sets since now they are roughly aligned . so how can i make this happen in python using either pandas or numpy , scipy etc . or is there better method ( other than the way i am thinking of ) to achieve the job ? maybe an interpolation ? thanks guys .
i'd choose interpolation . there is a special scipy class for that .
iiuc you do that with ` interpolate ` method with ` method ` parameter equals to ` linear ` to do linear interpolation or ` nearest ` if you'd like to fill gaps with the closes values for your resampled dataframe : #code
edit
for your resampling , ` resample ` method works only valid with datetimeindex , timedeltaindex or periodindex . so you could convert your column to ` timedelta ` then set it as index , resample , ` reset_index ` to get back to your original dataframe . also you'll need to call ` dt.total_seconds ` to convert from #url to only seconds as your original data : #code
thanks for the answer , this is something i want .
but do you know how to get from my original data set to the one with nans in it ?
@user try edited version for resampling
this is great , thanks !
@user you could mark it as accepted if you satisfied with that solution
maybe you could merge and then fill na's , for instance :
load the data : #code
merge , sort , forward-fill calculate average : #code
i've been thinking the same thing as well . but i think if i do this , it's better do a rolling mean or median ?
can't help you with that , it depends on your goals .

pandas saving to_csv followed by read_csv produces keyerror
working on a os x 10.11.1 i have the following pd.dataframe #code
then i save it with : #code
after i read it with : #code
i get the following : #code
and a keyerror when referencing : #code
i do not understand this . everything seems fine . i just can't reference anything in the dataframe . #code
produces similar keyerror #code
seems ok as well .
where is my error ?
df.ix [ " startdate "] works for me
great ! works for me too . why doesn't ` df [ " startdate "]` or ` df [ 0 ]` work ?
basically you need to specify what column and what value you want to select . by using ` df [ 0 ]` python does not understand what value or column to select from .
i found an error on my side . when i save it , it is a series , when i read it , it is converted to a dataframe . hence my confusion when trying to reference . but i can not read it back as a series .
found the answer : ` s = pd.read_csv ( filename , index_col=0 , squeeze=true )` returns the series . everything is fine from there .

concatenate pandas dataframe in a loop of files
i am trying to write a script that loops over files via a certain pattern / variable , then it concatenates the 8th column of the files while keeping the first 4 columns which are common to all files . the script works if i use the following command : #code
while this works , it is not practical for me to enter file names one by one as i sometimes have 100's of files , so cant type in a df ... function for each . instead , i was trying to use a for loop to do this but i couldnt figure it out . here is what i have so far : #code
however , this is not working .
the issues i am facing are as follows :
how can i be able to iterate over input files and generate different
variable names for each which i can then have it used in the
pd.concat function one after the other ?
how can i use a for loop to generate a string file name that is a
combination of ` df ` and an integer
how can i fix the above script get my desired item .
a minor issue is regarding the way i am using the col_index function : is there a way to use the column # rather than column names ? i know it works for ` index_col=0 ` or any single ` # ` . but i couldn't use integers for > 1 column of indexing .
note that all files have the exact same structure , and the index columns are the same .
your feedback is highly appreciated .
hi @user , welcome to so . very good question . beware of the formatting though . clarity is very important ... :) happy new year
if you're only ever interested in the first 4 and 8th column then just pass a list of the ordinal positions : ` pd.read_csv ( genefile , index_col=header_row [ 0:1 ] , sep= " \t " , usecols =[ 0 , 1 , 2 , 3 , 7 ])` also i think that if the index col is always the first 2 cols then you can just pass this everytime : ` pd.read_csv ( genefile , index_col =[ 0 , 1 ] , sep= " \t " , usecols =[ 0 , 1 , 2 , 3 , 7 ])`
additionally you don't need to name variables , what you want to do is just append each df to a list and then at the end concat all the dfs in the list
@user k-thanks , i'll take notice of that in future questions .
consider using merge with ` right_index ` and ` left_index ` arguments : #code
thank you . this works . but , here are few things that i got hung at . in reality , i wanted to skip columns 2 and 3 from the output . so i wrote : result = pd.read_csv ( " 1isoforms.fpkm_tracking.txt " , sep= " \t " , index_col =[ 0 , 3 ] , usecols =[ 0 , 3 , 7 ]) - however , that gave an error regarding ' list index out of range ' which was surprising to me as when i put usecols 0 , 1 , 2 , 3 , 7 , it works . second , i assume the {} and format ( i ) means that in every loop , it replaces {} with i , correct ? when you set right and left index as true , does this mean that it will only merge if all the indexes are the same ?
also , i really dont get what the suffixes do.i read it on the pandas command line website and tried some examples but it did not make sense . can you briefly explain it ?
yes to your above two questions . if you change ` index_col ` be sure to do it in both ` pd.read_csv() ` functions . suffixes set up here basically iterates : col1 , col2 , col3 , ... col100 . i use the for loop's ` i ` variable to do that .
thanks parfait . i did change it in all the pd.read_csv() and still giving the errors . i raed online that some bug is there when using index_col and usecols ? is that true ? very weird . i kinda find it hard to believe . i will write my whole script and see if that would work as needed . i will use your answer above as a template .
interesting ... possibly , ` read_csv ` expects multi-index side by side in sequence . try leaving out the ` index_col ` argument and after csv import , use [ set_index() ] ( #url ) .
yea trying that . in fact , my script didn't work when i implemented in my data structure . but it works if it is used exactly as above . the two differences i may have are : 1- my files actually are all called isoforms.fpkm_tracking.txt , but they are in different directories . second , some values of the index column may be the same in different rows . that led the command to write the same line as same # of all the rows and messed up the structure . aagain nevertheless , the script above works for what i asked for . perhaps i should have asked for the exact thing from the beginning . will work on it .

how to change both negative and nan column value to zero in pandas
my data frame is like : #code
how do i change the column b values ( only negative and nan to 0 )
so far i tried like : #code
but is there a efficient to do both in a single command ? i am new to pandas . please tell me if it is not the best way to do so .
i'm no pandas expert , but to me the two-liner given in the question seems far more readable then the answers so far .
welcome to stackoverflow . you can check [ tour ] ( #url ) .
you can use ` loc ` and ` isnull ` : #code
another approach is to use the property that ` nan ` isn't positive or negative so if we negate the positive mask you also return the ` nan ` values and we can set these to ` 0 ` also : #code

using pandas read_csv with a certificate
using this : #code
results in this :
` urlerror : urlopen error [ ssl : sslv3_alert_handshake_failure ] sslv3 alert handshake failure ( _ssl.c : 646 )
`
which i understand , but how can i load my cert and pass the password to read_csv .
i had looked into using ` urllib2 ` but didn't have any joy passing the file and password in .
ideally something like this is what i would like : #code
maybe use a file-like object wrapper of url reader that support certs . ` pd.read_csv ` doc says it takes ` file_handle ` .

splitting pipeline in sklearn
i have a pandas dataframe ` df ` with the following features :
visitor_id , feature_1 , feature_2 , ..., feature_100 , truth_labels
i implemented the following model on sklearn :
1st step : scaling `df.drop(['visitor_id ' , ' truth_labels '] , axis=1 )` using ` sklearn.preprocessing.standardscaler() `
2nd step : clustering `df.drop(['visitor_id ' , ' truth_labels '] , axis=1 )` using ` sklearn.cluster.minibatchkmeans() ` in 10 clusters . set `df['cluster ']` to corresponding clusters .
3rd step : fit 10 ` sklearn.linear_model.logisticregression() ` on `df.drop(['visitor_id '] , axis=1 )` , one per cluster .
i have two questions :
1- is it possible to build a ` pipeline ` in order to aggregate these three steps ? in particular , how can i specify that i want to train 10 distinct ` sklearn.linear_model.logisticregression() ` models on my data splitted by clusters ?
2- is it possible to save this full pipeline ? how ?
i think you'll need to create both a custom minibatchkmeans class including a ` fit_transform ` method and a custom logistic regression class to specify your desired training scheme . i don't think the objects will work in a pipeline ootb . for example , all the n-1 objects in the pipeline simple call ` fit_transform ` and pass the result to the next object as input . consider what ` fit_transform ` returns for ` minibatchkmeans ` : it's a distance matrix , which is not going to be useful when it gets passed to the logisticregression object . this is the reason i suggest making custom classes

vectorize integration of pandas.dataframe
i have a ` dataframe ` of force-displacement data . the displacement array has been set to the ` dataframe ` index , and the columns are my various force curves for different tests .
how do i calculate the work done ( which is " the area under the curve ") ?
i looked at ` numpy.trapz ` which seems to do what i need , but i think that i can avoid looping over each column like this : #code
i was hoping to create a new ` dataframe ` of the areas under the curves rather than a ` dict ` , and thought that ` dataframe.apply() ` or something might be appropriate but don't know where to start looking .
in short :
can i avoid the looping ?
can i create a ` dataframe ` of work done directly ?
thanks in advance for any help .
you could vectorize this by passing the whole ` dataframe ` to ` np.trapz ` and specifying the ` axis= ` argument , e.g. : #code
these give the following output : #code
there were a few things wrong with your original example :
you will get a ` keyerror ` if you try to assign directly to ` work_done [ col ]` because you created ` work_done ` as an empty ` dict ` ( ` {} `) , meaning that the corresponding dictionary key doesn't exist yet ( this would be allowed if ` work_done ` was a ` defaultdict ` instead of a ` dict `) .
` col ` is a column name rather than a row index , so it needs to index the second dimension of your dataframe ( i.e. ` .loc [: , col ]` rather than ` .loc [ col ]`) .
edit :
you could also generate the output ` dataframe ` directly by ` .apply ` ing ` np.trapz ` to each column , e.g. : #code
however , this isn't really ' proper ' vectorization - you are still calling ` np.trapz ` separately on each column . you can see this by comparing the speed of the ` .apply ` version against calling ` np.trapz ` directly : #code
this isn't an entirely fair comparison , since the second version excludes the extra time taken to construct the ` dataframe ` from the output numpy array , but this should still be smaller than the difference in time taken to perform the actual integration .
excellent answer thanks . i'll test it at some point over the weekend and tick the answer if it works .

clustering a pandas dataframe with seaborn - overlapping labels
i'm on mac os 10.10.2 , python 2.7.11 , seaborn 0.6.0 , pandas 0.17.1 , matplotlib 1.5.0 . i'm running the following code to hierarchical clusterize a dataframe with ` seaborn.clustermap ` : #code
which returns :
i have some troubles with the overlapping labels on the row side . according to the examples provided here , the default behaviour should set the labels horizontally . is this a bug ? anyone can reproduce it ?
the labels show up properly when i run it .
thanks for trying that ! may you show your python , pandas etc version ?
pandas ( 0.17.1 );
seaborn ( 0.6.0 ); matplotlib ( 1.4.3 ); python 2.7.10 ;o sx 10.9.4 .
so maybe the issue is related to matplotlib 1.5.0 ...
i have the same output as you @user ( vertically oriented labels ) with the following config : matplotlib 1.5.0 / pandas 0.17.0 / seaborn 0.6.0 / python 3.4.3 on ubuntu 15.04 .
a workaround to set the label horizontally is the following : #code

pandas dataframe grouped box plot from aggregated results
i want to draw box plot , but i don't have raw data but aggregated results in pandas dataframe .
is it still possible to draw box plot from the aggregated results ?
if not , what is the closest plot that i can get , to plot the min , max , mean , median , std-dev etc . i know i can plot them using line chart , but i need the boxplots to be grouped / clustered .
here is my data , the plotting part is missing . please help . thanks #code

pandas : read a small random sample from big csv , according to sampling policy
very related to read a small random sample from a big csv file into a python data frame .
i have a very big csv , with columns ` patient_id , visit_data ` . i want to read a small sample from it , but if i sample a patient i want to sample all of his records .
what have you tried so far ? provide some code .
if you want to keep working with ` .csv ` , you can read the files in chunks , select and concatenate the pertinent rows from each chunk along the below lines ( see docs ) : #code
however , i would recommend taking a look at hdf5 storage via ` pandas ` as this allows you to select via queries on indexed data rather than iterating through a file . and there are of course various ` sql ` -based options ( see basic example )

how do i plot multiple series with different x-values in pandas ?
how do i plot multiple series with different x-values in pandas ? i expect the following to work but it seems to produce multiple figures . #code
you would need to tell ` matplotlib ` through the ` pandas.plot ` interface that you want the plots on the same ` axis ` ( see docs ) : #code

pandas : writing data frame to excel ( .xls ) file issue
i am trying to write the data frame to excel and also making the cell width ( 20 ) and trying to hide grid lines . so far i did like : #code
my data frame looks like : #code
is this wrong way ? i don't see the changes in the output file . what i am doing wrong here ? is there a way to name my row header ?
here . when try to hide the grid line . the row ( in data frame['time ]) now column a in excel still has the grid .
what you do mean by " * is there a way to name my row ? * " the index is included in the written data by default and the index effectively names all of the rows .
i meant the row header . sorry !
you should include at least a sample of your dataframe to reproduce your problem .
@user please see the update one
iiuc , for the ` set_column ` width you're actually write your ` df ` twice ; the correct workflow should be the following ( edit : add ` engine ` keyword ): #code
this should correctly set the columns width . if you don't want to have the index ` time ` column in your output , you should set : #code
if you have previously set : #code
the grid issue actually it's still there when you plot the full dataframe . i think the the current ` excelwriter ` object doesn't support the index for the ` hide_gridlines() ` option , but i don't know if it is a bug or not .
edit : thanks to the comments , this isn't a bug .
in relation to the last paragraph , that isn't a bug . xlsxwriter hides the gridlines , which can be verified by going into print preview . the lines that are visible around the indexes are borders ( not gridlines ) which are added by default by pandas .
thanks @user for pointing this out !

pandas escape carriage return in to_csv
i have a string column that sometimes has carriage returns in the string : #code
when writing to and reading from csv , the dataframe gets corrupted : #code
question
what's the best way to fix this ? the one obvious method is to just clean the data first #code
but i was wondering if there's another solution such as escaping .
specify the ` lineterminator ` : #code

how to combine two dataframes only if there is a common index only , and leave empty cell otherwise
i have two files :
file1.txt : #code
file2.txt : #code
my desired output is :
file1.txt then add the values from file2 that match the first column only : #code
hence , the type and condition columns of file2.txt will be added . if value is in file1 but not in file2 , it will be replaced by just empty cell . if value is in file2 but not file1 , it will be ignored .
here is what i tried so far and it is not working :
inputting 2 data frames then trying to use data merge or join : #code
i also tried pd.concat with axis 1 but that also didnto work .
i then tried : #code
this also did not give the desired output and there were lot of "'" between strings ..
is there any suggested method ? i know how to merge to data frames if it has same # of rows and index , but i couldnt do it if i only want to do it using first file as a standard index . i know how to do it in r using the merge function then by.x and by.y , but r messes up all my header names ( the ones up are just an example ) . so it is best to do it in python .
you can use ` join ` to merge on the indices : #code
note : you can fill in the nan with empty string using fillna , but i like to leave them blank ( see this post ) .
that doesn't get you the following line : #code
but i don't understand how you'd get that ( ens4s is from d and cancer 2 is from k ) .
for some reason , join is giving me error : valueerror : columns overlap but no suffix specified : index([u'id '] , dtype='object ') . in regards to this option though , it looks like outer ensures keeping all samples , while left will use the df1 ( i.e. the one on the left ? ) as reference , is this what's supposed to do ? do you know how to come around the error ?
you are right about k . it was an error . i fixed that now .
@user if id is the index then join works , if it's not you need to use merge .
reading your files with ` sep= ' \t '` didn't parse properly for , but ` sep= ' \s+ '` did for your sample lines , and then the standard ` merge ` gives your desired result : #code
you can of course also move ' id ' to the ` index ` and use ` .join() ` , ` .concat() ` , or ` .merge ( left_index=true , right_index=true )` with the appropriate settings for ` left ` merge for each .
the df1.merge works , except when i use : result1 = df1.merge ( df2 , on='id ' , how='left ') ; result1.to_csv ( " merge1.csv ") ; i end up with an additional column in the beginning that has numbers 0 to 4 . how can i get rid of that ? also , is it possible to add the type and condition column after id immediately ? it would make more sense to add it there for me than at the end . should i post that as a different question or is it an easy step ? appreciate the responses
do to_csv ( path , index=false ) to get rid of the numbers .
for the column order , do df.loc [: , [ ' col1 ' , ' col2 ' .. ] to get desired order .
great , that works . any idea about how to have type and condition added early on right after id and without being sorted ?
so you mean for result1.loc [: , column name order ] , right ? i tried that but instead of using column names ( as these can change ) , i tried using column numbers as such : final_result = result1.loc [: , [ 1 , 5 , 2 , 3 , 4 ]] in order to order the columns of result1 as 1 , 5 , 2 , 3 , then the 4th . however that gave this error : ' none of [[ 1 , 5 , 2 , 3 , 4 ]] are in the [ columns ]' . cant i do it by numbers ?
use .iloc for numbers .
thanks , that didn't work . kept giving errors for syntax . but i tried something else : newdf2= result1.ix [: , [ 0 , 4 , 5 , 1 , 2 , 3 ]] and that worked . thanks aa lot stefan .
happy to help . do you think you could accept the answer or do you need anything else ?
hi stefan , yes of course i will accept it . i also was able to figure out the iloc syntax : newdf3= result1.iloc [: , ( 0 , 4 , 5 , 1 , 2 , 3 )]
a quick followup : i am trying to set up the columns in a consecutive pattern using a range of columns as the end column will be a variable in my case . here is what i tried and it is not working : newdf4= result1.ix [: , [ 0 , 4 , 5 , 1:3 ]] #the 1:3 is causing an error . i also tried : newdf3= result1.iloc [: , ( 0 , 4 , 5 , 1:33 )] #that also didn't work . i also tried : list_of_ints = list ( range ( 1 , 4 )) ; newdf6= result1.iloc [: , ( 0 , 4 , 5 , list_of_ints )] and that also didn't work . any idea how to fix this to have a combination of a list of integers , as well as a range of values for the columns to be extracted ?
if you are trying to use 1:33 as a ` label ` as opposed to a ` slice ` including ` integer ` position from ` 1 ` to ` 33 ` , than you should probably use it as a ` string ` - `'1 : 33 ` . see the documentation which is quite specific as to allowed inputs for the three basic selection methods : ` loc ` , ` iloc ` , and ` ix ` .
i have checked the doc . i am using it as follows : instead of using result1.idx [: , [ 0 , 4 , 5 , 1 , 2 , 3 ]] # to have the columns reordered as #0 , 4 , 5 , 1 , 2 , 3 ; i want to have the rage for 1:3 ( as the ' 3 ' will change depending on the number of files i am working with- i want to adapt this into another script . does it make sense ? how would you advise this can be done ?
would you mind posting this as a new question so you can show your data and desired output along with it ? not sure i fully understand what you are trying to accomplish .
yup . will do this in a minute .
#url

how to efficiently generate a special co-author network in python pandas ?
i'm trying to generate a network graph of individual authors given a table of articles . the table i start with is of articles with a single column for the " lead author " and a single column for " co-author " . since each article can have up to 5 authors , article rows may repeat as such : #code
my end goal is to have a nodes table , where each row is a unique author , and an edge table , where each row contains source and target author_id columns . the edges table is trivial , as i can merely create a dataframe using the requisite columns of the article table .
for example , for the above table i would have the following node table : #code
notice how the " is_published " shows if the author was ever a lead or co-author on at least one published paper . this is where i'm running into trouble creating a nodes table efficiently . currently i iterate through every row in the article table and run checks on if an author exists yet in the nodes table and whether to turn on the " is_published " flag . see the following code snippet as an example : #code
for my data set ( with tens of thousands of rows ) , this is somewhat slow , and i understand that loops should be avoided when possible when using pandas dataframes . i feel like the pandas ` apply ` function may be able to do what i need , but i'm at a loss as to how to implement it .
with ` df ` as your first ` dataframe ` , you should be able to : #code
for a unique list of ` author_id ` and ` co_author_id ` with their respective ` is_published ` information .
to only keep ` is_published=true ` if there is also a ` false ` entry : #code
` .sort_values() ` will sort ` true ` ( ` == 1 `) before ` false ` , and ` .drop_duplicates() ` by default keeps the first occurrence ( see docs ) . with this addition i guess you don't really need the first ` .drop_duplicates() ` anymore .
thanks , this is really close to what i'm looking for ! the only issue is if i have an author listed once with ` is_publisehd = false ` and once with ` is_publisehd = true ` . wouldn't your one liner keep both entries ?
yes , because these would not be duplicates . which one do you want to keep , or what's the logic for selecting between the two ?
the logic is to set " is_published " to true if the given author_id has at least once plublished paper . hence if there were duplicates , it would always choose the " is_published " = true if possible , otherwise default to " is_published = false
updated answer , let me know if this doesn't work .

fill in missing dates in dataframe using the mean
i have dates that i'm pulling into a dataframe at regular intervals .
the data is generally well-formed , but sometimes there are bad data in an otherwise date column .
i would always expect to have a date in the parsed 9 digit form : #code
how should i check and fix this ?
what i would like to do is replace whatever is not a date , with a date based on a variable that represents the last_update + 1 / 2 the update interval , so the items are not filtered out by later functions .
data as shown is published_parsed from feedparser . #code
could you edit your question , to show how the date is supposed to be ? specifically , in ( 2015 , 12 , 29 , 0 , 30 , 50 , 1 , 363 , 0 ) why are you reading in ( 0 , 30 , 50 , 1 , 363 , 0 )
data as shown is the published_parsed entry attribute from feedparser , it comes as 9 integers .
what does 1 , 363 , 0 represent ? the more you explain the easier it will be for everyone to help you , even those who may not be familiar with feedparser but are familiar with pandas . also , please include your desired output for the last_update + 1 / 2 values .
sure -- ( tm_year=2000 , tm_mon=11 , tm_mday=30 , tm_hour=0 , tm_min=0 , tm_sec=0 , tm_wday=3 , tm_yday=335 , tm_isdst=-1 )
i'd expect a date like ( 2015 , 12 , 29 , 0 , 30 , 50 , 1 , 363 , 0 ) -- but , replacing with just the trimmed date , but the long form is also ok .
you have 2 missing values , do you want to fill both of them in ? since , the values above and below the missing value are identical , you just want the same value for that ? have you considered using this [ format ] ( #url ) ? i've tried answering to the best of my understanding but feel free to clarify .
thank you for your help . yes , i'm going to store in the pandas format . this is for processing on the way into pandas .
happy to help , and welcome to stack overflow . if this answer or any other one solved your issue , please mark it as accepted . if not please let us know , how to further help resolve your question .
i realized the ' none ' was actually a none object , which just let me do a bit of alteration to your suggestion ( if x ! = none ) , then the other replace solution . thank you again for the guidance .
when working with dates and times in pandas , convert them to a pandas timestamp using ` pandas.to_datetime ` . to use this function , we will convert the list into a string with just the date and time elements . for your case , values that are not lists of length 9 will be considered bad and are replaced with a empty string `''` . #code
find the indices where the dates are missing use ` pd.isnull() ` : #code
to set the missing date as the midpoint between 2 dates : #code
#code
yields #code
explanation :
` calendar.timegm ` converts each time-tuple to a timestamp . unlike
` time.mktime ` , it interprets the time-tuple as being in utc , not local time .
` apply ` calls ` tuple_to_timestamp ` for each row of `df['orig ']` .
the nice thing about timestamps is that they are numeric , so you can then use
numerical methods such as ` series.interpolate ` to fill in nans with interpolated
values . note that the two nans do not get filled with same interpolated value ; their values are linearly interpolated based on their position as given by ` ts.index ` .
` pd.to_datetime ` converts to timestamps to dates .

pandas : read_html
i'm trying to extract us states from wiki url , and for which i'm using python pandas . #code
however , the above code is giving me an error l
importerror traceback ( most recent call last )
in ( )
1 import pandas as pd
----> 2 f_states = pd.read_html ( ' #url ')
if flavor in ( ' bs4 ' , ' html5lib ') :
662 if not _has_html5lib :
--> 663 raise importerror ( " html5lib not found , please install it ")
664 if not _has_bs4 :
665 raise importerror ( " beautifulsoup4 ( bs4 ) not found , please install it ")
importerror : html5lib not found , please install it
i installed html5lib and beautifulsoup4 as well , but it is not working .
can someone help pls .
running python 3.4 on a mac
new pyvenv #code
then ran your example .... #code
all works ...
thanks , this worked . although i have installed all of them , but i reinstalled it , and opened a new instance , and it worked . thanks
glad it helped you

pandas import error
i am trying to install pandas but it throws the following error #code
my pandas version is 0.17.0
and my numpy version is 1.9.2
can anyone help me on how to sort out the above issue ??
it says the compiled module used has been compiled for a different machine , can this be the case ?
take look at these questions : [ pandas valueerror : numpy.dtype has the wrong size , try recompiling ] ( #url ) and [ valueerror : numpy.dtype has the wrong size , try recompiling ] ( #url )
what operating system ? how did you install pandas and numpy ? did you get any warnings while installing numpy or pandas ? if so what were they ?
not a solution for the specific problem , but consider installing anaconda that could solve your problem .
i am trying all these in docker ( where all packages for my project is already installed by the system's team ) . i have python 2.7.10 .

pandas equivalent of graphlab.sframe.unpack() and .stack() ?
what is the best way in pandas to ( 1 ) split a column of dict into columns , and ( 2 ) expand a column of list into rows ?
these two operation can be done in graphlab by sframe.unpack() and sframe.stack() respectively .
the first operation can be done with an ` apply ` function returning a series ( see the accepted answer to this question ) , followed by a horizontal ` concat ` operation ( i.e. , with ` axis=1 `) .
the second operation is simply pandas's own ` stack ` .

how can i plot a correlation matrix as a set of ellipses , similar to the r open-air package ?
the figure below is plotted using the open-air r package :
i know matplotlib has the ` plt.matshow ` function ,
but it can't clearly show the relation between variables at the same time .
here is my early work ?
df is a pandas dataframe with 7 variables shows like below :
i don't know how to attach a ` .csv ` file to stackoverflow .
using ` plt.matshow ( df.corr() , cmap = plt.cm.greens )` , the figure shows like this :
the second figure can't represent the correlation relations of the variables as clearly as the first one .
edit :
i upload the csv file to google docs here .
you should provide a basic dataset to work with .
sorry , i'll provide it soon .
please don't post screenshots of your dataset - i can't copy / paste from an image . paste the actual values into your question as text .
what do you mean by representing the correlation relations ? do you mean the correlation coefficient values ? if so , please take a look at seaborn's annotated heatmap #url
[ here's a related answer that uses the r ` corrplot ` package ] ( #url )
assuming you are interested in showing cluster relations , the ` seaborn ` package mentioned in the comments also has a clustermap . using your correlation matrix ( looks like you want to show correlation coefficients as ` int ` in the ` [ -100 , 100 ]` range , you could do the following : #code
and then use ` seaborn.clustermap() ` as follows : #code
i'm not aware of any existing python library that does these " ellipse plots " , but it's not particularly hard to implement using a ` matplotlib.collections.ellipsecollection ` : #code
for example , using your data : #code
negative correlations can be plotted as ellipses with the opposite orientation : #code

pandas dataframe : how to group by values in a column and create new columns out of grouped values
i have a dataframe with two columns : #code
what i want is : #code
i know that the values in column x are repeated all n times .
i think that a column header is missing in your expected output , since you have five num columns and four headers .
you could use ` groupby / cumcount ` to assign column numbers and then call ` pivot ` : #code
yields #code
or , if you can really rely on the values in ` x ` being repeated in order n times , #code
yields #code
using ` reshape ` is quicker than calling ` groupby / cumcount ` and ` pivot ` , but it
is less robust since it relies on the values in ` y ` appearing in the right order .

bool operator in for timestamp in series does not work
for some reason i cannot check if a date is in a pandas series . #code
now ... #code
can someone explain me why ` d in ts ` does not work ?
tia ,
olivier .
when you use
` value in obj ` , python tries to pass value to ` __contains__ ` built-in function of obj
try to check the type of ts : #code
if you see ` pandas.core.series.series ` , you will find the definition of ` __contains__ ` as below : #code
so , try this : #code
it tries to check whether given key is in it's index or not , not checking in it's values #code
ok it makes it but then , why it is not consistent ? if i use integer : ` ts = pd.series ([ 1 , 2 , 3 ])` then ` 2 in ts ` gives ` true `
a code block alone does not provide a good answer . please add explanations ( why it solve the issue , where was the mistake , etc ... )
hope i could explain as you asked in edition to my previous answer :)
the long explanation is clear , thanks :-) my example with ` ts = pd.series ([ 1 , 2 , 3 ])` was just back luck , index having same values than values .

color of " curve density " in andrew curves
when plotting a large dataset of multivariate data using andrew curves , i end up with a broad " band " , since so many curves are overlapping ( see the picture ) .
my question is : how can i color the curves corresponding to the local " density of curves " ? i.e. something like a kernel density estimation , or geometric density .
my code for simply plotting the andrew curves are as follows : #code

python ( pandas ) fast mapping of multiple datetimes to their series indices ?
i have a large pandas dataframe in which one column is ( unordered ) datetimes from a known period ( the year 2013 ) . i need an efficient way to convert these datetimes to indices , where each index = # hours since start_time ( ' 2013-1-1 00 )' . there are duplicate times , which should map to duplicate indices .
obviously , this can be done one-at-a-time with a loop by using timedelta . it can also be done with a loop by using pandas series ( see the following snippet , which generates the ordered series of all datetimes since start_time ): #code
after running this snippet , one can get indices using the .index or .get_loc methods in a loop .
** however , is there a fast ( non-loopy ? ) way to take a column of arbitrary datetimes and find their respective indices ? **
for example , inputing the following column of datetimes :
2013-01-01 11:00 : 00
2013-01-01 11:00 : 00
2013-01-01 00:00 : 00
2013-12-30 18:00 : 00
should output the following indices : [ 11 , 11 , 0 , 8730 ]
use ` isin ` : #code
` between ` and ` between_time ` are also useful
this solution does not make duplicate timestamps produce duplicate indices . i have many duplicate timestamps , and need them to produce duplicate indices ** ( so the indices output is the same shape as the datetimes input ) ** . i updated the original question to reflect this . thanks for the answer , though ! it's definitely closer than my solution !
loc can take a list or array of labels to look up : #code
thank you for the responses . i have a new , faster solution that takes advantage of the fact that pandas supports datetime and timedelta formats . it turns out that the following is roughly twice as fast as colin's solution above ( although not as flexible ) , and it avoids the overhead of building a series of ordered datetimes : #code
where df is the pandas dataframe and ' mydatetimes ' is the column name that includes the datetimes .
timing the code yields that this solution performs 30,000 indices in :
0:00 : 00.009909 --> this snippet
0:00 : 00.017800 --> colin's solution with ts=series ( ... ) and ts.loc . i have excluded the one-time overhead of building a series from this timing

rearranging a non-consecutive order of columns in pandas dataframe
i have a pandas data frame ( result ) df with n ( variable ) columns that i generated using the merge of two other data frames : #code
result1 dataframe is expected to have a variable # of columns ( this is part of a larger script ) . i want to arrange the columns in a way that the last 2 columns will be the second and third consecutively , then all the remaining columns will follow ( while the first column stays as first column ) . if result1 is known to have 6 columns , then i could use : #code
but , i need the 1 , 2 , 3 to be in a range format as it is not practical to enter the whole of the numbers for each df . so , i thought of using : #code
that would be the idea way but this is creating syntax errors . any suggestions to fix this ?
instead of using slicing syntax , i'd just build a list and use that : #code
it works . thanks . much appreciated .

non-conformable arrays in quantile regression model from pandas dataframe using rpy2
i am doing a quantile regression on the engel dataset with rpy2 ( 2.7.6 ): #code
however this generates the following error : #code
from what i understand , non-conformable arrays in this case would mean there are some missing values or the ' arrays ' being used are different sizes . i can confirm that this is not the case : #code
what else could this error mean ? is it possible that the conversion from dataframe to data.frame in rpy2 is not working correctly or maybe i'm missing something here ? can anyone else confirm this error ?
just in case here is some info regarding the version of r and python . #code
any help would be appreciated .
edit 1 :
if i load the dataset directly from r i don't get an error : #code
so i think there is something wrong with the conversion with ` pandas2ri ` . the same error occurs when i try to convert the dataframe to data.frame manually with ` pandas2ri.py2ri ` .
edit 2 :
interestingly enough , if i used the deprecated ` pandas.rpy.common.convert_to_r_dataframe ` the error is gone : #code
there is definitely a bug in ` pandas2ri ` which is also confirmed here .
from an r perspective i cannot be much help since i get the expected result from `rq('foodexp ~ income ' , data=engel , tau= 0.5 )` . i'm wondering if you are actually getting substitution of the ` engel ` dataset into the r environment .
if you do everything in r , does it work ? if so , then the problem is probably in the conversion from pandas to r .
@user if i do everything in r i don't get the error . i can also confirm that it has to be something wrong with the ` pandas2ri ` conversion by loading the dataset directly from r in python ( see edit ) .
filed as a [ bug ] ( #url ) on rpy2's bitbucket page .
as answered on the rpy2 issue tracker :
the root of the issue seems to be that the columns in the pandas data frame are converted to array objects each with only one column . #code
the distinction is a subtle one , but this seems to be confusing the ` quantreg ` package . there are other r functions appear to be working independently of whether the objects is an array with one column or a vector .
turning the columns to r vectors appears to be what is required to solve the problem : #code
now i would like to gather more data about whether this could solve the issue without breaking things else . for this i turned the fix into a custom converter derived from the pandas converter : #code
the easiest way to use this new converter might be in a context manager : #code

how to scroll through a ( massive ) pandas dataframe ?
almost all my datasets are panel datasets . that is , i have a multi-index at the ( say ) company and time level .
since i perform a lot of different operations , merges , etc , i want to know if there's a way to ' see ' the data ( like in excel ) and scroll through my data quickly . that way i can spot if my code misses something or others .
what options are available for a large pandas dataframe ( 10gb+ ) . i use a powerful desktop with 128gb ram so i can easily fit the data into it ?
thanks !
the qgrid library could be helpful . it is designed for such kind of tasks and developed and used by skilled quants . you should try it out . it is easy installed with pip : #code
here is a live demo that allows you to do filtering , for example .
thanks ! do you know how i can load and see a pandas dataframe with it ? do you have a working example ?
does it work for you ?

values missing : overlaying points on boxplot subplots from a pandas dataframe
i have two dataframes : df1 with 5000 rows and 12 columns and df2 with a single row and identical 12 columns as df1 . i would like to plot each column on a separate subplot as a boxplot with values from df1 , and overlay it with scatter plots ( just one value per boxplot ) from df2 .
update : i tried using a transposed df2 with a numerical index . the following code does not give me any error but only one value on the second boxplot is visible .. i can't see the rest #code
tried attaching the image here but i am slow today ...
#url
using pandas plotting :
getting boxplot subplots only is easy using #code
but i don't know how to specify y for the scatter plot #code

import pandas does not work in terminal ; works in python shell
i'm having some issues with importing pandas into my ipython note book . however when i import pandas into the python shell it works .
let's take a look at my code : #code
and i get this error : #code
so if i just try to import pandas : #code
and i get this error : #code
however , when i check in the python shell , i see this : #code
i'm kinda at a loss for what i've done wrong . i think it might be because i have multiple versions of python installed , but i don't quite know what the issue is .
thanks
idle know all path but your system haven't any idea ! try install panda from source code . idle not same with python-terminal . ( a opinion : ` pip ` not equal to ` apt-get ` got a lot differences )

adding data pandas dataframe
i need a pandas dataframe in order to pass values to bokeh . #code
this code doesn't return an error , but the dataframe is empty .
afret reading this question i tried this way too : #code
and here's the error : #code
any suggestion ?
can you add [ minimal , complete , and verifiable example ] ( #url ) ?
try adding `orient='index '` : #code
using an improvised example : #code

what is the most efficient way to subtract rows of a dataframe ( python pandas ) by index in python pandas
i have a dataframe pandas like this : #code
and i need to subtract the time between one event and another , the result would : #code
i get this result with a for loop about pandas dataframe with for index , row in datos.iterrows() : . my function is : #code
please suggest a way to make function more efficiently .
thank you very much in advance .
use ` diff ` with ` fillna ` : #code

pandas : attributeerror : ' module ' object has no attribute ' __version__ '
when i try to import pandas into python i get this error : #code
but when i check if pandas is installed : #code
so i don't know what is wrong ? what is going on with my pandas ?
edit #code
for some reason there was no output to ` pip list |grep matplotlib `
edit2
i wanted to see if there was a different path to the executables ` ipython ` and ` python ` . so i ran this : #code
however in ipython , i get this : #code
could that be the problem ?
@user removed , thanks
please consider adding the output of ` pip list |grep matplotlib ` and ` conda list matplotlib ` to your question .
i've update the question with your request @user
remove ( or rename ) the file ` matplotlib.py ` from your current working directory . it shadows the real library with the same name .
remove it from " / users / me / miniconda2 / lib / python2.7 / ... " ?
from the directory you are working in .

calculating the number of consecutive periods that match a condition
given the data in the ` date ` and ` close ` columns , i'd like to calculate the values in the ` consecperiodsup ` column . this column gives the number of consecutive two-week periods that the ` close ` value has increased . #code
i've written the following code to give the ` upthisperiod ` column but i can't see how i would aggregate that to get the ` consecperiodsup ` column , or whether there is way to do it in a single calculation that i'm missing . #code
what output do you expect ?
expected output is in the ` consecperiodsup ` column . the input is just ` date ` and ` close ` .
this can be done by adapting the ` groupby ` , ` shift ` and ` cumsum ` trick described in the pandas cookbook , grouping like python s itertools.groupby . the main change is in dividing by the length of the period - 1 and then using the ` ceil ` function to round up to the next integer . #code

expanding mean over multiple series in pandas
i have a groupby object i apply expanding mean to . however i want that calculation over another series / group at the same time . here is my code : #code
how can i use ` pd.expanding_mean ` in a ` groupby ` object that will average over the `'home '` and `'away '` columns so i see their average wins / ties / losses over all venues ? now it just gives the prior win average for a team playing at home or away , not both home away .
i've been trying different levels and df.stack() and reindexing but no luck .
any help appreciated in getting there .
here is what the correct result for just home wins at home and home wins all venues : #code
you may have to introduce a ' team ' ` column ` to follow a team's record irrespective of venue . the below could get you closer . starting with : #code
to get : #code
next , stack so you can follow each team : #code
then , define what's a ' win ' , calculate overall record and apply ` expanding_mean ` : #code
since you have references for both games and teams , you could ` merge ` and ` filter ` to get your preferred layout .
thank for the answer , works perfectly . combining the wins in a function was the idea i was missing , makes it all clear

when extracting a series from a dataframe with multi-indexed columns i cannot read with read_csv
i have the following dataframe : #code
now i want to save each column individually to a csv file . ( don't ask why . i just have to . ) #code
note :
series.name
[ ' a ' , ' c1 ']
etc . always a list of strings instead of a string .
so far so good . then i want to read the series again : #code
but i get : #code
the headers are all mixed up . i suspect it is because of the multiindex . how can i write and read safely to / from a cvs file with multi-indexed columns ?
you can use the ` header =[ 0 , 1 ]` command : #code

how to concatenate pandas.dataframes columns
i have a dataframe called ` raw_df ` : #code
` raw_df ` looks like this : #code
at the moment there is no index but i would like the distance columns to be combined into one index so the columns are then : #code
note that there were 2 entries in forces1 for distance1 == 0 but i just took the first one .
stefan posted an amazing answer to my poorly-described question but it seemed to fill in any missing forces with other numbers ( which would be misleading because there were no force measurements for those distances in those tests ) . i have used ` np.nan ` for missing values as i think this is what ` pandas ` does .
perhaps ` pandas.dataframe ` was not designed for such data , and i should use ` numpy.genfromtxt ` instead and just select the columns i need on the fly : i don't see any advantage to using a ` pandas.dataframe ` if i'm selecting columns on the fly ( because i'm not using an index in that case ) .
thanks for any help .
would be nice having a sample of your original dataframe to work with .
what are your ` test_ids ` ? are all the values in ` distances1 ` , ` distances2 ` , etc . unique ?
test_ids is just a list of integers corresponding to the test numbers . for example , the first is 1 as in forces 1
and are all the values in ` distances1 ` , ` distances2 ` , etc . unique ?
the distances are not unique . thanks for starting to help
if i'm understanding correctly , you are starting from a situation similar to this : #code
and you are aiming to have the various ` distance ` columns form a single ` index ` while the respective ` force ` ` columns remain in place . you could ` stack ` the frame like so : #code
to get : #code
wow . that looks very good . i don't need the multiindex but otherwise looks right . i'll test it later tonight when my kids are in bed . thanks
is there a shortcut not to create the multiindex ?
updated so there's no ` multiindex ` . ` .stack() ` automatically creates the extra ` column ` , so i just dropped it .
thanks . some of the formatting may have been broken but it's not a problem . i'm intrigued by the magic string " level_4 " . will test in about an hour
thanks , fixed the formatting . no magic here - ` .stack() ` creates a ` series ` , adding all ` values ` gathered from the ` dataframe ` as ` column ` and creating a new ` index ` level with the ` column ` names , and creating a name for it . that's what you see after ` reset_index() ` - the name depends on the number of ` levels ` in the ` index ` prior to ` .stack() ` - ie , the number of ` force ` ` columns ` set as ` index ` just prior . i've changed it so you don't have to rely on knowledge of the number of ` force ` columns .

combine similar rows to one row in python dataframe
i have some dataframe as below , what i want to do is to combine the rows with same
" yyyymmdd " and " hr " into one row .
( there are several rows with same " yyyymmdd " and " hr " ) #code
part of the output i want should like this for instance : #code
please share some ideas that i can use in python pandas or sql , thanks !
are you using an sql database ? or is this just some sort of spreadsheet file ?
hi zabari ,
i used sql first , then import the data into python pandas
put your data into a pandas dataframe , and then groupby and get the max of each group ,
copy-pasting your example into a csv , it looks like this : #code
output :
use reset_index() in case you don't want the multi-index .
hi ezer k , thanks for the way you shared ! it works !!!
( and also thanks for the " reset_index() " you shared , it just make my dataframe clearer )

pandas : how do i split multiple lists in a columns into multiple rows ?
i have a pandas dataframe that looks like the following : #code
and want to convert it into the following format : #code
the columns datetime and values have the same dimension .
i have already asked a similar question here but couldn't manage to apply the solution for my problem with two columns .
what's the best way to convert the dataframe into the required format ?
you can extract from columns ` values ` and ` datetime ` new ` series ` and then merge them with original dataframe ` df ` by ` concat ` : #code
#code
you can use a nested list comprehension together with ` concat ` to extract the core part of your dataframe that repeats each row the desired number of times ( based on the length of its ` values ` list .
then use another nested list comprehension to generate each of the ` datetime ` and ` values ` columns . #code
you could iterate through the rows to extract the ` index ` and ` series ` info from the cells . i don't think that ` reshaping ` methods work well when you need to extract info at the same time :
sample data : #code
build new ` dataframe ` as you iterate through the ` dataframe ` ` rows ` : #code
to get : #code

pandas weighted average on rows that contain a certain value
i have a dataframe indexed by date like follows : #code
i need to carry out a weighted average calculation where the identifier is only a and b for each date . the calculation should be :
(( 56 * 21 ) + ( 78 * 2 )) / ( 21 + 2 ) = 57.91
output : #code
i have experimented with group bys and sums but i am struggling to rejoin the dataframe together with the weighted average .
what would be the easiest way of carrying out this calculation ? thanks !
you could do that with ` isin ` method to subset your original dataframe to smaller one . then you could do your calculations with that subset dataframe and then using ` loc ` you could assign it to the original dataframe : #code
really nice solution , thanks !
you could use ` dropna ` method to drop all rows which contain ` nan ` in any of the columns . or you could use ` fillna ` to fill ` nan ` value to what you want . you could also check ` nans ` with ` notnull() ` and ` isnull ` methods .

stack columns in pandas dataframe to achieve record format
i have a dataframe with the first column being country name , and the next 12 columns as annual gdp figures ( with column headers ' 1999 ' , ' 2000 ' , ' 2001 ' , etc ): #code
how would i stack the table such that i have one column for country name , one column for year , and one column for gdp figures ? this is my code so far : #code
which results in : #code
ultimately i'm looking for something like this : #code
clearly i am new to python and pandas . any help would be appreciated .
what is the original data looks like ? is it a dictionary or a list ?
i've read a csv into a pandas dataframe .
what does it look like before you applied the ` .stack() `
column for country name containing text , columns for 1999 through 2010 containing floats .
instead of describing it , showing ` gdp.head ( 20 )` ( the original frame , i mean ) would make it easier to copy and paste .
edited post to show original data .
you could use ` pd.melt ` and then ` sort_values ` : #code

